{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:05:39.243074Z",
     "iopub.status.busy": "2024-12-11T22:05:39.242732Z",
     "iopub.status.idle": "2024-12-11T22:05:41.357756Z",
     "shell.execute_reply": "2024-12-11T22:05:41.357292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=analysis.ipynb\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env WANDB_NOTEBOOK_NAME analysis.ipynb\n",
    "%env WANDB_SILENT true\n",
    "%matplotlib agg\n",
    "# ipympl\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import re\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.collections as mpl_col\n",
    "import matplotlib.gridspec as mpl_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import mpl_toolkits.mplot3d as mp3d\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "import data\n",
    "import inept\n",
    "\n",
    "# Get args\n",
    "# import sys\n",
    "# run_id = sys.argv[1]\n",
    "# key = sys.argv[2]\n",
    "# num_nodes_override = int(sys.argv[3])\n",
    "\n",
    "# Set params\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data')\n",
    "PLOT_FOLDER = os.path.join(BASE_FOLDER, '../plots')\n",
    "\n",
    "# Style\n",
    "sns.set_context('paper', font_scale=1.25)\n",
    "sns.set_style('white')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# MPL params\n",
    "mpl.rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "# Disable gradients\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HIGH PRIORITY\n",
    "  - Add more accuracy metrics\n",
    "  - Perturbation analysis with inverse transform\n",
    "  - Add 2D functionality\n",
    "  - Add optional UMAP\n",
    "\n",
    "- LOW PRIORITY\n",
    "  - Switch to `mayavi` instead of mpl to have true 3d and proper layering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load All Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:05:41.360205Z",
     "iopub.status.busy": "2024-12-11T22:05:41.359978Z",
     "iopub.status.idle": "2024-12-11T22:05:43.035550Z",
     "shell.execute_reply": "2024-12-11T22:05:43.035032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy found at stage 14\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "run_id = (\n",
    "    '32jqyk54',  # MERFISH Random 100 Max\n",
    "    'maofk1f2',  # ExSeq NR\n",
    "    'f6ajo2am',  # smFish NR\n",
    "    'vb1x7bae',  # MERFISH NR\n",
    "    '473vyon2',  # ISS NR\n",
    "    '4i9rhkfe',  # ISS Random 200 20k\n",
    "    'k52g4dx3',  # Random 100x (OLD)\n",
    "    '2dt27jy2',  # No random 20k (OLD)\n",
    ")[0]\n",
    "stage_override = None  # Manually override policy stage selection\n",
    "num_nodes_override = 1_000\n",
    "max_nodes_override = 100\n",
    "seed_override = None  # 43\n",
    "\n",
    "# Load run\n",
    "api = wandb.Api()\n",
    "run = api.run(f'oafish/INEPT/{run_id}')\n",
    "config = defaultdict(lambda: {})\n",
    "for k, v in run.config.items():\n",
    "    dict_name, key = k.split('/')\n",
    "    config[dict_name][key] = v\n",
    "config = dict(config)\n",
    "\n",
    "# Reproducibility\n",
    "seed = seed_override if seed_override is not None else config['note']['seed']\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load data\n",
    "modalities, types, features = data.load_data(config['data']['dataset'], DATA_FOLDER)\n",
    "data_dict = config['data']\n",
    "# data_dict = inept.utilities.overwrite_dict(data_dict, {'standardize': True})  # Old model compatibility\n",
    "if num_nodes_override is not None: data_dict = inept.utilities.overwrite_dict(data_dict, {'num_nodes': num_nodes_override})\n",
    "ppc = inept.utilities.Preprocessing(**data_dict, device=DEVICE)\n",
    "modalities = ppc.fit_transform(modalities)\n",
    "modalities, types = ppc.subsample(modalities, types)\n",
    "modalities = ppc.cast(modalities)\n",
    "\n",
    "# Load env\n",
    "env = inept.environments.trajectory(*modalities, **config['env'], device=DEVICE)\n",
    "\n",
    "# Get latest policy\n",
    "latest_mdl = [-1, None]  # Pkl\n",
    "latest_wgt = [-1, None]  # State dict\n",
    "for file in run.files():\n",
    "    # Find mdl files\n",
    "    matches = re.findall(f'^(?:models|trained_models)/policy_(\\w+).(mdl|wgt)$', file.name)\n",
    "    if len(matches) > 0: stage = int(matches[0][0]); ftype = matches[0][1]\n",
    "    else: continue\n",
    "\n",
    "    # Record\n",
    "    latest_known_stage = latest_mdl[0] if ftype == 'mdl' else latest_wgt[0]\n",
    "    if (stage_override is None and stage > latest_known_stage) or (stage_override is not None and stage == stage_override):\n",
    "        if ftype == 'mdl': latest_mdl = [stage, file]\n",
    "        elif ftype == 'wgt': latest_wgt = [stage, file]\n",
    "print(f'Policy found at stage {latest_mdl[0]}')\n",
    "\n",
    "# Load file\n",
    "load_type = 'wgt'\n",
    "if load_type == 'mdl':\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        latest_mdl[1].download(tmpdir, replace=True)\n",
    "        policy = torch.load(os.path.join(tmpdir, latest_mdl[1].name))\n",
    "elif load_type == 'wgt':\n",
    "    # Mainly used in the case of old argument names, also more secure\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        latest_wgt[1].download(tmpdir, replace=True)\n",
    "        config_to_use = config['policy']\n",
    "        # config_to_use = inept.utilities.overwrite_dict(config['policy'], {'positional_dim': 6, 'modal_dims': [76]})  # Old model compatibility\n",
    "        if max_nodes_override is not None: inept.utilities.overwrite_dict(config['policy'], {'max_nodes': max_nodes_override})\n",
    "        policy = inept.models.PPO(**config_to_use)\n",
    "        incompatible_keys = policy.load_state_dict(torch.load(os.path.join(tmpdir, latest_wgt[1].name), weights_only=True))\n",
    "policy = policy.to(DEVICE).eval()\n",
    "policy.actor.set_action_std(1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:05:43.037838Z",
     "iopub.status.busy": "2024-12-11T22:05:43.037560Z",
     "iopub.status.idle": "2024-12-11T22:05:43.058961Z",
     "shell.execute_reply": "2024-12-11T22:05:43.058520Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Standardize implementation\n",
    "labels = types[0][:, 0]\n",
    "times = types[0][:, 0]  # Temporary time annotation, will change per-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:05:43.060936Z",
     "iopub.status.busy": "2024-12-11T22:05:43.060791Z",
     "iopub.status.idle": "2024-12-11T22:05:43.081784Z",
     "shell.execute_reply": "2024-12-11T22:05:43.081376Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose key\n",
    "# TODO: Calculate both, plot one (?)\n",
    "analysis_key = ['discovery', 'temporal'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:05:43.083736Z",
     "iopub.status.busy": "2024-12-11T22:05:43.083573Z",
     "iopub.status.idle": "2024-12-11T22:05:43.105549Z",
     "shell.execute_reply": "2024-12-11T22:05:43.105143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize memories\n",
    "memories = {}\n",
    "\n",
    "# Default present function\n",
    "def get_present_default(\n",
    "    *args,\n",
    "    timestep,\n",
    "    **kwargs,\n",
    "):\n",
    "    return torch.ones(modalities[0].shape[0], dtype=bool), timestep+1 >= config['train']['max_ep_timesteps']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:05:43.107607Z",
     "iopub.status.busy": "2024-12-11T22:05:43.107452Z",
     "iopub.status.idle": "2024-12-11T22:05:43.130913Z",
     "shell.execute_reply": "2024-12-11T22:05:43.130468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deployment list\n",
    "deployment = [None]\n",
    "# Reverse alphabetical (ExSeq, MERFISH, smFISH, ISS, MouseVisual)\n",
    "type_order = np.unique(labels)[::-1]\n",
    "deployment_general = {\n",
    "    'labels': list(type_order),\n",
    "    'delay': 50*np.arange(len(type_order)),\n",
    "    'rates': [1] + [.015]*(len(type_order)-1),\n",
    "    'origins': [None] + list(type_order[:-1])}\n",
    "deployment += [deployment_general]\n",
    "\n",
    "# Choose Deployment\n",
    "deployment = deployment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:05:43.132872Z",
     "iopub.status.busy": "2024-12-11T22:05:43.132726Z",
     "iopub.status.idle": "2024-12-11T22:05:43.155574Z",
     "shell.execute_reply": "2024-12-11T22:05:43.155157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "# Takes in combination of variables, outputs present, end\n",
    "def get_present_deployment(\n",
    "    *args,\n",
    "    env,\n",
    "    timestep,\n",
    "    present,\n",
    "    labels,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Copy status\n",
    "    present = present.clone()\n",
    "    state = env.get_state().clone()\n",
    "\n",
    "    # Iterate over each label\n",
    "    for label, delay, rate, origin in zip(*deployment.values()):\n",
    "        # If delay has been reached\n",
    "        if timestep >= delay:\n",
    "            # Look at each node\n",
    "            for i in range(len(present)):\n",
    "                # If label matches and not already present\n",
    "                if labels[i] == label and not present[i]:\n",
    "                    # Roll for appearance\n",
    "                    if np.random.rand() < rate:\n",
    "                        # Mark as present and set origin\n",
    "                        if origin is not None:\n",
    "                            state[i] = state[np.random.choice(np.argwhere((labels==origin)*present.cpu().numpy()).flatten())]\n",
    "                        present[i] = True\n",
    "\n",
    "    # Return\n",
    "    env.set_state(state)\n",
    "    return present, timestep+1 >= config['train']['max_ep_timesteps']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:05:43.157630Z",
     "iopub.status.busy": "2024-12-11T22:05:43.157476Z",
     "iopub.status.idle": "2024-12-11T22:05:43.180289Z",
     "shell.execute_reply": "2024-12-11T22:05:43.179857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stage order list\n",
    "temporal = [None]\n",
    "# Reverse alphabetical (ExSeq, MERFISH, smFISH, ISS, MouseVisual)\n",
    "temporal_general = {'stages': [[l] for l in np.unique(labels)[::-1]]}\n",
    "temporal += [temporal_general]\n",
    "\n",
    "# Choose stage order\n",
    "temporal = temporal[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:05:43.182246Z",
     "iopub.status.busy": "2024-12-11T22:05:43.182092Z",
     "iopub.status.idle": "2024-12-11T22:05:43.204750Z",
     "shell.execute_reply": "2024-12-11T22:05:43.204370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "current_stage = None  # TODO: Move into class\n",
    "stage_start = 0\n",
    "max_stage_len = 500\n",
    "\n",
    "def get_present_temporal(\n",
    "    *args,\n",
    "    timestep,\n",
    "    env,\n",
    "    times,  # np.array\n",
    "    present,\n",
    "    vel_threshold=3e-2,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Clone data\n",
    "    present = present.clone()\n",
    "    state = env.get_state().clone()\n",
    "\n",
    "    # Defaults\n",
    "    global current_stage, stage_start\n",
    "    if timestep == 0:\n",
    "        current_stage = 0\n",
    "        stage_start = 0\n",
    "\n",
    "    # Initiate change if vel is low\n",
    "    if present.sum() > 0: vel_threshold_met = state[present, env.dim:].square().sum(dim=-1).sqrt().max(dim=-1).values < vel_threshold\n",
    "    else: vel_threshold_met = False\n",
    "\n",
    "    update = vel_threshold_met or timestep - stage_start >= max_stage_len\n",
    "    if update:\n",
    "        # Make change to next stage\n",
    "        current_stage += 1\n",
    "        stage_start = timestep\n",
    "        if current_stage >= len(temporal['stages']): return present, True\n",
    "    \n",
    "    # Update present if needed\n",
    "    if update or timestep == 0:\n",
    "        present = torch.tensor(np.isin(times, temporal['stages'][current_stage]))\n",
    "    \n",
    "    return present, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:05:43.206564Z",
     "iopub.status.busy": "2024-12-11T22:05:43.206409Z",
     "iopub.status.idle": "2024-12-11T22:06:56.467532Z",
     "shell.execute_reply": "2024-12-11T22:06:56.467046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 20\r",
      "Timestep: 40\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 60\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 80\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 100\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 120\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 140\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 160\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 180\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 200\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 220\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 240\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 260\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 280\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 300\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 320\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 340\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 360\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 380\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 400\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 420\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 440\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 460\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 480\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 500\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 520\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 540\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 560\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 580\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 600\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 620\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 640\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 660\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 680\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 700\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 720\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 740\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 760\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 780\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 800\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 820\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 840\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 860\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 880\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 900\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 920\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 940\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 960\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 980\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose key\n",
    "get_present_dict = {\n",
    "    'discovery': get_present_deployment if deployment is not None else get_present_default,\n",
    "    'temporal': get_present_temporal if temporal is not None else get_present_default,\n",
    "}\n",
    "get_present_func = get_present_dict[analysis_key]\n",
    "\n",
    "# Initialize\n",
    "env.reset(); memories[analysis_key] = defaultdict(lambda: [])\n",
    "\n",
    "# Modify\n",
    "present = torch.zeros(modalities[0].shape[0], dtype=bool, device=DEVICE)\n",
    "present, _ = get_present_func(\n",
    "    env=env,\n",
    "    timestep=0,\n",
    "    present=present,\n",
    "    labels=labels,\n",
    "    times=times,\n",
    "    deployment=deployment,\n",
    ")\n",
    "\n",
    "# Continue initializing\n",
    "memories[analysis_key]['present'].append(present)\n",
    "memories[analysis_key]['states'].append(env.get_state())\n",
    "memories[analysis_key]['rewards'].append(torch.zeros(modalities[0].shape[0], device=DEVICE))\n",
    "\n",
    "# Simulate\n",
    "timestep = 1\n",
    "while True:\n",
    "    # CLI\n",
    "    if timestep % 20 == 0:\n",
    "        cli_out = f'Timestep: {timestep}'\n",
    "        if analysis_key == 'temporal': cli_out += f' - Stage: {current_stage}'\n",
    "        print(cli_out, end='\\r')\n",
    "\n",
    "    # Step\n",
    "    state = env.get_state(include_modalities=True)\n",
    "    actions = torch.zeros((modalities[0].shape[0], env.dim), device=DEVICE)\n",
    "    actions[present] = policy.act_macro(\n",
    "        state[present],\n",
    "        keys=torch.arange(modalities[0].shape[0], device=DEVICE)[present],\n",
    "        max_batch=config['train']['max_batch'],\n",
    "    )\n",
    "    rewards = torch.zeros(modalities[0].shape[0])\n",
    "    # TODO: Currently, rewards factor in non-present nodes\n",
    "    rewards, _, _ = env.step(actions, return_itemized_rewards=True)\n",
    "    new_state = env.get_state()\n",
    "    new_state[~present] = state[~present, :2*env.dim]  # Don't move un-spawned nodes\n",
    "    env.set_state(new_state)\n",
    "\n",
    "    # Record\n",
    "    memories[analysis_key]['present'].append(present)\n",
    "    memories[analysis_key]['states'].append(env.get_state())\n",
    "    memories[analysis_key]['rewards'].append(rewards)\n",
    "\n",
    "    # Modify\n",
    "    present, end = get_present_func(\n",
    "        env=env,\n",
    "        timestep=timestep, \n",
    "        present=present, \n",
    "        labels=labels,\n",
    "        times=times,\n",
    "        deployment=deployment,\n",
    "    )\n",
    "\n",
    "    # End\n",
    "    if end: break\n",
    "    timestep += 1\n",
    "\n",
    "# Stack\n",
    "memories[analysis_key]['present'] = torch.stack(memories[analysis_key]['present'])\n",
    "memories[analysis_key]['states'] = torch.stack(memories[analysis_key]['states'])\n",
    "memories[analysis_key]['rewards'] = torch.stack(memories[analysis_key]['rewards'])\n",
    "memories[analysis_key] = dict(memories[analysis_key])\n",
    "\n",
    "# CLI\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T22:06:56.469638Z",
     "iopub.status.busy": "2024-12-11T22:06:56.469496Z",
     "iopub.status.idle": "2024-12-11T22:41:20.427142Z",
     "shell.execute_reply": "2024-12-11T22:41:20.426352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 / 79\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 / 79\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 79\r"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "skip = 10\n",
    "present = memories[analysis_key]['present'].cpu()[::skip]\n",
    "states = memories[analysis_key]['states'].cpu()[::skip]\n",
    "rewards = memories[analysis_key]['rewards'].cpu()[::skip]\n",
    "env.set_modalities(modalities)\n",
    "env.reset()\n",
    "env.get_distance_match()\n",
    "modal_dist = env.dist\n",
    "\n",
    "# Parameters\n",
    "interval = 1e3*env.delta/3  # Time between frames (3x speedup)\n",
    "min_max_vel = 0 if analysis_key == 'temporal' else 1e-2  # Stop at first frame all vels are below target. 0 for full play\n",
    "frame_override = None  # Manually enter number of frames to draw\n",
    "num_lines = 25  # Number of attraction and repulsion lines\n",
    "rotations_per_second = .1  # Camera azimuthal rotations per second\n",
    "\n",
    "# Create plot\n",
    "figsize = (17, 10)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "# grid = mpl_grid.GridSpec(1, 2, width_ratios=(2, 1))\n",
    "# ax1 = fig.add_subplot(grid[0], projection='3d')\n",
    "# ax2 = fig.add_subplot(grid[1])\n",
    "# fig.tight_layout(pad=2)\n",
    "ax1 = fig.add_axes([1 /figsize[0], 1 /figsize[1], 8 /figsize[0], 8 /figsize[1]], projection='3d')\n",
    "ax2 = fig.add_axes([12 /figsize[0], 1 /figsize[1], 4 /figsize[0], 8 /figsize[1]])\n",
    "\n",
    "# Initialize nodes\n",
    "get_node_data = lambda frame: states[frame, :, :3]\n",
    "nodes = [\n",
    "    ax1.plot(\n",
    "        # *get_node_data(0)[labels==l].T,\n",
    "        [], [],\n",
    "        label=l,\n",
    "        linestyle='',\n",
    "        marker='o',\n",
    "        ms=6,\n",
    "        zorder=2.3,\n",
    "    )[0]\n",
    "    for l in np.unique(labels)\n",
    "]\n",
    "\n",
    "# Initialize velocity arrows\n",
    "arrow_length_scale = 1\n",
    "get_arrow_xyz_uvw = lambda frame: (states[frame, :, :3], states[frame, :, env.dim:env.dim+3])\n",
    "arrows = ax1.quiver(\n",
    "    [], [], [],\n",
    "    [], [], [],\n",
    "    arrow_length_ratio=0,\n",
    "    length=arrow_length_scale,\n",
    "    lw=2,\n",
    "    color='gray',\n",
    "    alpha=.4,\n",
    "    zorder=2.2,\n",
    ")\n",
    "\n",
    "# Initialize modal lines\n",
    "# relative_connection_strength = [np.array([(1-dist[j, k].item()/dist.max().item())**2 for j, k in product(*[range(s) for s in dist.shape]) if j < k]) for dist in modal_dist]\n",
    "get_distance_discrepancy = lambda frame: [np.array([((states[frame, j, :3] - states[frame, k, :3]).square().sum().sqrt() - dist[j, k].cpu()).item() for j, k in product(*[range(s) for s in dist.shape]) if j < k]) for dist in modal_dist]\n",
    "get_modal_lines_segments = lambda frame, dist: np.array(states[frame, [[j, k] for j, k in product(*[range(s) for s in dist.shape]) if j < k], :3])\n",
    "clip_dd_to_alpha = lambda dd: np.clip(np.abs(dd), 0, 2) / 2\n",
    "# Randomly select lines to show\n",
    "line_indices = [[j, k] for j, k in product(*[range(s) for s in modal_dist[0].shape]) if j < k]\n",
    "total_lines = int((modal_dist[0].shape[0]**2 - modal_dist[0].shape[0]) / 2)  # Only considers first modality\n",
    "line_selection = [\n",
    "    np.random.choice(total_lines, num_lines, replace=False) if num_lines is not None else list(range(total_lines)) for dist in modal_dist\n",
    "]\n",
    "modal_lines = [\n",
    "    mp3d.art3d.Line3DCollection(\n",
    "        get_modal_lines_segments(0, dist)[line_selection[i]],\n",
    "        label=f'Modality {i}',\n",
    "        lw=2,\n",
    "        zorder=2.1,\n",
    "    )\n",
    "    for i, dist in enumerate(modal_dist)\n",
    "]\n",
    "for ml in modal_lines: ax1.add_collection(ml)\n",
    "\n",
    "# Silhouette scoring\n",
    "if analysis_key == 'discovery':\n",
    "    # TODO: Update from 3 to env.dim\n",
    "    get_silhouette_samples = lambda frame: sklearn.metrics.silhouette_samples(states[frame, :, :3].cpu(), labels)\n",
    "    bars = [ax2.bar(l, 0) for l in np.unique(labels)]\n",
    "    ax2.axhline(y=0, color='black')\n",
    "    ax2.set(ylim=(-1, 1))\n",
    "\n",
    "# Temporal comparison\n",
    "elif analysis_key == 'temporal':\n",
    "    def get_temporal_discrepancy(frame, recalculate=True):\n",
    "        if recalculate: env.set_modalities([m[present[frame], :] for m in modalities])\n",
    "        env.set_positions(states[frame, present[frame], :env.dim].to(DEVICE))\n",
    "        return float(env.get_distance_match().mean().detach().cpu())\n",
    "    temporal_eval_plot = ax2.plot([], [], color='black', marker='o')[0]\n",
    "    # TODO: Highlight training regions\n",
    "    num_stages = len(temporal['stages'])  # present.unique(dim=0).shape[0]\n",
    "\n",
    "    # Styling\n",
    "    ax2.set_xticks(np.arange(num_stages), temporal['stages'])\n",
    "    ax2.set_xlim([-.5, num_stages-.5])\n",
    "    ax2.set_ylim([0, 1e0])\n",
    "    ax2.set_title('Temporal Discrepancy')\n",
    "    # ax2.set_yscale('symlog')\n",
    "\n",
    "# Limits\n",
    "# TODO: Double-check that the `present` indexing works\n",
    "ax1.set(\n",
    "    xlim=(states[present][:, 0].min(), states[present][:, 0].max()),\n",
    "    ylim=(states[present][:, 1].min(), states[present][:, 1].max()),\n",
    "    zlim=(states[present][:, 2].min(), states[present][:, 2].max()),\n",
    ")\n",
    "\n",
    "# Legends\n",
    "l1 = ax1.legend(handles=nodes, loc='upper left')\n",
    "ax1.add_artist(l1)\n",
    "l2 = ax1.legend(handles=[\n",
    "    ax1.plot([], [], color='red', label='Repulsion')[0],\n",
    "    ax1.plot([], [], color='blue', label='Attraction')[0],\n",
    "], loc='upper right')\n",
    "ax1.add_artist(l2)\n",
    "ax2.spines[['right', 'top', 'bottom', 'left']].set_visible(False)\n",
    "\n",
    "# Styling\n",
    "ax1.set(xlabel='x', ylabel='y', zlabel='z')\n",
    "get_angle = lambda frame: (30, (360*rotations_per_second)*(frame*interval/1000)-60, 0)\n",
    "ax1.view_init(*get_angle(0))\n",
    "\n",
    "# Update function\n",
    "def update(frame):\n",
    "    # Adjust nodes\n",
    "    for i, l in enumerate(np.unique(labels)):\n",
    "        present_labels = present[frame] * torch.tensor(labels==l)\n",
    "        data = get_node_data(frame)[present_labels].T\n",
    "        nodes[i].set_data(*data[:2])\n",
    "        nodes[i].set_3d_properties(data[2])\n",
    "\n",
    "    # Adjust arrows\n",
    "    xyz_xyz = [[xyz, xyz+arrow_length_scale*uvw] for i, (xyz, uvw) in enumerate(zip(*get_arrow_xyz_uvw(frame))) if present[frame, i]]\n",
    "    arrows.set_segments(xyz_xyz)\n",
    "\n",
    "    # Adjust lines\n",
    "    for i, (dist, ml) in enumerate(zip(modal_dist, modal_lines)):\n",
    "        ml.set_segments(get_modal_lines_segments(frame, dist)[line_selection[i]])\n",
    "        distance_discrepancy = get_distance_discrepancy(frame)[i][line_selection[i]]\n",
    "        color = np.array([(0., 0., 1.) if dd > 0 else (1., 0., 0.) for dd in distance_discrepancy])\n",
    "        alpha = np.expand_dims(clip_dd_to_alpha(distance_discrepancy), -1)\n",
    "        for j, line_index in enumerate(line_selection[i]):\n",
    "            if not present[frame, line_indices[line_index]].all(): alpha[j] = 0.\n",
    "        ml.set_color(np.concatenate((color, alpha), axis=-1))\n",
    "\n",
    "    # Barplots\n",
    "    if analysis_key == 'discovery':\n",
    "        for bar, l in zip(bars, np.unique(labels)):\n",
    "            bar[0].set_height(get_silhouette_samples(frame)[labels==l].mean())\n",
    "\n",
    "        # Styling\n",
    "        ax2.set_title(f'Silhouette Coefficient : {get_silhouette_samples(frame).mean():5.2f}') \n",
    "\n",
    "    # Line plots\n",
    "    elif analysis_key == 'temporal':\n",
    "        # Defaults\n",
    "        global current_stage  # TODO: Find better solution\n",
    "\n",
    "        # Calculate discrepancy using env\n",
    "        recalculate = (frame == 0) or not (present[frame] == present[frame-1]).all()  # Only recalculate dist if needed\n",
    "        discrepancy = get_temporal_discrepancy(frame, recalculate=recalculate)\n",
    "\n",
    "        # Adjust plot\n",
    "        xdata = temporal_eval_plot.get_xdata()\n",
    "        ydata = temporal_eval_plot.get_ydata()\n",
    "        if not ((frame == 0 and len(xdata) > 0)):\n",
    "            if frame == 0: current_stage = 0\n",
    "            if recalculate:\n",
    "                xdata = np.append(xdata, current_stage)\n",
    "                ydata = np.append(ydata, None)\n",
    "                current_stage += 1  # Technically one ahead\n",
    "            ydata[-1] = discrepancy\n",
    "            temporal_eval_plot.set_xdata(xdata)\n",
    "            temporal_eval_plot.set_ydata(ydata)\n",
    "\n",
    "    # Styling\n",
    "    ax1.set_title(f'{skip*frame: 4} : {rewards[frame].mean():5.2f}')  \n",
    "    ax1.view_init(*get_angle(frame))\n",
    "\n",
    "    # CLI\n",
    "    print(f'{frame} / {frames-1}', end='\\r')\n",
    "    if frame == frames-1: print()\n",
    "\n",
    "# Compile animation\n",
    "frames = states[..., env.dim:env.dim+3].square().sum(dim=-1).sqrt().max(dim=-1).values < min_max_vel\n",
    "frames = np.array([(frames[i] or frames[i+1]) if i != len(frames)-1 else frames[i] for i in range(len(frames))])  # Disregard interrupted sections of low movement\n",
    "frames = np.argwhere(frames)\n",
    "frames = frames[0, 0].item()+1 if len(frames) > 0 else states.shape[0]\n",
    "frames = frames if frame_override is None else frame_override\n",
    "ani = animation.FuncAnimation(\n",
    "    fig=fig,\n",
    "    func=update,\n",
    "    frames=frames,\n",
    "    interval=interval,\n",
    ")\n",
    "\n",
    "# Display animation as it renders\n",
    "# plt.show()\n",
    "\n",
    "# Display complete animation\n",
    "# from IPython.display import HTML\n",
    "# HTML(ani.to_jshtml())\n",
    "\n",
    "# Save animation\n",
    "file_type = 'mp4'\n",
    "if file_type == 'mp4': writer = animation.FFMpegWriter(fps=int(1e3/interval), extra_args=['-vcodec', 'libx264'], bitrate=8e3)  # Faster\n",
    "elif file_type == 'gif': writer = animation.FFMpegWriter(fps=int(1e3/interval))  # Slower\n",
    "ani.save(os.path.join(PLOT_FOLDER, f'{config[\"data\"][\"dataset\"]}_{analysis_key}.{file_type}'), writer=writer, dpi=300)\n",
    "\n",
    "# CLI\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
