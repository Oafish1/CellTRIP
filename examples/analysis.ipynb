{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T18:27:18.898240Z",
     "iopub.status.busy": "2025-01-15T18:27:18.897819Z",
     "iopub.status.idle": "2025-01-15T18:27:21.049625Z",
     "shell.execute_reply": "2025-01-15T18:27:21.049119Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env WANDB_NOTEBOOK_NAME analysis.ipynb\n",
    "%env WANDB_SILENT true\n",
    "%matplotlib agg\n",
    "# ipympl\n",
    "\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import sklearn.neighbors\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "# Enable text output in notebooks\n",
    "import tqdm.auto\n",
    "import tqdm.notebook\n",
    "tqdm.notebook.tqdm = tqdm.auto.tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import data\n",
    "import celltrip\n",
    "\n",
    "# Set params\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data')\n",
    "PLOT_FOLDER = os.path.join(BASE_FOLDER, '../plots')\n",
    "\n",
    "# Style\n",
    "sns.set_context('paper', font_scale=1.25)\n",
    "sns.set_style('white')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# MPL params\n",
    "mpl.rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "# Disable gradients\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO\n",
    "  - Maybe filter loss plot to selected stage?\n",
    "  - Add outdir, datadir, etc.\n",
    "  - Get file location rather than `abspath`\n",
    "  - Perturbation analysis for all features, filtered to only important ones, as well as .txt output\n",
    "  - Synchronize colors for each method\n",
    "  - Add perturbation and trajectory\n",
    "  - Try using known reference points (i.e. positional data) to impute absolute, rather than relative, values\n",
    "  - Maybe add buffer to perturbation analysis start\n",
    "  - Add arguments like wandb username/project, etc. as well as local db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Create a video of the specified CellTRIP model')\n",
    "\n",
    "# Main parameters\n",
    "group = parser.add_argument_group('General')\n",
    "group.add_argument('run_id', type=str, help='Run ID from WandB to use for processing')\n",
    "group.add_argument('analysis_key', choices=('convergence', 'discovery', 'temporal', 'perturbation'), nargs='+', type=str, help='Type of analyses to perform (one or more)')\n",
    "group.add_argument('-S', '--seed', type=int, help='Override simulation seed')\n",
    "group.add_argument('--gpu', default='0', type=str, help='GPU(s) to use')\n",
    "\n",
    "# Model parameters\n",
    "group = parser.add_argument_group('Simulation')\n",
    "group.add_argument('-b', '--batch', metavar='MAX_BATCH', dest='max_batch', type=int, help='Override number of nodes which can calculate actions simultaneously')\n",
    "group.add_argument('--num', metavar='NUM_NODES', dest='num_nodes', type=int, help='Override number of nodes to take from data')\n",
    "group.add_argument('--nodes', metavar='NUM_NEIGHBORS', dest='max_nodes', type=int, help='Override neighbors considered by each node')\n",
    "group.add_argument('--stage', type=int, help='Override model stage to use. 0 is random initialization')\n",
    "\n",
    "# Simulation specific arguments\n",
    "group = parser.add_argument_group('Analysis')\n",
    "group.add_argument('--discovery_key', type=int, default=0, help='Type of discovery analysis (0: Auto)')\n",
    "group.add_argument('--temporal_key', type=int, default=0, help='Type of temporal analysis (0: Auto, 1: TemporalBrain)')\n",
    "group.add_argument('--force', action='store_true', help='Rerun analysis even if already stored in memory')\n",
    "\n",
    "# Video parameters\n",
    "group = parser.add_argument_group('Video')\n",
    "group.add_argument('--novid', action='store_true', help='Skip video generation')\n",
    "group.add_argument('-g', '--gif', action='store_true', help='Output as a GIF rather than MP4')\n",
    "group.add_argument('-s', '--skip', type=int, default=5, help='Number of steps to advance each frame')\n",
    "group.add_argument('--reduction', choices=('umap', 'pca', 'none'), default='pca', type=str, dest='reduction_type', help='Reduction type to use for high-dimensional projections in 3D visualization')\n",
    "group.add_argument('--force_reduction', action='store_true', help='Force reduction, even if unnecessary')\n",
    "group.add_argument('--reduction_batch', type=int, default=100_000, help='Max number of states to reduce in one computation')\n",
    "\n",
    "# Legacy compatibility\n",
    "group = parser.add_argument_group('Legacy Compatiiblity')\n",
    "group.add_argument('--total_statistics', action='store_true', help='Compatibility argument to compute mean and variance over all samples')\n",
    "\n",
    "# List of common runs\n",
    "# 'brf6n6sn': TemporalBrain Random 100 Max\n",
    "# 'rypltvk5': MMD-MA Random 100 Max (requires `total_statistics`)\n",
    "# '32jqyk54': MERFISH Random 100 Max\n",
    "# 'c8zsunc9': ISS Random 100 Max\n",
    "# 'maofk1f2': ExSeq NR\n",
    "# 'f6ajo2am': smFish NR\n",
    "# 'vb1x7bae': MERFISH NR\n",
    "# '473vyon2': ISS NR\n",
    "\n",
    "# Notebook defaults and script handling\n",
    "if not celltrip.utilities.is_notebook():\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args('--novid --total_statistics rypltvk5 convergence discovery temporal perturbation'.split(' '))  # MMD-MA\n",
    "    # args = parser.parse_args('--novid 32jqyk54 convergence discovery temporal perturbation'.split(' '))  # MERFISH\n",
    "    # args = parser.parse_args('--novid c8zsunc9 convergence discovery temporal perturbation'.split(' '))  # ISS\n",
    "\n",
    "# Set env vars\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=args.gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data, Model, and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T18:27:21.052179Z",
     "iopub.status.busy": "2025-01-15T18:27:21.051926Z",
     "iopub.status.idle": "2025-01-15T18:27:24.431802Z",
     "shell.execute_reply": "2025-01-15T18:27:24.431274Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load run\n",
    "print(f'Loading run {args.run_id}')\n",
    "api = wandb.Api()\n",
    "run = api.run(f'oafish/CellTRIP/{args.run_id}')\n",
    "config = defaultdict(lambda: {})\n",
    "for k, v in run.config.items():\n",
    "    dict_name, key = k.split('/')\n",
    "    config[dict_name][key] = v\n",
    "config = dict(config)\n",
    "\n",
    "# Reproducibility\n",
    "notebook_seed = args.seed if args.seed is not None else config['note']['seed']  # Potentially destructive if randomly sampled\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "torch.manual_seed(notebook_seed)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed(notebook_seed)\n",
    "np.random.seed(notebook_seed)\n",
    "\n",
    "# Get latest policy\n",
    "print('\\tFinding model')\n",
    "latest_mdl = [0, None]  # Pkl\n",
    "latest_wgt = [0, None]  # State dict\n",
    "# Compatibility with models of the previous naming convention\n",
    "# for file in run.files():\n",
    "#     matches = re.findall(f'^(?:models|trained_models)/policy_(\\w+).(mdl|wgt)$', file.name)\n",
    "#     if len(matches) > 0: stage = int(matches[0][0]); ftype = matches[0][1]\n",
    "#     else: continue\n",
    "#     if stage == 0: add_one = True; break\n",
    "# else: add_one = False\n",
    "# Iterate through model files\n",
    "for file in run.files():\n",
    "    matches = re.findall(f'^(?:models|trained_models)/policy_(\\w+).(mdl|wgt)$', file.name)\n",
    "    if len(matches) > 0: stage = int(matches[0][0]); ftype = matches[0][1]\n",
    "    else: continue\n",
    "    # if add_one: stage += 1\n",
    "\n",
    "    # Record\n",
    "    latest_known_stage = latest_mdl[0] if ftype == 'mdl' else latest_wgt[0]\n",
    "    if (args.stage is None and stage > latest_known_stage) or (args.stage is not None and stage == args.stage):\n",
    "        if ftype == 'mdl': latest_mdl = [stage, file]\n",
    "        elif ftype == 'wgt': latest_wgt = [stage, file]\n",
    "print(f'\\t\\tMDL policy found at stage {latest_mdl[0]}')\n",
    "print(f'\\t\\tWGT policy found at stage {latest_wgt[0]}')\n",
    "\n",
    "# Load data\n",
    "print(f'\\tLoading dataset {config[\"data\"][\"dataset\"]}')\n",
    "modalities, types, features = data.load_data(config['data']['dataset'], DATA_FOLDER)\n",
    "# config['data'] = celltrip.utilities.overwrite_dict(config['data'], {'standardize': True})  # Old model compatibility\n",
    "# config['data'] = celltrip.utilities.overwrite_dict(config['data'], {'top_variant': config['data']['pca_dim'], 'pca_dim': None})  # Swap PCA with top variant (testing)\n",
    "if args.num_nodes is not None: config['data'] = celltrip.utilities.overwrite_dict(config['data'], {'num_nodes': args.num_nodes})\n",
    "if args.max_batch is not None: config['train'] = celltrip.utilities.overwrite_dict(config['train'], {'max_batch': args.max_batch})\n",
    "for k in ('standardize', 'pca_dim', 'top_variant'):\n",
    "    # Legacy compatibility for missing default arguments\n",
    "    if k not in config['data']: config['data'][k] = None\n",
    "ppc = celltrip.utilities.Preprocessing(**config['data'], device=DEVICE)\n",
    "modalities, features = ppc.fit_transform(modalities, features, total_statistics=args.total_statistics)\n",
    "modalities, types = ppc.subsample(modalities, types)\n",
    "modalities = ppc.cast(modalities)\n",
    "labels = types[0][:, 0]\n",
    "times = types[0][:, -1]\n",
    "\n",
    "# Load env\n",
    "env = celltrip.environments.trajectory(*modalities, **config['env'], **config['stages']['env'][0], device=DEVICE)\n",
    "for weight_stage in config['stages']['env'][1:latest_mdl[0]+1]:\n",
    "    env.set_rewards(weight_stage)\n",
    "application_type = 'integration' if len(env.reward_distance_target) == len(modalities) else 'imputation'\n",
    "\n",
    "# Load model file\n",
    "load_type = 'WGT'\n",
    "if load_type == 'MDL' and latest_mdl[0] != 0:\n",
    "    print('\\tLoading MDL model')\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        latest_mdl[1].download(tmpdir, replace=True)\n",
    "        policy = torch.load(os.path.join(tmpdir, latest_mdl[1].name))\n",
    "elif load_type == 'WGT' and latest_wgt[0] != 0:\n",
    "    print('\\tLoading WGT model')\n",
    "    # Mainly used in the case of old argument names, also generally more secure\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        latest_wgt[1].download(tmpdir, replace=True)\n",
    "        # config['policy'] = celltrip.utilities.overwrite_dict(config['policy'], {'positional_dim': 6, 'modal_dims': [76]})  # Old model compatibility\n",
    "        if args.max_nodes is not None: config['policy'] = celltrip.utilities.overwrite_dict(config['policy'], {'max_nodes': args.max_nodes})\n",
    "        policy = celltrip.models.PPO(**config['policy'])\n",
    "        incompatible_keys = policy.load_state_dict(torch.load(os.path.join(tmpdir, latest_wgt[1].name), weights_only=True))\n",
    "else:\n",
    "    print('\\tGenerating random model')\n",
    "    # Use random model\n",
    "    policy = celltrip.models.PPO(**config['policy'])\n",
    "policy = policy.to(DEVICE).eval()\n",
    "policy.actor.set_action_std(1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T18:27:25.129441Z",
     "iopub.status.busy": "2025-01-15T18:27:25.129271Z",
     "iopub.status.idle": "2025-01-15T18:27:25.155699Z",
     "shell.execute_reply": "2025-01-15T18:27:25.155265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose key\n",
    "optimize_memory = True  # Saves memory by shrinking env based on present, also fixes reward calculation for non-full present mask\n",
    "perturbation_features = [np.random.choice(len(fs), 10, replace=False) for i, fs in enumerate(features) if (i not in env.reward_distance_target) or (len(env.reward_distance_target) == len(modalities))]\n",
    "\n",
    "# Define matching state manager classes\n",
    "state_manager_class = {\n",
    "    'convergence': celltrip.utilities.ConvergenceStateManager,\n",
    "    'discovery': celltrip.utilities.DiscoveryStateManager,\n",
    "    'temporal': celltrip.utilities.TemporalStateManager,\n",
    "    'perturbation': celltrip.utilities.PerturbationStateManager,\n",
    "}\n",
    "\n",
    "# Discovery list\n",
    "discovery = []\n",
    "# Reverse alphabetical (ExSeq, MERFISH, smFISH, ISS, MouseVisual)\n",
    "type_order = np.unique(labels)[::-1]\n",
    "discovery_general = {\n",
    "    'labels': list(type_order),\n",
    "    'delay': 50*np.arange(len(type_order)),\n",
    "    'rates': [1] + [.015]*(len(type_order)-1),\n",
    "    'origins': [None] + list(type_order[:-1])}\n",
    "discovery += [discovery_general]\n",
    "# Choose Discovery\n",
    "discovery = discovery[args.discovery_key]\n",
    "\n",
    "# Stage order list\n",
    "temporal = []\n",
    "# Reverse alphabetical (ExSeq, MERFISH, smFISH, ISS, MouseVisual)\n",
    "temporal_general = {'stages': [[l] for l in np.unique(times)[::-1]]}\n",
    "temporal_temporalBrain = {'stages': [\n",
    "    ['EaFet1'],\n",
    "    ['EaFet2'],\n",
    "    ['LaFet1'],\n",
    "    ['LaFet2'],\n",
    "    ['Inf1'],\n",
    "    ['Inf2'],\n",
    "    ['Child1'],\n",
    "    ['Child2'],\n",
    "    ['Adol1'],\n",
    "    ['Adol2'],\n",
    "    ['Adult1'],\n",
    "    ['Adult2'],\n",
    "]}\n",
    "temporal += [temporal_general]\n",
    "temporal += [temporal_temporalBrain]\n",
    "# Choose stage order\n",
    "temporal = temporal[args.temporal_key]\n",
    "\n",
    "# Perturbation feature names\n",
    "perturbation_feature_names = [[fnames[pf] for pf in pfs] for pfs, fnames in zip(perturbation_features, features)]\n",
    "\n",
    "# Initialize memories\n",
    "memories = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T18:27:25.157740Z",
     "iopub.status.busy": "2025-01-15T18:27:25.157597Z",
     "iopub.status.idle": "2025-01-15T18:27:48.169953Z",
     "shell.execute_reply": "2025-01-15T18:27:48.169487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load memories\n",
    "fname =                                     f'{args.run_id}'\n",
    "if args.stage is not None: fname +=         f'_{args.stage:02}'\n",
    "fname +=                                    f'_{config[\"data\"][\"dataset\"]}'\n",
    "fname +=                                    f'_memories.pkl.gzip'\n",
    "\n",
    "# Load memories\n",
    "if os.path.exists(fname):\n",
    "    print('Loading existing memories')\n",
    "    with gzip.open(fname, 'rb') as f: memories = pickle.load(f)\n",
    "\n",
    "# Run simulation if needed\n",
    "for ak in args.analysis_key:\n",
    "    if ak not in memories or args.force:\n",
    "        print(f'Running {ak} simulation')\n",
    "\n",
    "        # Profiling\n",
    "        profile = False\n",
    "        if profile: torch.cuda.memory._record_memory_history(max_entries=100000)\n",
    "\n",
    "        # Choose state manager\n",
    "        state_manager = state_manager_class[ak](\n",
    "            device=DEVICE,\n",
    "            discovery=discovery,\n",
    "            temporal=temporal,\n",
    "            perturbation_features=perturbation_features,\n",
    "            modal_targets=env.reward_distance_target,\n",
    "            num_nodes=modalities[0].shape[0],\n",
    "            dim=env.dim,\n",
    "            # vel_threshold=1e-1,  # Temporal testing\n",
    "        )\n",
    "\n",
    "        # Utility parameters\n",
    "        get_current_stage = lambda: (\n",
    "            state_manager.current_stage\n",
    "            if np.array([ak in akt for akt in ('temporal', 'perturbation')]).any()\n",
    "            else -1\n",
    "        )\n",
    "        get_max_stage = lambda: (\n",
    "            len(temporal['stages'])-1 if ak == 'temporal'\n",
    "            else sum([len(pf) for pf in perturbation_features])+1 if ak == 'perturbation'\n",
    "            else -1\n",
    "        )\n",
    "        # TODO: Make perturbation more memory-efficient\n",
    "        use_modalities = np.array([ak in akt for akt in ('perturbation',)]).any()\n",
    "\n",
    "        # Initialize\n",
    "        env.set_modalities(modalities); env.reset(); memories[ak] = defaultdict(lambda: [])\n",
    "\n",
    "        # Modify\n",
    "        state_vars, end = state_manager(\n",
    "            # present=present,\n",
    "            state=env.get_state(),\n",
    "            modalities=ppc.cast(ppc.inverse_transform(ppc.inverse_cast(modalities)), device='cpu') if use_modalities else modalities,\n",
    "            labels=labels,\n",
    "            times=times,\n",
    "        )\n",
    "        present = state_vars['present']\n",
    "        memory_mask = present if optimize_memory else torch.ones_like(present, device=DEVICE)\n",
    "        full_state = state_vars['state']\n",
    "        env.set_state(full_state[memory_mask])\n",
    "        raw_modalities = state_vars['modalities']\n",
    "        processed_modalities = [m[memory_mask.cpu()] for m in raw_modalities]\n",
    "        if use_modalities: processed_modalities = ppc.cast(ppc.transform(ppc.inverse_cast(processed_modalities)))\n",
    "        env.set_modalities(processed_modalities)\n",
    "\n",
    "        # Continue initializing\n",
    "        memories[ak]['present'].append(present.cpu())\n",
    "        memories[ak]['states'].append(full_state.cpu())\n",
    "        memories[ak]['stages'].append(get_current_stage())\n",
    "        memories[ak]['rewards'].append(torch.zeros(modalities[0].shape[0]))\n",
    "\n",
    "        # Simulate\n",
    "        get_desc = lambda ts, st: f'\\tTimestep {ts}' + (f', Stage {st+1}/{get_max_stage()+1}' if st != -1 else '')\n",
    "        timestep = 0; pbar = tqdm(ascii=True, desc=get_desc(timestep, get_current_stage()), ncols=100)  # CLI\n",
    "        while True:\n",
    "            # Step\n",
    "            state = env.get_state(include_modalities=True)\n",
    "            actions = torch.zeros((modalities[0].shape[0], env.dim), device=DEVICE)\n",
    "            actions[present] = policy.act_macro(\n",
    "                state if optimize_memory else state[present],\n",
    "                keys=torch.arange(modalities[0].shape[0], device=DEVICE)[present],\n",
    "                max_batch=config['train']['max_batch'],\n",
    "            )\n",
    "            rewards = torch.zeros(modalities[0].shape[0], device=DEVICE)\n",
    "            new_rewards, _, _ = env.step(actions[present] if optimize_memory else actions, return_itemized_rewards=True)\n",
    "            if optimize_memory: rewards[present] = new_rewards\n",
    "            else: rewards = new_rewards\n",
    "            full_state[present] = env.get_state() if optimize_memory else env.get_state()[present]\n",
    "            if not optimize_memory: env.set_state(full_state)  # Don't move un-spawned nodes\n",
    "\n",
    "            # Modify\n",
    "            state_vars, end = state_manager(\n",
    "                present=present,\n",
    "                state=full_state,\n",
    "                modalities=raw_modalities,\n",
    "                labels=labels,\n",
    "                times=times,\n",
    "            )\n",
    "            present_change = (state_vars['present'] != present).any()\n",
    "            present = state_vars['present']\n",
    "            memory_mask = present if optimize_memory else torch.ones_like(present, device=DEVICE)\n",
    "            full_state = state_vars['state']\n",
    "            env.set_state(full_state[memory_mask])\n",
    "            # Only modify if changes\n",
    "            if (\n",
    "                torch.tensor([(rm != svm).any() for rm, svm in zip(raw_modalities, state_vars['modalities'])]).any()\n",
    "                or (optimize_memory and present_change)\n",
    "            ):\n",
    "                raw_modalities = state_vars['modalities']\n",
    "                processed_modalities = [m[memory_mask.cpu()] for m in raw_modalities]\n",
    "                if use_modalities: processed_modalities = ppc.cast(ppc.transform(ppc.inverse_cast(processed_modalities)))\n",
    "                env.set_modalities(processed_modalities)\n",
    "\n",
    "            # Record\n",
    "            memories[ak]['present'].append(present.cpu())\n",
    "            memories[ak]['states'].append(full_state.cpu())\n",
    "            memories[ak]['stages'].append(get_current_stage())\n",
    "            memories[ak]['rewards'].append(rewards.cpu())\n",
    "\n",
    "            # CLI\n",
    "            timestep += 1\n",
    "            update_timestep = 10\n",
    "            if timestep % update_timestep == 0:\n",
    "                pbar.set_description(get_desc(timestep, get_current_stage()))\n",
    "                pbar.update(update_timestep)\n",
    "\n",
    "            # End\n",
    "            if end: break\n",
    "\n",
    "        # CLI\n",
    "        pbar.close()\n",
    "\n",
    "        # Stack\n",
    "        memories[ak]['present'] = torch.stack(memories[ak]['present'])\n",
    "        memories[ak]['states'] = torch.stack(memories[ak]['states'])\n",
    "        memories[ak]['stages'] = torch.tensor(memories[ak]['stages'])\n",
    "        memories[ak]['rewards'] = torch.stack(memories[ak]['rewards'])\n",
    "        memories[ak] = dict(memories[ak])\n",
    "\n",
    "        # Profiling\n",
    "        if profile:\n",
    "            torch.cuda.memory._dump_snapshot('cuda_profile.pkl')\n",
    "            torch.cuda.memory._record_memory_history(enabled=None)\n",
    "\n",
    "        # Save into half-accuracy gzip (8,506 KB -> 3,236 KB)\n",
    "        print(f'\\tSaving memories')\n",
    "        compressed_type = torch.float16\n",
    "        with gzip.open(fname, 'wb') as f:\n",
    "            func_attr = lambda attr: attr.type(compressed_type) if attr.dtype not in (torch.long, torch.bool) else attr\n",
    "            celltrip.utilities.dict_map(memories[ak], func_attr, inplace=True)\n",
    "            pickle.dump(memories, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "for ak in args.analysis_key:\n",
    "    print(f'Statistics {ak}')\n",
    "\n",
    "    ## Stages\n",
    "    stages, counts = np.unique(memories[ak]['stages'], return_counts=True)\n",
    "    print('\\tSteps per Stage')\n",
    "    for s, c in zip(stages, counts):\n",
    "        print(f'\\t\\t\\t{s}\\t{c}')\n",
    "        \n",
    "    ## Memory\n",
    "    print('\\tCompressed Memory Sizes')\n",
    "    for k in memories[ak]:\n",
    "        t_size = sum([t.element_size() * t.nelement() if isinstance(t, torch.Tensor) else 64/8 for t in memories[ak][k]]) / 1024**3\n",
    "        print(f'\\t\\t\\t{k} size\\t{t_size:.3f} Gb')\n",
    "\n",
    "    ## Performance\n",
    "    print(f'\\tAverage Reward: {memories[ak][\"rewards\"].cpu().mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plotting static analyses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\tTraining rewards')\n",
    "\n",
    "# Load history from wandb\n",
    "history = run.history(samples=2000)\n",
    "history['timestep'] = history['end_timestep']\n",
    "history['Runtime (h)'] = history['_runtime'] / 60**2\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(18, 6), layout='constrained')\n",
    "def plot_without_zeros(x, y, **kwargs):\n",
    "    x, y = x[np.argwhere(y != 0).flatten()], y[np.argwhere(y != 0).flatten()]\n",
    "    ax.plot(x, y, **kwargs)\n",
    "ax.plot(history['timestep'], history['average_reward'], color='black', lw=3, label='Average Reward')\n",
    "plot_without_zeros(history['timestep'], history['rewards/bound'], color='red', alpha=.75, lw=2, label='Boundary Penalty')\n",
    "plot_without_zeros(history['timestep'], history['rewards/velocity'], color='goldenrod', alpha=.75, lw=2, label='Velocity Penalty')\n",
    "plot_without_zeros(history['timestep'], history['rewards/action'], color='green', alpha=.75, lw=2, label='Action Penalty')\n",
    "plot_without_zeros(history['timestep'], history['rewards/distance'], color='blue', alpha=.75, lw=2, label='Distance Reward')\n",
    "plot_without_zeros(history['timestep'], history['rewards/origin'], color='darkorange', alpha=.75, lw=2, label='Origin Reward')\n",
    "\n",
    "# Stage ticks\n",
    "unique, stage_idx = np.unique(history['stage'][::-1], return_index=True)\n",
    "stage_idx = len(history['stage']) - stage_idx\n",
    "stage_idx = stage_idx[:-1]\n",
    "[ax.axvline(x=history['timestep'][idx], color='black', alpha=.5, linestyle='dashed', lw=1) for idx in stage_idx]\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel('Timestep')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.legend(loc='lower right', ncols=3)\n",
    "\n",
    "# Styling\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "ax.set_xlim([0, history['timestep'].max()])\n",
    "\n",
    "# Save plot\n",
    "fname = f'{args.run_id}_{config[\"data\"][\"dataset\"]}_loss.pdf'\n",
    "fig.savefig(os.path.join(PLOT_FOLDER, fname), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_methods(prefix):\n",
    "    # Get other methods\n",
    "    method_results = {}\n",
    "    try:\n",
    "        method_dir = os.path.join(BASE_FOLDER, '../other_methods/runs', config['data']['dataset'])\n",
    "        method_names = next(os.walk(method_dir))[1]\n",
    "    except: method_names = []\n",
    "    for name in method_names:\n",
    "        # Get output files\n",
    "        files = os.listdir(os.path.join(method_dir, name))\n",
    "        r = re.compile(f'^{prefix}(\\d+)(?:_(\\d+))?.txt$')\n",
    "        files = list(filter(r.match, files))\n",
    "\n",
    "        # Record\n",
    "        for file in files:\n",
    "            modality, seed = r.match(file)[1], r.match(file)[2]\n",
    "            method_results[(name, modality, seed)] = os.path.join(method_dir, name, file)\n",
    "\n",
    "    return method_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison metrics\n",
    "metric_rand = lambda X: np.random.rand()\n",
    "metric_silhouette = lambda X: sklearn.metrics.silhouette_score(X, labels)\n",
    "metric_ch_score = lambda X: sklearn.metrics.calinski_harabasz_score(X, labels)\n",
    "def metric_knn_ami(X):\n",
    "    knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "    knn.fit(X, labels)\n",
    "    pred = knn.predict(X)\n",
    "    return sklearn.metrics.adjusted_mutual_info_score(labels, pred)\n",
    "\n",
    "# Metric metadata\n",
    "metric_tuples = {\n",
    "    'rand': (metric_rand, {'label': 'Random'}),\n",
    "    'sc': (metric_silhouette, {'label': 'Silhouette Coefficient'}),\n",
    "    'knn_ami': (metric_knn_ami, {'label': 'KNN Adjusted Mutual Information'}),\n",
    "    'ch': (metric_ch_score, {'label': 'Calinski Harabasz Index', 'scale': 'log'}),\n",
    "}\n",
    "\n",
    "# Metric selection per analysis type\n",
    "comparison_dict = {\n",
    "    'integration': {\n",
    "        'prefix': 'P',\n",
    "        'metrics': (metric_tuples['ch'], metric_tuples['knn_ami']),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Integration method comparison\n",
    "desired_application = 'integration'\n",
    "if 'convergence' in args.analysis_key and application_type == desired_application:\n",
    "    print(f'\\tComparison Integration')\n",
    "\n",
    "    # Select metrics\n",
    "    (metric_x, kwargs_x), (metric_y, kwargs_y) = comparison_dict[desired_application]['metrics']\n",
    "\n",
    "    # Get other methods\n",
    "    method_results = get_other_methods(comparison_dict[desired_application][\"prefix\"])\n",
    "\n",
    "    # Add CellTRIP\n",
    "    method_results[('CellTRIP', '-1', notebook_seed)] = memories['convergence']['states'][-1].detach().cpu()\n",
    "\n",
    "    # Compile and calculate performances\n",
    "    raw_performance = pd.DataFrame(columns=['Method', 'Modality', 'Seed', 'x', 'y'])\n",
    "    for key, fname in method_results.items():\n",
    "        method, modality, seed = key\n",
    "        if method == 'CellTRIP': data = fname\n",
    "        else: data = np.loadtxt(fname)\n",
    "        raw_performance.loc[raw_performance.shape[0]] = [*key, metric_x(data), metric_y(data)]\n",
    "\n",
    "    # Aggregate to group statistics\n",
    "    group = raw_performance.groupby(['Method', 'Modality'])\n",
    "    group_mean = group[['x', 'y']].mean().rename(columns=lambda n: f'{n}_mean')\n",
    "    group_var = group[['x', 'y']].var().rename(columns=lambda n: f'{n}_var')\n",
    "    group_count = group[['x']].count().rename(columns={'x': 'Count'})\n",
    "    performance = group_mean.join(group_var).join(group_count).fillna(0).reset_index()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'\\t\\t{\"Method\":<10}\\tModal\\tx Mean\\tx Var\\ty Mean\\ty Var\\tCount')\n",
    "    for i, r in performance.sort_values('Modality').iterrows():\n",
    "        print(f'\\t\\t{r[\"Method\"]:<8}\\t{r[\"Modality\"]}\\t{r[\"x_mean\"]:.3f}\\t{r[\"x_var\"]:.3f}\\t{r[\"y_mean\"]:.3f}\\t{r[\"y_var\"]:.3f}\\t{r[\"Count\"]}')\n",
    "\n",
    "    # Plot with text\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6), sharex=True, layout='constrained')\n",
    "    method_colors = {}\n",
    "    annotations = []\n",
    "    for i, r in performance.iterrows():\n",
    "        # Set color\n",
    "        if r['Method'] not in method_colors: method_colors[r['Method']] = sns.color_palette()[len(method_colors)]\n",
    "        \n",
    "        # Plot\n",
    "        ax.scatter(\n",
    "            r['x_mean'],\n",
    "            r['y_mean'],\n",
    "            color=method_colors[r['Method']],\n",
    "            s=100,\n",
    "        )\n",
    "\n",
    "        # Cross lines\n",
    "        ax.plot([r['x_mean']-r['x_var'], r['x_mean']+r['x_var']], 2*[r['y_mean']], ls='--', color='gray', zorder=.3)\n",
    "        ax.plot(2*[r['x_mean']], [r['y_mean']-r['y_var'], r['y_mean']+r['y_var']], ls='--', color='gray', zorder=.3)\n",
    "\n",
    "        # Annotate\n",
    "        text = f'{r[\"Method\"]}' + (f' ({r[\"Modality\"]})' if r['Modality'] != -1 else '')\n",
    "        annotations.append(ax.text(\n",
    "            r['x_mean'], r['y_mean'], text,\n",
    "            ha='center', va='center', fontsize='large'))\n",
    "\n",
    "    # Styling\n",
    "    # ax.spines[['right', 'top', 'bottom', 'left']].set_visible(False)\n",
    "    # ax.axvline(x=0, ls='-', alpha=.6, color='black', zorder=.1)\n",
    "    # ax.axhline(y=0, ls='-', alpha=.6, color='black', zorder=.1)\n",
    "    ax.set(\n",
    "        **{'x'+k: v for k, v in kwargs_x.items()},\n",
    "        **{'y'+k: v for k, v in kwargs_y.items()},\n",
    "    )\n",
    "    ax.tick_params(axis='both', which='both', bottom=True, left=True)\n",
    "\n",
    "    # Adjust Annotation Positions\n",
    "    from adjustText import adjust_text\n",
    "    adjust_text(\n",
    "        annotations,\n",
    "        # add_objects=ax.get_children()[0],\n",
    "        expand=(2, 3), \n",
    "        arrowprops=dict(\n",
    "            arrowstyle='-|>',\n",
    "            mutation_scale=10,\n",
    "            shrinkA=2, shrinkB=7,\n",
    "            color='black',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Save plot\n",
    "    fname =                                     f'{args.run_id}'\n",
    "    if args.stage is not None: fname +=         f'_{args.stage:02}'\n",
    "    fname +=                                    f'_{config[\"data\"][\"dataset\"]}'\n",
    "    fname +=                                    f'_comparison'\n",
    "    fname +=                                    f'_{desired_application}.pdf'\n",
    "    fig.savefig(os.path.join(PLOT_FOLDER, fname), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single modality imputation\n",
    "if 'convergence' in args.analysis_key and application_type == 'imputation' and len(env.reward_distance_target) == 1:\n",
    "    # Get other methods\n",
    "    method_results = get_other_methods('I')\n",
    "\n",
    "    # Add CellTRIP\n",
    "    raw_celltrip = memories['convergence']['states'][-1].detach().cpu()[:, :env.dim]\n",
    "    # shrunk_celltrip = sklearn.decomposition.PCA(n_components=modalities[env.reward_distance_target[0]].shape[1]).fit_transform(raw_celltrip)\n",
    "    method_results[('CellTRIP', env.reward_distance_target[0]+1, notebook_seed)] = raw_celltrip\n",
    "\n",
    "    # Calculate modal dist\n",
    "    # raw_modalities = ppc.cast(ppc.inverse_transform(ppc.inverse_cast(modalities)))\n",
    "    modal_dist = [celltrip.utilities.euclidean_distance(m) for m in modalities]\n",
    "\n",
    "    # Compile and calculate performances\n",
    "    raw_performance = None\n",
    "    for key, fname in method_results.items():\n",
    "        method, modality, seed = key\n",
    "        if method == 'CellTRIP': data = fname\n",
    "        else: data = np.loadtxt(fname)\n",
    "        data = torch.Tensor(data)\n",
    "\n",
    "        # Scale results\n",
    "        if method == 'CellTRIP': data = data * np.sqrt(data.shape[1])  # TODO: See if this is justified\n",
    "        # if not method == 'CellTRIP': data = data / ppc.standardize_std[int(modality)-1]\n",
    "\n",
    "        # Compute error\n",
    "        data_dist = celltrip.utilities.euclidean_distance(data)\n",
    "        sample_mse = (data_dist - modal_dist[int(modality)-1].cpu()).square().mean(dim=-1)\n",
    "        feature_mse = (data_dist - modal_dist[int(modality)-1].cpu()).square().mean(dim=0)\n",
    "\n",
    "        # Record\n",
    "        df = pd.DataFrame({'Method': method, 'Modality': modality, 'Seed': seed, 'Metric': 'Sample Inter-Cell MSE', 'Value': sample_mse})\n",
    "        if raw_performance is None: raw_performance = df\n",
    "        else: raw_performance = pd.concat((raw_performance, df), ignore_index=True, axis=0)\n",
    "\n",
    "    # Fill NA (For non-random methods)\n",
    "    raw_performance = raw_performance.fillna(0)\n",
    "\n",
    "    # Plot performances\n",
    "    main_metric = 'Sample Inter-Cell MSE'\n",
    "\n",
    "    # Filter to only the best result from each method\n",
    "    best_seeds = (\n",
    "        raw_performance.loc[raw_performance['Metric'] == main_metric]\n",
    "        .groupby(['Method', 'Modality', 'Seed'])[['Value']].mean().reset_index()\n",
    "        .sort_values('Value', ascending=False).groupby(['Method', 'Modality'])[['Seed']].first().reset_index().to_numpy()\n",
    "    )\n",
    "    mask = np.zeros(raw_performance.shape[0], dtype=bool)\n",
    "    for idx in best_seeds:\n",
    "        mask += (raw_performance.to_numpy()[:, :3] == idx).all(axis=-1)\n",
    "    filtered_performance = raw_performance.iloc[mask]\n",
    "\n",
    "    # Generate visuals\n",
    "    filtered_performance = filtered_performance.loc[filtered_performance['Metric'] == main_metric]\n",
    "    order = filtered_performance['Method'].unique()\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3, 6), sharex=True, layout='constrained')\n",
    "    sns.boxplot(\n",
    "        data=filtered_performance,\n",
    "        x='Method', y='Value', hue='Method',\n",
    "        order=order,\n",
    "        ax=ax)\n",
    "    with warnings.catch_warnings(record=False) as w:\n",
    "        warnings.simplefilter('ignore')\n",
    "        sns.stripplot(\n",
    "            data=filtered_performance.sample(frac=.05),\n",
    "            x='Method', y='Value', hue='Method',\n",
    "            order=order,\n",
    "            size=2, palette=sns.color_palette(['black']), legend=False, dodge=False, ax=ax)\n",
    "\n",
    "    # Styling\n",
    "    ax.set(title=main_metric, xlabel=None, ylabel=None)\n",
    "    ax.tick_params(axis='both', which='both', bottom=False, left=True)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    # Xlabels\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='center', va='baseline')\n",
    "    max_height = max([l.get_window_extent(renderer=ax.figure.canvas.get_renderer()).height for l in ax.get_xticklabels()])\n",
    "    fontsize = ax.get_xticklabels()[0].get_size()\n",
    "    pad = fontsize / 2 + max_height / 2\n",
    "    ax.tick_params(axis='x', pad=pad)\n",
    "\n",
    "    # Save plot\n",
    "    fname =                                     f'{args.run_id}'\n",
    "    if args.stage is not None: fname +=         f'_{args.stage:02}'\n",
    "    fname +=                                    f'_{config[\"data\"][\"dataset\"]}'\n",
    "    fname +=                                    f'_comparison'\n",
    "    fname +=                                    f'_imputation.pdf'\n",
    "    fig.savefig(os.path.join(PLOT_FOLDER, fname), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation significance analysis\n",
    "if 'perturbation' in memories:\n",
    "    print('\\tFeature effect size')\n",
    "    \n",
    "    # Get last idx for each stage\n",
    "    stages = memories['perturbation']['stages'].cpu().numpy()\n",
    "    unique_stages, unique_idx = np.unique(stages[::-1], return_index=True)\n",
    "    unique_idx = stages.shape[0] - unique_idx - 1\n",
    "    # unique_stages, unique_idx = unique_stages[::-1], unique_idx[::-1]\n",
    "\n",
    "    # Record perturbation feature pairs\n",
    "    perturbation_feature_triples = [(i, f, n) for i, (fs, ns) in enumerate(zip(perturbation_features, perturbation_feature_names)) for f, n in zip(fs, ns)]\n",
    "\n",
    "    # Compute effect sizes for each\n",
    "    effect_sizes = []\n",
    "    for stage, idx in zip(unique_stages, unique_idx):\n",
    "        # Get state\n",
    "        state = memories['perturbation']['states'][idx]\n",
    "\n",
    "        # Record steady state after convergence\n",
    "        if stage == 0:\n",
    "            steady_state = state\n",
    "            continue\n",
    "\n",
    "        # Get perturbed feature\n",
    "        m_idx, pf, pf_name = perturbation_feature_triples[stage-1]\n",
    "\n",
    "        # Compute effect size\n",
    "        effect_size = (state[:, :env.dim] - steady_state[:, :env.dim]).square().sum(dim=-1).sqrt().mean(dim=-1).item()\n",
    "        effect_sizes.append(effect_size)\n",
    "\n",
    "    # Print effect sizes\n",
    "    i = 0\n",
    "    for j, (pfs, pfns) in enumerate(zip(perturbation_features, perturbation_feature_names)):\n",
    "        print(f'\\t\\tModality {j}')\n",
    "        for pf, pfn in zip(pfs, pfns):\n",
    "            print(f'\\t\\t\\t{pfn:<15}{effect_sizes[i]:.02e}')\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plotting dynamic visualizations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T18:27:48.249939Z",
     "iopub.status.busy": "2025-01-15T18:27:48.249780Z",
     "iopub.status.idle": "2025-01-15T18:28:39.620056Z",
     "shell.execute_reply": "2025-01-15T18:28:39.619296Z"
    }
   },
   "outputs": [],
   "source": [
    "for ak in args.analysis_key:\n",
    "    if args.novid: break\n",
    "    print(f'\\tVideo {ak}')\n",
    "\n",
    "    # Prepare data\n",
    "    present = memories[ak]['present'].cpu()\n",
    "    states = memories[ak]['states'].cpu()\n",
    "    stages = memories[ak]['stages'].cpu()\n",
    "    rewards = memories[ak]['rewards'].cpu()\n",
    "    base_env = celltrip.environments.trajectory(*[torch.empty((0, 0)) for _ in range(len(modalities))], **config['env'])\n",
    "\n",
    "    # Testing for portions of large datasets\n",
    "    # sub_idx = np.random.choice(modalities[0].shape[0], 1_000, replace=False)\n",
    "    # modalities, labels, times = [m[sub_idx] for m in modalities], labels[sub_idx], times[sub_idx]\n",
    "    # present, states, rewards = present[:, sub_idx], states[:, sub_idx], rewards[:, sub_idx]\n",
    "\n",
    "    # Testing for larger dims\n",
    "    # states = torch.concatenate((states, states), dim=-1)\n",
    "    # base_env.dim *= 2\n",
    "\n",
    "    # Skip data\n",
    "    present, states, stages, rewards = present[::args.skip], states[::args.skip], stages[::args.skip], rewards[::args.skip]\n",
    "    # if states_3d is not None: states_3d = states_3d[::args.skip]\n",
    "\n",
    "    # Reduce dimensions\n",
    "    if states.shape[-1] > 2*3 or args.force_reduction:\n",
    "        print('\\t\\tReducing state dimensionality')\n",
    "        # Get idx of last state in designated stage\n",
    "        stage_unique, stage_idx = np.unique(stages.numpy()[::-1], return_index=True)\n",
    "        stage_idx = memories[ak]['stages'].shape[0] - stage_idx - 1\n",
    "\n",
    "        # Choose reduction type\n",
    "        if args.reduction_type == 'umap':\n",
    "            import umap\n",
    "            fit_reducer = lambda data: umap.UMAP(n_components=3, random_state=notebook_seed).fit(data)\n",
    "            transform_reducer = lambda reducer, data: torch.Tensor(reducer.transform(data))\n",
    "        elif args.reduction_type == 'pca':\n",
    "            import sklearn.decomposition\n",
    "            fit_reducer = lambda data: sklearn.decomposition.PCA(n_components=3, random_state=notebook_seed).fit(data)\n",
    "            transform_reducer = lambda reducer, data: torch.Tensor(reducer.transform(data))\n",
    "        elif args.reduction_type is None or args.reduction_type == 'none':\n",
    "            initialize_reducer = lambda: None\n",
    "            transform_reducer = lambda reducer, data: data\n",
    "\n",
    "        # Get steady state\n",
    "        if ak in ('convergence', 'discovery', 'perturbation',):\n",
    "            reducer = fit_reducer(states[stage_idx[0]])\n",
    "            get_reducer = lambda stage: reducer\n",
    "        elif ak in ('temporal',):\n",
    "            get_reducer = lambda stage: fit_reducer(states[stage_idx[stage]])\n",
    "\n",
    "        # UMAP\n",
    "        get_desc = lambda stage: f'\\t\\t\\tProjecting ({stage}/{stage_unique.max()})'\n",
    "        states_3d = []; pbar = tqdm(total=states.shape[0]*states.shape[1], desc=get_desc(0), ascii=True, ncols=100)\n",
    "        for stage in stage_unique:\n",
    "            pbar.set_description(get_desc(stage))\n",
    "            stage_states = states[stages==stage].reshape((-1, states.shape[-1]))\n",
    "            for i in range(0, stage_states.shape[0], args.reduction_batch):\n",
    "                states_3d.append(transform_reducer(get_reducer(stage), stage_states[i:i+args.reduction_batch]))\n",
    "                pbar.update(stage_states[i:i+args.reduction_batch].shape[0])\n",
    "        pbar.close()\n",
    "        states_3d = torch.concatenate(states_3d, dim=0).reshape((*states.shape[:-1], 3))\n",
    "        states_3d = torch.concatenate((states_3d, torch.zeros_like(states_3d)), dim=-1)\n",
    "    else:\n",
    "        states_3d = None\n",
    "\n",
    "    # CLI\n",
    "    print('\\t\\tGenerating video')\n",
    "\n",
    "    # Parameters\n",
    "    interval = 1e3*env.delta/3  # Time between frames (3x speedup)\n",
    "    min_max_vel = 1e-2 if ak in ('convergence', 'discovery') else -1  # Stop at first frame all vels are below target. 0 for full play\n",
    "    frame_override = None  # Manually enter number of frames to draw\n",
    "    rotations_per_second = .1  # Camera azimuthal rotations per second\n",
    "    num_lines = 100\n",
    "    if ak == 'temporal': num_lines *= len(temporal['stages'])**2\n",
    "\n",
    "    # Create plot based on key\n",
    "    # NOTE: Standard 1-padding all around and between figures\n",
    "    # NOTE: Left, bottom, width, height\n",
    "    if ak in ('convergence', 'discovery'):\n",
    "        figsize = (15, 10)\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        axs = [\n",
    "            fig.add_axes([1 /figsize[0], 1 /figsize[1], 8 /figsize[0], 8 /figsize[1]], projection='3d'),\n",
    "            fig.add_axes([10 /figsize[0], 5.5 /figsize[1], 4 /figsize[0], 3.5 /figsize[1]]),\n",
    "            fig.add_axes([10 /figsize[0], 1 /figsize[1], 4 /figsize[0], 3.5 /figsize[1]]),\n",
    "        ]\n",
    "        views = [\n",
    "            celltrip.utilities.View3D,\n",
    "            celltrip.utilities.ViewTemporalScatter,\n",
    "            celltrip.utilities.ViewSilhouette,\n",
    "        ]\n",
    "\n",
    "    elif ak == 'temporal':\n",
    "        figsize = (15, 10)\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        axs = [\n",
    "            fig.add_axes([1 /figsize[0], 1 /figsize[1], 8 /figsize[0], 8 /figsize[1]], projection='3d'),\n",
    "            fig.add_axes([10 /figsize[0], 5.5 /figsize[1], 4 /figsize[0], 3.5 /figsize[1]]),\n",
    "            fig.add_axes([10 /figsize[0], 1 /figsize[1], 4 /figsize[0], 3.5 /figsize[1]]),\n",
    "        ]\n",
    "        views = [\n",
    "            celltrip.utilities.View3D,\n",
    "            celltrip.utilities.ViewTemporalScatter,\n",
    "            celltrip.utilities.ViewTemporalDiscrepancy,\n",
    "        ]\n",
    "\n",
    "    elif ak in ('perturbation',):\n",
    "        figsize = (20, 10)\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        axs = [\n",
    "            fig.add_axes([1 /figsize[0], 1 /figsize[1], 8 /figsize[0], 8 /figsize[1]], projection='3d'),\n",
    "            fig.add_axes([10 /figsize[0], 5.5 /figsize[1], 8 /figsize[0], 3.5 /figsize[1]]),\n",
    "            fig.add_axes([10 /figsize[0], 1 /figsize[1], 3.5 /figsize[0], 3.5 /figsize[1]]),\n",
    "            fig.add_axes([14.5 /figsize[0], 1 /figsize[1], 3.5 /figsize[0], 3.5 /figsize[1]]),\n",
    "        ]\n",
    "        views = [\n",
    "            celltrip.utilities.View3D,\n",
    "            celltrip.utilities.ViewPerturbationEffect,\n",
    "            celltrip.utilities.ViewTemporalScatter,\n",
    "            celltrip.utilities.ViewSilhouette,\n",
    "        ]\n",
    "\n",
    "    # Initialize views\n",
    "    arguments = {\n",
    "        # Data\n",
    "        'present': present,\n",
    "        'states': states,\n",
    "        'states_3d': states_3d,\n",
    "        'stages': stages,\n",
    "        'rewards': rewards,\n",
    "        'modalities': modalities,\n",
    "        'labels': labels,\n",
    "        # Data params\n",
    "        'dim': base_env.dim,\n",
    "        'modal_targets': base_env.reward_distance_target,\n",
    "        'temporal_stages': temporal['stages'],\n",
    "        'perturbation_features': perturbation_features,\n",
    "        'perturbation_feature_names': perturbation_feature_names,\n",
    "        'partitions': times if ak in ('temporal',) else None,\n",
    "        # Arguments\n",
    "        'interval': interval,\n",
    "        'skip': args.skip,\n",
    "        'seed': notebook_seed,\n",
    "        # Styling\n",
    "        'num_lines': num_lines,\n",
    "        'ms': 5,  # 3\n",
    "        'lw': 1,\n",
    "    }\n",
    "    views = [view(**arguments, ax=ax) for view, ax in zip(views, axs)]\n",
    "\n",
    "    # Compile animation\n",
    "    frames = states[..., env.dim:env.dim+3].square().sum(dim=-1).sqrt().max(dim=-1).values < min_max_vel\n",
    "    frames = np.array([(frames[i] or frames[i+1]) if i != len(frames)-1 else frames[i] for i in range(len(frames))])  # Disregard interrupted sections of low movement\n",
    "    frames = np.argwhere(frames)\n",
    "    frames = frames[0, 0].item()+1 if len(frames) > 0 else states.shape[0]\n",
    "    frames = frames if frame_override is None else frame_override\n",
    "\n",
    "    # Update function\n",
    "    pbar = tqdm(ascii=True, total=frames+1, desc='\\t\\t\\tRendering', ncols=100)  # CLI, runs frame 0 twice\n",
    "    def update(frame):\n",
    "        # Update views\n",
    "        for view in views:\n",
    "            view.update(frame)\n",
    "\n",
    "        # CLI\n",
    "        update_timestep = 1\n",
    "        if frame % update_timestep == 0:\n",
    "            pbar.update(update_timestep)\n",
    "\n",
    "    # Test individual frames\n",
    "    # for frame in range(frames):\n",
    "    #     update(frame)\n",
    "    #     # print()\n",
    "    #     # print('saving')\n",
    "    #     fig.savefig(os.path.join('temp/plots', f'frame_{frame}.png'), dpi=300)\n",
    "    #     break\n",
    "\n",
    "    # Initialize animation\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig=fig,\n",
    "        func=update,\n",
    "        frames=frames,\n",
    "        interval=interval,\n",
    "    )\n",
    "\n",
    "    # Display animation as it renders\n",
    "    # plt.show()\n",
    "\n",
    "    # Display complete animation\n",
    "    # from IPython.display import HTML\n",
    "    # HTML(ani.to_jshtml())\n",
    "\n",
    "    # Save animation\n",
    "    # NOTE: Requires `sudo apt-get install ffmpeg`\n",
    "    file_type = 'mp4' if not args.gif else 'gif'\n",
    "    if file_type == 'mp4': writer = animation.FFMpegWriter(fps=int(1e3/interval), extra_args=['-vcodec', 'libx264'], bitrate=8e3)  # Faster\n",
    "    elif file_type == 'gif': writer = animation.FFMpegWriter(fps=int(1e3/interval))  # Slower\n",
    "    fname =                                     f'{args.run_id}'\n",
    "    if args.stage is not None: fname +=         f'_{args.stage:02}'\n",
    "    fname +=                                    f'_{config[\"data\"][\"dataset\"]}'\n",
    "    fname +=                                    f'_{ak}'\n",
    "    fname +=                                    f'.{file_type}'\n",
    "    ani.save(os.path.join(PLOT_FOLDER, fname), writer=writer, dpi=300)\n",
    "\n",
    "    # CLI\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
