{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T18:27:18.898240Z",
     "iopub.status.busy": "2025-01-15T18:27:18.897819Z",
     "iopub.status.idle": "2025-01-15T18:27:21.049625Z",
     "shell.execute_reply": "2025-01-15T18:27:21.049119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=analysis.ipynb\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env WANDB_NOTEBOOK_NAME analysis.ipynb\n",
    "%env WANDB_SILENT true\n",
    "%matplotlib agg\n",
    "# ipympl\n",
    "\n",
    "from collections import defaultdict\n",
    "import colorsys\n",
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import sklearn.neighbors\n",
    "import torch\n",
    "import umap\n",
    "import wandb\n",
    "\n",
    "# Enable text output in notebooks\n",
    "import tqdm.auto\n",
    "import tqdm.notebook\n",
    "tqdm.notebook.tqdm = tqdm.auto.tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import data\n",
    "import celltrip\n",
    "\n",
    "# Set params\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data')\n",
    "PLOT_FOLDER = os.path.join(BASE_FOLDER, '../plots')\n",
    "\n",
    "# Style\n",
    "sns.set_context('paper', font_scale=1.25)\n",
    "sns.set_style('white')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# MPL params\n",
    "mpl.rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "# Disable gradients\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minor\n",
    "  - Re-apply PCA before pinning (better accuracy? If not, find out why acc got worse)\n",
    "  - Allow original data transform to be inactive in the case of 2 dimensions already\n",
    "  - Add imputation visualization for dim > 2 (Use original data transform)\n",
    "  - Apply inverse transform to imputation results and other methods + process->inverse for raw other methods\n",
    "  - Get file location rather than `abspath` + outdir, datadir, wandb, etc.\n",
    "  - No rotation versions of vids, add --rotation speed scaling argument\n",
    "  - Add temporal integration performance comparisons (and imputation)\n",
    "  - .txt output for perturbations\n",
    "\n",
    "- Medium\n",
    "  - Filter perturbations for video\n",
    "\n",
    "- Major\n",
    "  - Add trajectory\n",
    "\n",
    "- Later\n",
    "  - Add lockfile for gzip memories\n",
    "  - Convert to functions for use as a library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Create a video of the specified CellTRIP model')\n",
    "\n",
    "# Main parameters\n",
    "group = parser.add_argument_group('General')\n",
    "group.add_argument('run_id', type=str, help='Run ID from WandB to use for processing')\n",
    "group.add_argument('analysis_key', choices=('convergence', 'discovery', 'temporal', 'perturbation'), nargs='+', type=str, help='Type of analyses to perform (one or more)')\n",
    "group.add_argument('-S', '--seed', type=int, help='Override simulation seed')\n",
    "group.add_argument('--gpu', default='0', type=str, help='GPU(s) to use')\n",
    "\n",
    "# Model parameters\n",
    "group = parser.add_argument_group('Model')\n",
    "group.add_argument('-b', '--batch', metavar='MAX_BATCH', dest='max_batch', type=int, help='Override number of nodes which can calculate actions simultaneously')\n",
    "group.add_argument('--num', metavar='NUM_NODES', dest='num_nodes', type=int, help='Override number of nodes to take from data')\n",
    "group.add_argument('--nodes', metavar='NUM_NEIGHBORS', dest='max_nodes', type=int, help='Override neighbors considered by each node')\n",
    "group.add_argument('--stage', type=int, help='Override model stage to use. 0 is random initialization')\n",
    "\n",
    "# Simulation specific arguments\n",
    "group = parser.add_argument_group('Simulation')\n",
    "group.add_argument('--discovery_key', type=int, default=0, help='Type of discovery analysis (0: Auto)')\n",
    "group.add_argument('--temporal_key', type=int, default=0, help='Type of temporal analysis (0: Auto, 1: TemporalBrain)')\n",
    "group.add_argument('--force', action='store_true', help='Rerun analysis even if already stored in memory')\n",
    "\n",
    "# Analysis arguments\n",
    "group = parser.add_argument_group('Analysis')\n",
    "group.add_argument('--original', action='store_true', help='Visualize input data')\n",
    "\n",
    "# Video parameters\n",
    "group = parser.add_argument_group('Video')\n",
    "group.add_argument('--novid', action='store_true', help='Skip video generation')\n",
    "group.add_argument('-g', '--gif', action='store_true', help='Output as a GIF rather than MP4')\n",
    "group.add_argument('-s', '--skip', type=int, default=5, help='Number of steps to advance each frame')\n",
    "group.add_argument('--reduction', choices=('umap', 'pca', 'none'), default='pca', type=str, dest='reduction_type', help='Reduction type to use for high-dimensional projections in 3D visualization')\n",
    "group.add_argument('--force_reduction', action='store_true', help='Force reduction, even if unnecessary')\n",
    "group.add_argument('--reduction_batch', type=int, default=100_000, help='Max number of states to reduce in one computation')\n",
    "\n",
    "# Legacy compatibility\n",
    "group = parser.add_argument_group('Legacy Compatiiblity')\n",
    "group.add_argument('--total_statistics', action='store_true', help='Compatibility argument to compute mean and variance over all samples')\n",
    "\n",
    "# List of common runs\n",
    "# 'brf6n6sn': TemporalBrain Random 100 Max\n",
    "# 'rypltvk5': MMD-MA Random 100 Max (requires `total_statistics`)\n",
    "# '32jqyk54': MERFISH Random 100 Max\n",
    "# 'c8zsunc9': ISS Random 100 Max\n",
    "# 'maofk1f2': ExSeq NR\n",
    "# 'f6ajo2am': smFish NR\n",
    "# 'vb1x7bae': MERFISH NR\n",
    "# '473vyon2': ISS NR\n",
    "\n",
    "# Notebook defaults and script handling\n",
    "if not celltrip.utilities.is_notebook():\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    # args = parser.parse_args('--novid --original --total_statistics rypltvk5 convergence discovery temporal perturbation'.split(' '))  # MMD-MA\n",
    "    args = parser.parse_args('--novid --original 32jqyk54 convergence discovery temporal perturbation'.split(' '))  # MERFISH\n",
    "    # args = parser.parse_args('--novid --original c8zsunc9 convergence discovery temporal perturbation'.split(' '))  # ISS\n",
    "    # args = parser.parse_args('--gpu 1 brf6n6sn temporal'.split(' '))  # TemporalBrain\n",
    "\n",
    "# Set env vars\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=args.gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data, Model, and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T18:27:21.052179Z",
     "iopub.status.busy": "2025-01-15T18:27:21.051926Z",
     "iopub.status.idle": "2025-01-15T18:27:24.431802Z",
     "shell.execute_reply": "2025-01-15T18:27:24.431274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading run 32jqyk54\n",
      "\tFinding model\n",
      "\t\tMDL policy found at stage 14\n",
      "\t\tWGT policy found at stage 14\n",
      "\tLoading dataset MERFISH\n",
      "\tLoading WGT model\n"
     ]
    }
   ],
   "source": [
    "# Load run\n",
    "print(f'Loading run {args.run_id}')\n",
    "api = wandb.Api()\n",
    "run = api.run(f'oafish/CellTRIP/{args.run_id}')\n",
    "config = defaultdict(lambda: {})\n",
    "for k, v in run.config.items():\n",
    "    dict_name, key = k.split('/')\n",
    "    config[dict_name][key] = v\n",
    "config = dict(config)\n",
    "\n",
    "# Reproducibility\n",
    "notebook_seed = args.seed if args.seed is not None else config['note']['seed']  # Potentially destructive if randomly sampled\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "torch.manual_seed(notebook_seed)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed(notebook_seed)\n",
    "np.random.seed(notebook_seed)\n",
    "\n",
    "# Get latest policy\n",
    "print('\\tFinding model')\n",
    "latest_mdl = [0, None]  # Pkl\n",
    "latest_wgt = [0, None]  # State dict\n",
    "# Compatibility with models of the previous naming convention\n",
    "# for file in run.files():\n",
    "#     matches = re.findall(f'^(?:models|trained_models)/policy_(\\w+).(mdl|wgt)$', file.name)\n",
    "#     if len(matches) > 0: stage = int(matches[0][0]); ftype = matches[0][1]\n",
    "#     else: continue\n",
    "#     if stage == 0: add_one = True; break\n",
    "# else: add_one = False\n",
    "# Iterate through model files\n",
    "for file in run.files():\n",
    "    matches = re.findall(f'^(?:models|trained_models)/policy_(\\w+).(mdl|wgt)$', file.name)\n",
    "    if len(matches) > 0: stage = int(matches[0][0]); ftype = matches[0][1]\n",
    "    else: continue\n",
    "    # if add_one: stage += 1\n",
    "\n",
    "    # Record\n",
    "    latest_known_stage = latest_mdl[0] if ftype == 'mdl' else latest_wgt[0]\n",
    "    if (args.stage is None and stage > latest_known_stage) or (args.stage is not None and stage == args.stage):\n",
    "        if ftype == 'mdl': latest_mdl = [stage, file]\n",
    "        elif ftype == 'wgt': latest_wgt = [stage, file]\n",
    "print(f'\\t\\tMDL policy found at stage {latest_mdl[0]}')\n",
    "print(f'\\t\\tWGT policy found at stage {latest_wgt[0]}')\n",
    "\n",
    "# Load data\n",
    "print(f'\\tLoading dataset {config[\"data\"][\"dataset\"]}')\n",
    "modalities, types, features = data.load_data(config['data']['dataset'], DATA_FOLDER)\n",
    "# config['data'] = celltrip.utilities.overwrite_dict(config['data'], {'standardize': True})  # Old model compatibility\n",
    "# config['data'] = celltrip.utilities.overwrite_dict(config['data'], {'top_variant': config['data']['pca_dim'], 'pca_dim': None})  # Swap PCA with top variant (testing)\n",
    "if args.num_nodes is not None: config['data'] = celltrip.utilities.overwrite_dict(config['data'], {'num_nodes': args.num_nodes})\n",
    "if args.max_batch is not None: config['train'] = celltrip.utilities.overwrite_dict(config['train'], {'max_batch': args.max_batch})\n",
    "for k in ('standardize', 'pca_dim', 'top_variant'):\n",
    "    # Legacy compatibility for missing default arguments\n",
    "    if k not in config['data']: config['data'][k] = None\n",
    "ppc = celltrip.utilities.Preprocessing(**config['data'], device=DEVICE)\n",
    "modalities, features = ppc.fit_transform(modalities, features, total_statistics=args.total_statistics)\n",
    "modalities, types = ppc.subsample(modalities, types)\n",
    "modalities = ppc.cast(modalities)\n",
    "# Assumes modalities are aligned\n",
    "labels = types[0][:, 0]\n",
    "times = types[0][:, -1]\n",
    "\n",
    "# Load env\n",
    "env = celltrip.environments.trajectory(*modalities, **config['env'], **config['stages']['env'][0], device=DEVICE)\n",
    "for weight_stage in config['stages']['env'][1:latest_mdl[0]+1]:\n",
    "    env.set_rewards(weight_stage)\n",
    "application_type = 'integration' if len(env.reward_distance_target) == len(modalities) else 'imputation'\n",
    "\n",
    "# Load model file\n",
    "load_type = 'WGT'\n",
    "if load_type == 'MDL' and latest_mdl[0] != 0:\n",
    "    print('\\tLoading MDL model')\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        latest_mdl[1].download(tmpdir, replace=True)\n",
    "        policy = torch.load(os.path.join(tmpdir, latest_mdl[1].name))\n",
    "elif load_type == 'WGT' and latest_wgt[0] != 0:\n",
    "    print('\\tLoading WGT model')\n",
    "    # Mainly used in the case of old argument names, also generally more secure\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        latest_wgt[1].download(tmpdir, replace=True)\n",
    "        # config['policy'] = celltrip.utilities.overwrite_dict(config['policy'], {'positional_dim': 6, 'modal_dims': [76]})  # Old model compatibility\n",
    "        if args.max_nodes is not None: config['policy'] = celltrip.utilities.overwrite_dict(config['policy'], {'max_nodes': args.max_nodes})\n",
    "        policy = celltrip.models.PPO(**config['policy'])\n",
    "        incompatible_keys = policy.load_state_dict(torch.load(os.path.join(tmpdir, latest_wgt[1].name), weights_only=True))\n",
    "else:\n",
    "    print('\\tGenerating random model')\n",
    "    # Use random model\n",
    "    policy = celltrip.models.PPO(**config['policy'])\n",
    "policy = policy.to(DEVICE).eval()\n",
    "policy.actor.set_action_std(1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T18:27:25.129441Z",
     "iopub.status.busy": "2025-01-15T18:27:25.129271Z",
     "iopub.status.idle": "2025-01-15T18:27:25.155699Z",
     "shell.execute_reply": "2025-01-15T18:27:25.155265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose key\n",
    "optimize_memory = True  # Saves memory by shrinking env based on present, also fixes reward calculation for non-full present mask\n",
    "# perturbation_features = [  # 10 Random\n",
    "#     np.random.choice(len(fs), 10, replace=False)\n",
    "#     if (i not in env.reward_distance_target) or (len(env.reward_distance_target) == len(modalities))\n",
    "#     else []\n",
    "#     for i, fs in enumerate(features)]\n",
    "perturbation_features = [  # All features\n",
    "    np.arange(len(fs))\n",
    "    if (i not in env.reward_distance_target) or (len(env.reward_distance_target) == len(modalities))\n",
    "    else []\n",
    "    for i, fs in enumerate(features)]\n",
    "\n",
    "# Define matching state manager classes\n",
    "state_manager_class = {\n",
    "    'convergence': celltrip.utilities.ConvergenceStateManager,\n",
    "    'discovery': celltrip.utilities.DiscoveryStateManager,\n",
    "    'temporal': celltrip.utilities.TemporalStateManager,\n",
    "    'perturbation': celltrip.utilities.PerturbationStateManager,\n",
    "}\n",
    "\n",
    "# Discovery list\n",
    "discovery = []\n",
    "# Reverse alphabetical (ExSeq, MERFISH, smFISH, ISS, MouseVisual)\n",
    "type_order = np.unique(labels)[::-1]\n",
    "discovery_general = {\n",
    "    'labels': list(type_order),\n",
    "    'delay': 50*np.arange(len(type_order)),\n",
    "    'rates': [1] + [.015]*(len(type_order)-1),\n",
    "    'origins': [None] + list(type_order[:-1])}\n",
    "discovery += [discovery_general]\n",
    "# Choose Discovery\n",
    "discovery = discovery[args.discovery_key]\n",
    "\n",
    "# Stage order list\n",
    "temporal = []\n",
    "# Reverse alphabetical (ExSeq, MERFISH, smFISH, ISS, MouseVisual)\n",
    "temporal_general = {'stages': [[l] for l in np.unique(times)[::-1]]}\n",
    "temporal_temporalBrain = {'stages': [\n",
    "    ['EaFet1'],\n",
    "    ['EaFet2'],\n",
    "    ['LaFet1'],\n",
    "    ['LaFet2'],\n",
    "    ['Inf1'],\n",
    "    ['Inf2'],\n",
    "    ['Child1'],\n",
    "    ['Child2'],\n",
    "    ['Adol1'],\n",
    "    ['Adol2'],\n",
    "    ['Adult1'],\n",
    "    ['Adult2'],\n",
    "]}\n",
    "temporal += [temporal_general]\n",
    "temporal += [temporal_temporalBrain]\n",
    "# Choose stage order\n",
    "temporal = temporal[args.temporal_key]\n",
    "\n",
    "# Perturbation extras\n",
    "perturbation_feature_names = [[fnames[pf] for pf in pfs] for pfs, fnames in zip(perturbation_features, features)]\n",
    "perturbation_feature_tuples = [(i, f, n) for i, (fs, ns) in enumerate(zip(perturbation_features, perturbation_feature_names)) for f, n in zip(fs, ns)]\n",
    "perturbation_feature_tuples = [(*pft, i) for i, pft in enumerate(perturbation_feature_tuples)]\n",
    "\n",
    "# Initialize memories\n",
    "memories = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T18:27:25.157740Z",
     "iopub.status.busy": "2025-01-15T18:27:25.157597Z",
     "iopub.status.idle": "2025-01-15T18:27:48.169953Z",
     "shell.execute_reply": "2025-01-15T18:27:48.169487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing memories\n"
     ]
    }
   ],
   "source": [
    "# Load memories\n",
    "fname =                                     f'{args.run_id}'\n",
    "if args.stage is not None: fname +=         f'_{args.stage:02}'\n",
    "fname +=                                    f'_{config[\"data\"][\"dataset\"]}'\n",
    "fname +=                                    f'_memories.pkl.gzip'\n",
    "\n",
    "# Load memories\n",
    "if os.path.exists(fname):\n",
    "    print('Loading existing memories')\n",
    "    with gzip.open(fname, 'rb') as f: memories = pickle.load(f)\n",
    "\n",
    "# Run simulation if needed\n",
    "for ak in args.analysis_key:\n",
    "    if ak not in memories or args.force:\n",
    "        print(f'Running {ak} simulation')\n",
    "\n",
    "        # Profiling\n",
    "        profile = False\n",
    "        if profile: torch.cuda.memory._record_memory_history(max_entries=100000)\n",
    "\n",
    "        # Choose state manager\n",
    "        state_manager = state_manager_class[ak](\n",
    "            device=DEVICE,\n",
    "            discovery=discovery,\n",
    "            temporal=temporal,\n",
    "            perturbation_features=perturbation_features,\n",
    "            modal_targets=env.reward_distance_target,\n",
    "            num_nodes=modalities[0].shape[0],\n",
    "            dim=env.dim,\n",
    "            # vel_threshold=1e-1,  # Temporal testing\n",
    "        )\n",
    "\n",
    "        # Utility parameters\n",
    "        get_current_stage = lambda: (\n",
    "            state_manager.current_stage\n",
    "            if np.array([ak in akt for akt in ('temporal', 'perturbation')]).any()\n",
    "            else -1\n",
    "        )\n",
    "        get_max_stage = lambda: (\n",
    "            len(temporal['stages'])-1 if ak == 'temporal'\n",
    "            else sum([len(pf) for pf in perturbation_features])+1 if ak == 'perturbation'\n",
    "            else -1\n",
    "        )\n",
    "        # TODO: Make perturbation more memory-efficient\n",
    "        use_modalities = np.array([ak in akt for akt in ('perturbation',)]).any()\n",
    "\n",
    "        # Initialize\n",
    "        env.set_modalities(modalities); env.reset(); memories[ak] = defaultdict(lambda: [])\n",
    "\n",
    "        # Modify\n",
    "        state_vars, end = state_manager(\n",
    "            # present=present,\n",
    "            state=env.get_state(),\n",
    "            modalities=ppc.cast(ppc.inverse_transform(ppc.inverse_cast(modalities)), device='cpu') if use_modalities else modalities,\n",
    "            labels=labels,\n",
    "            times=times,\n",
    "        )\n",
    "        present = state_vars['present']\n",
    "        memory_mask = present if optimize_memory else torch.ones_like(present, device=DEVICE)\n",
    "        full_state = state_vars['state']\n",
    "        env.set_state(full_state[memory_mask])\n",
    "        raw_modalities = state_vars['modalities']\n",
    "        processed_modalities = [m[memory_mask.cpu()] for m in raw_modalities]\n",
    "        if use_modalities: processed_modalities = ppc.cast(ppc.transform(ppc.inverse_cast(processed_modalities)))\n",
    "        env.set_modalities(processed_modalities)\n",
    "\n",
    "        # Continue initializing\n",
    "        memories[ak]['present'].append(present.cpu())\n",
    "        memories[ak]['states'].append(full_state.cpu())\n",
    "        memories[ak]['stages'].append(get_current_stage())\n",
    "        memories[ak]['rewards'].append(torch.zeros(modalities[0].shape[0]))\n",
    "\n",
    "        # Simulate\n",
    "        get_desc = lambda ts, st: f'\\tTimestep {ts}' + (f', Stage {st+1}/{get_max_stage()+1}' if st != -1 else '')\n",
    "        timestep = 0; pbar = tqdm(ascii=True, desc=get_desc(timestep, get_current_stage()), ncols=100)  # CLI\n",
    "        while True:\n",
    "            # Step\n",
    "            state = env.get_state(include_modalities=True)\n",
    "            actions = torch.zeros((modalities[0].shape[0], env.dim), device=DEVICE)\n",
    "            actions[present] = policy.act_macro(\n",
    "                state if optimize_memory else state[present],\n",
    "                keys=torch.arange(modalities[0].shape[0], device=DEVICE)[present],\n",
    "                max_batch=config['train']['max_batch'],\n",
    "            )\n",
    "            rewards = torch.zeros(modalities[0].shape[0], device=DEVICE)\n",
    "            new_rewards, _, _ = env.step(actions[present] if optimize_memory else actions, return_itemized_rewards=True)\n",
    "            if optimize_memory: rewards[present] = new_rewards\n",
    "            else: rewards = new_rewards\n",
    "            full_state[present] = env.get_state() if optimize_memory else env.get_state()[present]\n",
    "            if not optimize_memory: env.set_state(full_state)  # Don't move un-spawned nodes\n",
    "\n",
    "            # Modify\n",
    "            state_vars, end = state_manager(\n",
    "                present=present,\n",
    "                state=full_state,\n",
    "                modalities=raw_modalities,\n",
    "                labels=labels,\n",
    "                times=times,\n",
    "            )\n",
    "            present_change = (state_vars['present'] != present).any()\n",
    "            present = state_vars['present']\n",
    "            memory_mask = present if optimize_memory else torch.ones_like(present, device=DEVICE)\n",
    "            full_state = state_vars['state']\n",
    "            env.set_state(full_state[memory_mask])\n",
    "            # Only modify if changes\n",
    "            if (\n",
    "                torch.tensor([(rm != svm).any() for rm, svm in zip(raw_modalities, state_vars['modalities'])]).any()\n",
    "                or (optimize_memory and present_change)\n",
    "            ):\n",
    "                raw_modalities = state_vars['modalities']\n",
    "                processed_modalities = [m[memory_mask.cpu()] for m in raw_modalities]\n",
    "                if use_modalities: processed_modalities = ppc.cast(ppc.transform(ppc.inverse_cast(processed_modalities)))\n",
    "                env.set_modalities(processed_modalities)\n",
    "\n",
    "            # Record\n",
    "            memories[ak]['present'].append(present.cpu())\n",
    "            memories[ak]['states'].append(full_state.cpu())\n",
    "            memories[ak]['stages'].append(get_current_stage())\n",
    "            memories[ak]['rewards'].append(rewards.cpu())\n",
    "\n",
    "            # CLI\n",
    "            timestep += 1\n",
    "            update_timestep = 10\n",
    "            if timestep % update_timestep == 0:\n",
    "                pbar.set_description(get_desc(timestep, get_current_stage()))\n",
    "                pbar.update(update_timestep)\n",
    "\n",
    "            # End\n",
    "            if end: break\n",
    "\n",
    "        # CLI\n",
    "        pbar.close()\n",
    "\n",
    "        # Stack\n",
    "        memories[ak]['present'] = torch.stack(memories[ak]['present'])\n",
    "        memories[ak]['states'] = torch.stack(memories[ak]['states'])\n",
    "        memories[ak]['stages'] = torch.tensor(memories[ak]['stages'])\n",
    "        memories[ak]['rewards'] = torch.stack(memories[ak]['rewards'])\n",
    "        memories[ak] = dict(memories[ak])\n",
    "\n",
    "        # Profiling\n",
    "        if profile:\n",
    "            torch.cuda.memory._dump_snapshot('cuda_profile.pkl')\n",
    "            torch.cuda.memory._record_memory_history(enabled=None)\n",
    "\n",
    "        # Save into half-accuracy gzip (8,506 KB -> 3,236 KB)\n",
    "        print(f'\\tSaving memories')\n",
    "        compressed_type = torch.float16\n",
    "        with gzip.open(fname, 'wb') as f:\n",
    "            func_attr = lambda attr: attr.type(compressed_type) if attr.dtype not in (torch.long, torch.bool) else attr\n",
    "            celltrip.utilities.dict_map(memories[ak], func_attr, inplace=True)\n",
    "            pickle.dump(memories, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics convergence\n",
      "\tSteps per Stage: -1 (1000)\n",
      "\tCompressed Memory Sizes\n",
      "\t\tpresent size\t0.002 Gb\n",
      "\t\tstates size\t0.024 Gb\n",
      "\t\tstages size\t0.000 Gb\n",
      "\t\trewards size\t0.004 Gb\n",
      "\tAverage Reward: 0.037\n",
      "Statistics discovery\n",
      "\tSteps per Stage: -1 (1000)\n",
      "\tCompressed Memory Sizes\n",
      "\t\tpresent size\t0.002 Gb\n",
      "\t\tstates size\t0.024 Gb\n",
      "\t\tstages size\t0.000 Gb\n",
      "\t\trewards size\t0.004 Gb\n",
      "\tAverage Reward: 0.077\n",
      "Statistics temporal\n",
      "\tSteps per Stage: 0 (330), 1 (500), 2 (350), 3 (293), 4 (393), 5 (197)\n",
      "\tCompressed Memory Sizes\n",
      "\t\tpresent size\t0.004 Gb\n",
      "\t\tstates size\t0.050 Gb\n",
      "\t\tstages size\t0.000 Gb\n",
      "\t\trewards size\t0.008 Gb\n",
      "\tAverage Reward: 0.025\n",
      "Statistics perturbation\n",
      "\tSteps per Stage: 0 (1000), 1 (274), 2 (279), 3 (265), 4 (500), 5 (211), 6 (273), 7 (312), 8 (385), 9 (320), 10 (347), 11 (294), 12 (333), 13 (500), 14 (191), 15 (358), 16 (202), 17 (201), 18 (211), 19 (291), 20 (191), 21 (246), 22 (251), 23 (263), 24 (173), 25 (229), 26 (190), 27 (341), 28 (500), 29 (396), 30 (364), 31 (263), 32 (328), 33 (237), 34 (336), 35 (361), 36 (300), 37 (317), 38 (500), 39 (221), 40 (230), 41 (499), 42 (207), 43 (326), 44 (261), 45 (383), 46 (196), 47 (500), 48 (361), 49 (224), 50 (236), 51 (265), 52 (271), 53 (233), 54 (273), 55 (246), 56 (307), 57 (267), 58 (219), 59 (303), 60 (244), 61 (202), 62 (327), 63 (397), 64 (284), 65 (267), 66 (251), 67 (279), 68 (250), 69 (348), 70 (403), 71 (275), 72 (252), 73 (358), 74 (300), 75 (342), 76 (299), 77 (359), 78 (274), 79 (226), 80 (391), 81 (195), 82 (343), 83 (186), 84 (316), 85 (256), 86 (258), 87 (283), 88 (253), 89 (298), 90 (281), 91 (429), 92 (500), 93 (273), 94 (233), 95 (352), 96 (243), 97 (261), 98 (340), 99 (246), 100 (260), 101 (255), 102 (210), 103 (377), 104 (367), 105 (226), 106 (204), 107 (265), 108 (455), 109 (227), 110 (263), 111 (194), 112 (194), 113 (207), 114 (256), 115 (226), 116 (241), 117 (210), 118 (210), 119 (209), 120 (331), 121 (236), 122 (247), 123 (288), 124 (285), 125 (399), 126 (400), 127 (193), 128 (269), 129 (272), 130 (462), 131 (199), 132 (327), 133 (201), 134 (339), 135 (193), 136 (400), 137 (291), 138 (363), 139 (214), 140 (457), 141 (239), 142 (309), 143 (177), 144 (215), 145 (218), 146 (269), 147 (208), 148 (190), 149 (382), 150 (213), 151 (470), 152 (390), 153 (320), 154 (393), 155 (270), 156 (500), 157 (386), 158 (251), 159 (280), 160 (500), 161 (214), 162 (459), 163 (199), 164 (301), 165 (443), 166 (296), 167 (249), 168 (225), 169 (190), 170 (194), 171 (207), 172 (274), 173 (261), 174 (183), 175 (500), 176 (380), 177 (322), 178 (312), 179 (500), 180 (273), 181 (252), 182 (241), 183 (285), 184 (253), 185 (238), 186 (308), 187 (316), 188 (207), 189 (267), 190 (189), 191 (199), 192 (315), 193 (366), 194 (214), 195 (323), 196 (256), 197 (325), 198 (357), 199 (260), 200 (280), 201 (302), 202 (213), 203 (222), 204 (338), 205 (276), 206 (277), 207 (500), 208 (200), 209 (220), 210 (391), 211 (436), 212 (236), 213 (190), 214 (488), 215 (274), 216 (231), 217 (245), 218 (237), 219 (281), 220 (249), 221 (253), 222 (263), 223 (276), 224 (444), 225 (342), 226 (221), 227 (387), 228 (271), 229 (229), 230 (419), 231 (418), 232 (316), 233 (317), 234 (226), 235 (366), 236 (241), 237 (181), 238 (293), 239 (229), 240 (215), 241 (229), 242 (193), 243 (232), 244 (231), 245 (500), 246 (406), 247 (500), 248 (500), 249 (348), 250 (337), 251 (378), 252 (236), 253 (261)\n",
      "\tCompressed Memory Sizes\n",
      "\t\tpresent size\t0.151 Gb\n",
      "\t\tstates size\t1.815 Gb\n",
      "\t\tstages size\t0.001 Gb\n",
      "\t\trewards size\t0.302 Gb\n",
      "\tAverage Reward: -0.001\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "for ak in args.analysis_key:\n",
    "    print(f'Statistics {ak}')\n",
    "\n",
    "    ## Stages\n",
    "    stages, counts = np.unique(memories[ak]['stages'], return_counts=True)\n",
    "    print('\\tSteps per Stage: ' + ', '.join([f'{s} ({c})' for s, c in zip(stages, counts)]))\n",
    "        \n",
    "    ## Memory\n",
    "    print('\\tCompressed Memory Sizes')\n",
    "    for k in memories[ak]:\n",
    "        t_size = sum([t.element_size() * t.nelement() if isinstance(t, torch.Tensor) else 64/8 for t in memories[ak][k]]) / 1024**3\n",
    "        print(f'\\t\\t{k} size\\t{t_size:.3f} Gb')\n",
    "\n",
    "    ## Performance\n",
    "    print(f'\\tAverage Reward: {memories[ak][\"rewards\"].cpu().mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing transforms\n",
      "\tSteady states\n",
      "\tOriginal data\n",
      "\tPinning CellTRIP results to feature space\n"
     ]
    }
   ],
   "source": [
    "# Make transforms\n",
    "print('Initializing transforms')\n",
    "\n",
    "# Case variables\n",
    "total_representatives = ('convergence', 'discovery', 'perturbation')\n",
    "total_representative = total_representatives[\n",
    "    np.argwhere(np.isin(total_representatives, args.analysis_key))[0][0]\n",
    "] if np.isin(total_representatives, args.analysis_key).any() else None\n",
    "temporal_representatives = ('temporal',)\n",
    "temporal_representative = temporal_representatives[\n",
    "    np.argwhere(np.isin(temporal_representatives, args.analysis_key))[0][0]\n",
    "] if np.isin(temporal_representatives, args.analysis_key).any() else None\n",
    "\n",
    "# Steady states (`steady_state`)\n",
    "print('\\tSteady states')\n",
    "steady_state = defaultdict(lambda: [])\n",
    "# Temporal case\n",
    "if temporal_representative:\n",
    "    # Cast states, etc.\n",
    "    present = memories[temporal_representative]['present'].cpu().numpy()\n",
    "    states = memories[temporal_representative]['states'].cpu().numpy()\n",
    "    stages = memories[temporal_representative]['stages'].cpu().numpy()\n",
    "    rewards = memories[temporal_representative]['rewards'].cpu().numpy()\n",
    "    # Get last idx for each stage\n",
    "    stage_unique, stage_idx = np.unique(stages[::-1], return_index=True)\n",
    "    stage_idx = stages.shape[0] - stage_idx - 1\n",
    "    # Record\n",
    "    for temporal_num, temporal_stage in enumerate(temporal['stages']):\n",
    "        steady_state['temporal'].append(states[stage_idx[temporal_num], present[stage_idx[temporal_num]]])\n",
    "# Default case\n",
    "if total_representative:\n",
    "    # Cast states, etc.\n",
    "    present = memories[total_representative]['present'].cpu().numpy()\n",
    "    states = memories[total_representative]['states'].cpu().numpy()\n",
    "    stages = memories[total_representative]['stages'].cpu().numpy()\n",
    "    rewards = memories[total_representative]['rewards'].cpu().numpy()\n",
    "    # Get last idx for each stage\n",
    "    stage_unique, stage_idx = np.unique(stages[::-1], return_index=True)\n",
    "    stage_idx = stages.shape[0] - stage_idx - 1\n",
    "    # Record\n",
    "    steady_state['total'].append(states[stage_idx[0]])\n",
    "\n",
    "# Original data transforms (`transform_original`)\n",
    "print('\\tOriginal data')\n",
    "transform_original_2d = defaultdict(lambda: [])\n",
    "# Base\n",
    "def temp_func(m):\n",
    "    reducer = umap.UMAP(n_components=2, n_jobs=1, random_state=notebook_seed).fit(m)\n",
    "    return lambda x: reducer.transform(x)\n",
    "# Temporal case\n",
    "if temporal_representative:\n",
    "    for temporal_stage in temporal['stages']:\n",
    "        for m in modalities:\n",
    "            m = m.detach().cpu().numpy()\n",
    "            mask = np.isin(times, temporal_stage)\n",
    "            transform_original_2d['temporal'].append(celltrip.utilities.LazyComputation(temp_func, m[mask]))\n",
    "# Default case\n",
    "if total_representative:\n",
    "    for m in modalities:\n",
    "        m = m.detach().cpu().numpy()\n",
    "        transform_original_2d['total'].append(celltrip.utilities.LazyComputation(temp_func, m))\n",
    "\n",
    "# Single modality imputation pinning functions (`transform_pin`)\n",
    "print(f'\\tPinning CellTRIP results to feature space')\n",
    "transform_pin = defaultdict(lambda: [])\n",
    "# Base\n",
    "def temp_func(source_points):\n",
    "    # TODO: Add train-val\n",
    "    # PCA into desired dimensions\n",
    "    pinning_reducer_transform = lambda x: x\n",
    "    # if ss.shape[1] != modalities[env.reward_distance_target[0]].shape[1]:\n",
    "    #     pinning_reducer = sklearn.decomposition.PCA(n_components=modalities[env.reward_distance_target[0]].shape[1])\n",
    "    #     pinning_reducer_transform = lambda x: pinning_reducer.transform(x)\n",
    "    #     ss = pinning_reducer.fit_transform(ss)\n",
    "    target_points = modalities[env.reward_distance_target[0]].detach().cpu().numpy()\n",
    "    # Solve for transformation\n",
    "    A = np.hstack([source_points, np.ones((source_points.shape[0], 1))])\n",
    "    B = target_points\n",
    "    pinning_matrix = np.linalg.lstsq(A, B, rcond=None)[0]\n",
    "    # Define transformation function\n",
    "    def pin_points(points):\n",
    "        reduced_points = pinning_reducer_transform(points.reshape([-1, points.shape[-1]])).reshape([*points.shape[:-1], -1])\n",
    "        A = np.concatenate([reduced_points, np.ones((*reduced_points.shape[:-1], 1))], axis=-1)\n",
    "        return np.dot(A, pinning_matrix)\n",
    "    return pin_points\n",
    "# Temporal case\n",
    "if temporal_representative:\n",
    "    for temporal_num, temporal_stage in enumerate(temporal['stages']):\n",
    "        transform_pin['temporal'].append(celltrip.utilities.LazyComputation(temp_func, steady_state['temporal'][temporal_num][:, :env.dim]))\n",
    "# Default case\n",
    "if total_representative:\n",
    "    transform_pin['total'].append(celltrip.utilities.LazyComputation(temp_func, steady_state['total'][0][:, :env.dim]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting static analyses\n"
     ]
    }
   ],
   "source": [
    "print('Plotting static analyses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_methods(prefix):\n",
    "    # Get other methods\n",
    "    method_results = {}\n",
    "    try:\n",
    "        method_dir = os.path.join(BASE_FOLDER, '../other_methods/runs', config['data']['dataset'])\n",
    "        method_names = next(os.walk(method_dir))[1]\n",
    "    except: method_names = []\n",
    "    for name in method_names:\n",
    "        # Get output files\n",
    "        files = os.listdir(os.path.join(method_dir, name))\n",
    "        r = re.compile(f'^{prefix}(\\d+)(?:_(\\d+))?.txt$')\n",
    "        files = list(filter(r.match, files))\n",
    "\n",
    "        # Record\n",
    "        for file in files:\n",
    "            modality, seed = r.match(file)[1], r.match(file)[2]\n",
    "            method_results[(name, modality, seed)] = os.path.join(method_dir, name, file)\n",
    "\n",
    "    return method_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining rewards\n"
     ]
    }
   ],
   "source": [
    "print('\\tTraining rewards')\n",
    "\n",
    "# Load history from wandb\n",
    "history = run.history(samples=2e3)\n",
    "history['timestep'] = history['end_timestep']\n",
    "history['Runtime (h)'] = history['_runtime'] / 60**2\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(18, 2), layout='constrained')\n",
    "def plot_without_zeros(x, y, **kwargs):\n",
    "    y = y.copy(); y[y==0] = np.nan\n",
    "    ax.plot(x, y, **kwargs)\n",
    "ax.plot(history['timestep'], history['average_reward'], color='black', lw=3, label='Average Reward')\n",
    "plot_without_zeros(history['timestep'], history['rewards/bound'], color='red', alpha=.75, lw=2, label='Boundary Penalty')\n",
    "plot_without_zeros(history['timestep'], history['rewards/velocity'], color='goldenrod', alpha=.75, lw=2, label='Velocity Penalty')\n",
    "plot_without_zeros(history['timestep'], history['rewards/action'], color='green', alpha=.75, lw=2, label='Action Penalty')\n",
    "plot_without_zeros(history['timestep'], history['rewards/distance'], color='blue', alpha=.75, lw=2, label='Distance Reward')\n",
    "plot_without_zeros(history['timestep'], history['rewards/origin'], color='darkorange', alpha=.75, lw=2, label='Origin Reward')\n",
    "\n",
    "# Stage ticks (first of each stage)\n",
    "unique, stage_idx = np.unique(history['stage'][::-1], return_index=True)\n",
    "stage_idx = len(history['stage']) - stage_idx\n",
    "stage_idx = stage_idx[:-1]\n",
    "[ax.axvline(x=history['timestep'][idx], color='black', alpha=.5, linestyle='dashed', lw=1) for idx in stage_idx]\n",
    "\n",
    "# Stage labels\n",
    "stage_titles = ['Boundary', 'Origin', 'Action & Velocity', 'Distance']\n",
    "for i, st in enumerate(stage_titles):\n",
    "    lower = history['timestep'][stage_idx[i-1]] if i != 0 else 0\n",
    "    upper = history['timestep'][stage_idx[i]]\n",
    "    center = (upper + lower) / 2\n",
    "    trans = mpl.transforms.blended_transform_factory(ax.transData, ax.transAxes)\n",
    "    ax.text(center, .05, st, ha='center', va='bottom', alpha=.75, transform=trans)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel('Timestep')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.legend(loc='lower right', ncols=3)\n",
    "\n",
    "# Styling\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "ax.set_xlim([0, history['timestep'].max()])\n",
    "\n",
    "# Save plot\n",
    "fname = f'{args.run_id}_{config[\"data\"][\"dataset\"]}_loss.pdf'\n",
    "fig.savefig(os.path.join(PLOT_FOLDER, fname), transparent=True, dpi=300)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tOriginal data visualization\n"
     ]
    }
   ],
   "source": [
    "if args.original:\n",
    "    print(f'\\tOriginal data visualization')\n",
    "\n",
    "    # Get vertical sections\n",
    "    times_to_use = times if types[0].shape[1] > 1 else -np.ones_like(times)\n",
    "    unique_times = np.unique(times_to_use)\n",
    "\n",
    "    # Create figure\n",
    "    fig_shape = (len(unique_times), len(modalities))\n",
    "    fig, axs = plt.subplots(*fig_shape, figsize=[8*sh for sh in fig_shape[::-1]])\n",
    "    axs = axs.reshape(fig_shape)\n",
    "\n",
    "    # Plot\n",
    "    for i, time in enumerate(unique_times):\n",
    "        for j, m in enumerate(modalities):\n",
    "            # Generate UMAP\n",
    "            m = m.detach().cpu().numpy()\n",
    "            m_reduced = transform_original_2d['temporal' if len(unique_times) > 1 else 'total'][j](m[times_to_use == time])\n",
    "\n",
    "            # Plot\n",
    "            for l in np.unique(labels):\n",
    "                axs[i][j].scatter(*m_reduced[labels == l].T, label=l)\n",
    "\n",
    "            # Labels\n",
    "            if i == 0: axs[0][j].set_title(f'Modality {j+1}')\n",
    "\n",
    "            # Styling\n",
    "            axs[i][j].spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "        # Labels\n",
    "        if time != -1: axs[i][0].set_ylabel(time)\n",
    "\n",
    "    # Styling\n",
    "    axs[0][-1].legend()\n",
    "\n",
    "    # Save plot\n",
    "    fname =                                     f'{args.run_id}'\n",
    "    if args.stage is not None: fname +=         f'_{args.stage:02}'\n",
    "    fname +=                                    f'_{config[\"data\"][\"dataset\"]}'\n",
    "    fname +=                                    f'_original.pdf'\n",
    "    fig.savefig(os.path.join(PLOT_FOLDER, fname), transparent=True, dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison metrics\n",
    "metric_rand = lambda X: np.random.rand()\n",
    "metric_silhouette = lambda X: sklearn.metrics.silhouette_score(X, labels)\n",
    "metric_ch_score = lambda X: sklearn.metrics.calinski_harabasz_score(X, labels)\n",
    "def metric_knn_ami(X):\n",
    "    knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=10)\n",
    "    knn.fit(X, labels)\n",
    "    pred = knn.predict(X)\n",
    "    return sklearn.metrics.adjusted_mutual_info_score(labels, pred)\n",
    "\n",
    "# Metric metadata\n",
    "metric_tuples = {\n",
    "    'rand': (metric_rand, {'label': 'Random'}),\n",
    "    'sc': (metric_silhouette, {'label': 'Silhouette Coefficient'}),\n",
    "    'knn_ami': (metric_knn_ami, {'label': 'KNN Adjusted Mutual Information'}),\n",
    "    'ch': (metric_ch_score, {'label': 'Calinski Harabasz Index', 'scale': 'log'}),\n",
    "}\n",
    "\n",
    "# Metric selection per analysis type\n",
    "comparison_dict = {\n",
    "    'integration': {\n",
    "        'prefix': 'P',\n",
    "        'metrics': (metric_tuples['ch'], metric_tuples['knn_ami']),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Integration method comparison\n",
    "desired_application = 'integration'\n",
    "if 'convergence' in args.analysis_key and application_type == desired_application:\n",
    "    print(f'\\tIntegration performance comparison')\n",
    "\n",
    "    # Select metrics\n",
    "    (metric_x, kwargs_x), (metric_y, kwargs_y) = comparison_dict[desired_application]['metrics']\n",
    "\n",
    "    # Get other methods\n",
    "    method_results = get_other_methods(comparison_dict[desired_application][\"prefix\"])\n",
    "\n",
    "    # Add CellTRIP\n",
    "    method_results[('CellTRIP', '-1', notebook_seed)] = memories['convergence']['states'][-1].detach().cpu()\n",
    "\n",
    "    # Compile and calculate performances\n",
    "    raw_performance = pd.DataFrame(columns=['Method', 'Modality', 'Seed', 'x', 'y'])\n",
    "    for key, fname in method_results.items():\n",
    "        method, modality, seed = key\n",
    "        if method == 'CellTRIP': data = fname\n",
    "        else: data = np.loadtxt(fname)\n",
    "        raw_performance.loc[raw_performance.shape[0]] = [*key, metric_x(data), metric_y(data)]\n",
    "\n",
    "    # Aggregate to group statistics\n",
    "    group = raw_performance.groupby(['Method', 'Modality'])\n",
    "    group_mean = group[['x', 'y']].mean().rename(columns=lambda n: f'{n}_mean')\n",
    "    group_var = group[['x', 'y']].var().rename(columns=lambda n: f'{n}_var')\n",
    "    group_count = group[['x']].count().rename(columns={'x': 'Count'})\n",
    "    performance = group_mean.join(group_var).join(group_count).fillna(0).reset_index()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'\\t\\t{\"Method\":<10}\\tModal\\tx Mean\\tx Var\\ty Mean\\ty Var\\tCount')\n",
    "    for i, r in performance.sort_values('Modality').iterrows():\n",
    "        print(f'\\t\\t{r[\"Method\"]:<8}\\t{r[\"Modality\"]}\\t{r[\"x_mean\"]:.3f}\\t{r[\"x_var\"]:.3f}\\t{r[\"y_mean\"]:.3f}\\t{r[\"y_var\"]:.3f}\\t{r[\"Count\"]}')\n",
    "\n",
    "    # Plot with text\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6), sharex=True, layout='constrained')\n",
    "    method_colors = {}\n",
    "    annotations = []\n",
    "    for i, r in performance.iterrows():\n",
    "        # Set color\n",
    "        if r['Method'] not in method_colors: method_colors[r['Method']] = sns.color_palette()[len(method_colors)]\n",
    "        \n",
    "        # Plot\n",
    "        ax.scatter(\n",
    "            r['x_mean'],\n",
    "            r['y_mean'],\n",
    "            color=method_colors[r['Method']],\n",
    "            s=100,\n",
    "        )\n",
    "\n",
    "        # Cross lines\n",
    "        ax.plot([r['x_mean']-r['x_var'], r['x_mean']+r['x_var']], 2*[r['y_mean']], ls='--', color='gray', zorder=.3)\n",
    "        ax.plot(2*[r['x_mean']], [r['y_mean']-r['y_var'], r['y_mean']+r['y_var']], ls='--', color='gray', zorder=.3)\n",
    "\n",
    "        # Annotate\n",
    "        text = f'{r[\"Method\"]}' + (f' ({r[\"Modality\"]})' if r['Modality'] != '-1' else '')\n",
    "        color = 'black' if r['Method'] != 'CellTRIP' else 'red'\n",
    "        annotations.append(ax.text(\n",
    "            r['x_mean'], r['y_mean'], text,\n",
    "            ha='center', va='center', color=color, fontsize='large'))\n",
    "\n",
    "    # Styling\n",
    "    # ax.spines[['right', 'top', 'bottom', 'left']].set_visible(False)\n",
    "    # ax.axvline(x=0, ls='-', alpha=.6, color='black', zorder=.1)\n",
    "    # ax.axhline(y=0, ls='-', alpha=.6, color='black', zorder=.1)\n",
    "    ax.set(\n",
    "        **{'x'+k: v for k, v in kwargs_x.items()},\n",
    "        **{'y'+k: v for k, v in kwargs_y.items()},\n",
    "    )\n",
    "    ax.tick_params(axis='both', which='both', bottom=True, left=True)\n",
    "\n",
    "    # Adjust Annotation Positions\n",
    "    from adjustText import adjust_text\n",
    "    adjust_text(\n",
    "        annotations,\n",
    "        # add_objects=ax.get_children()[0],\n",
    "        expand=(2, 3), \n",
    "        arrowprops=dict(\n",
    "            arrowstyle='-|>',\n",
    "            mutation_scale=10,\n",
    "            shrinkA=2, shrinkB=7,\n",
    "            color='black',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Save plot\n",
    "    fname =                                     f'{args.run_id}'\n",
    "    if args.stage is not None: fname +=         f'_{args.stage:02}'\n",
    "    fname +=                                    f'_{config[\"data\"][\"dataset\"]}'\n",
    "    fname +=                                    f'_comparison'\n",
    "    fname +=                                    f'_{desired_application}.pdf'\n",
    "    fig.savefig(os.path.join(PLOT_FOLDER, fname), transparent=True, dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine imputation method order\n",
    "imputation_order = np.unique([k[0] for k in get_other_methods('I')]).tolist() + ['CellTRIP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSingle modality imputation performance comparison\n"
     ]
    }
   ],
   "source": [
    "# Single modality imputation\n",
    "if total_representative and len(env.reward_distance_target) == 1:\n",
    "    print(f'\\tSingle modality imputation performance comparison')\n",
    "    # TODO: Add inverse transform here\n",
    "    \n",
    "    # Get other methods\n",
    "    method_results = get_other_methods('I')\n",
    "    # Add CellTRIP\n",
    "    # Multiplication undoes CellTRIP's feature scaling in `euclidean_distance`\n",
    "    method_results[('CellTRIP', str(env.reward_distance_target[0]+1), notebook_seed)] = transform_pin['total'][0](\n",
    "        steady_state['total'][0][:, :3] * np.sqrt(modalities[env.reward_distance_target[0]].shape[1]))\n",
    "    # Calculate modal dist\n",
    "    # raw_modalities = ppc.cast(ppc.inverse_transform(ppc.inverse_cast(modalities)))\n",
    "    modal_dist = [celltrip.utilities.euclidean_distance(m) for m in modalities]\n",
    "    # Add target data\n",
    "    target_points = modalities[env.reward_distance_target[0]].cpu().numpy()\n",
    "\n",
    "    # Compile and calculate performances\n",
    "    raw_performance = None\n",
    "    for key, fname in method_results.items():\n",
    "        method, modality, seed = key\n",
    "        if method == 'CellTRIP': data = fname\n",
    "        else: data = np.loadtxt(fname)\n",
    "        data = torch.Tensor(data)\n",
    "\n",
    "        # Compute error\n",
    "        data_dist = celltrip.utilities.euclidean_distance(data)\n",
    "        sample_mse = (data_dist - modal_dist[int(modality)-1].cpu()).square().mean(dim=-1)\n",
    "        feature_mse = (data_dist - modal_dist[int(modality)-1].cpu()).square().mean(dim=0)\n",
    "        raw_sample_mse = ((data - target_points)**2).mean(dim=-1)\n",
    "        raw_feature_mse = ((data - target_points)**2).mean(dim=0)\n",
    "\n",
    "        # Record\n",
    "        df = pd.DataFrame({'Method': method, 'Modality': modality, 'Seed': seed, 'Metric': 'Inter-Cell MSE', 'Value': sample_mse})\n",
    "        df = pd.concat((df, pd.DataFrame({'Method': method, 'Modality': modality, 'Seed': seed, 'Metric': 'Feature Inter-Cell MSE', 'Value': feature_mse})))\n",
    "        df = pd.concat((df, pd.DataFrame({'Method': method, 'Modality': modality, 'Seed': seed, 'Metric': 'MSE', 'Value': raw_sample_mse})))\n",
    "        df = pd.concat((df, pd.DataFrame({'Method': method, 'Modality': modality, 'Seed': seed, 'Metric': 'Feature MSE', 'Value': raw_feature_mse})))\n",
    "        if raw_performance is None: raw_performance = df\n",
    "        else: raw_performance = pd.concat((raw_performance, df), ignore_index=True, axis=0)\n",
    "\n",
    "    # Fill NA (For non-random methods)\n",
    "    raw_performance = raw_performance.fillna(0)\n",
    "\n",
    "    # Plot performances\n",
    "    metrics_to_plot = ('Inter-Cell MSE', 'MSE')\n",
    "    fig, axs = plt.subplots(\n",
    "        1, len(metrics_to_plot), figsize=(3*len(metrics_to_plot), 4),\n",
    "        # sharey=True,\n",
    "        layout='constrained')\n",
    "    for i, (ax, metric) in enumerate(zip(axs, metrics_to_plot)):\n",
    "        # Filter to only the best result from each method\n",
    "        best_seeds = (\n",
    "            raw_performance.loc[raw_performance['Metric'] == metric]\n",
    "            .groupby(['Method', 'Modality', 'Seed'])[['Value']].mean().reset_index()\n",
    "            .sort_values('Value', ascending=False).groupby(['Method', 'Modality'])[['Seed']].first().reset_index().to_numpy()\n",
    "        )\n",
    "        mask = np.zeros(raw_performance.shape[0], dtype=bool)\n",
    "        for idx in best_seeds:\n",
    "            mask += (raw_performance.to_numpy()[:, :3] == idx).all(axis=-1)\n",
    "        filtered_performance = raw_performance.iloc[mask]\n",
    "\n",
    "        # Generate visuals\n",
    "        filtered_performance = filtered_performance.loc[filtered_performance['Metric'] == metric]\n",
    "        order = filtered_performance.loc[filtered_performance['Metric'] == metric].groupby('Method')['Value'].mean().sort_values(ascending=False).index.to_list()\n",
    "        sns.boxplot(\n",
    "            data=filtered_performance,\n",
    "            x='Method', y='Value', hue='Method',\n",
    "            order=order,\n",
    "            hue_order=imputation_order,\n",
    "            ax=ax)\n",
    "        with warnings.catch_warnings(record=False) as w:\n",
    "            warnings.simplefilter('ignore')\n",
    "            sns.stripplot(\n",
    "                data=filtered_performance.sample(frac=.05),\n",
    "                x='Method', y='Value', hue='Method',\n",
    "                order=order,\n",
    "                hue_order=imputation_order,\n",
    "                size=2, palette=sns.color_palette(['black']), legend=False, dodge=False, ax=ax)\n",
    "\n",
    "        # Styling\n",
    "        ax.set(title=metric, xlabel=None, ylabel=None)\n",
    "        ax.set_yscale('log')\n",
    "        ax.spines[['right', 'top']].set_visible(False)\n",
    "        # if i == 0: ax.tick_params(axis='both', which='both', bottom=False, left=True)\n",
    "        ax.tick_params(axis='both', which='both', bottom=False, left=True)\n",
    "\n",
    "        # Xlabels\n",
    "        ax.set_xticks(ax.get_xticks())\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='center', va='baseline')\n",
    "        max_height = max([l.get_window_extent(renderer=ax.figure.canvas.get_renderer()).height for l in ax.get_xticklabels()])\n",
    "        fontsize = ax.get_xticklabels()[0].get_size()\n",
    "        pad = fontsize / 2 + max_height / 2\n",
    "        ax.tick_params(axis='x', pad=pad)\n",
    "        method_loc = np.argwhere(np.array(order) == 'CellTRIP')[0][0]\n",
    "        ax.get_xticklabels()[method_loc].set_color('red')\n",
    "\n",
    "    # Save plot\n",
    "    fname =                                     f'{args.run_id}'\n",
    "    if args.stage is not None: fname +=         f'_{args.stage:02}'\n",
    "    fname +=                                    f'_{config[\"data\"][\"dataset\"]}'\n",
    "    fname +=                                    f'_comparison'\n",
    "    fname +=                                    f'_imputation.pdf'\n",
    "    fig.savefig(os.path.join(PLOT_FOLDER, fname), transparent=True, dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Error Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tImputation method error visualization\n"
     ]
    }
   ],
   "source": [
    "# Single 2D modality imputation\n",
    "if total_representative and len(env.reward_distance_target) == 1 and modalities[env.reward_distance_target[0]].shape[1] == 2:\n",
    "    print(f'\\tImputation method error visualization')\n",
    "\n",
    "    # Get other methods\n",
    "    method_results = get_other_methods('I')\n",
    "    # Add CellTRIP\n",
    "    method_results[('CellTRIP', str(env.reward_distance_target[0]+1), notebook_seed)] = transform_pin['total'][0](steady_state['total'][0][:, :3])\n",
    "    # Add Measured\n",
    "    method_results[('Measured', str(env.reward_distance_target[0]+1), None)] = modalities[env.reward_distance_target[0]].cpu().numpy()\n",
    "\n",
    "    # Precalculate params - get polar coordinates\n",
    "    centered_points = target_points - target_points.mean(axis=0)\n",
    "    r = (centered_points**2).sum(axis=-1)**(1/2)\n",
    "    theta = np.arctan2(centered_points[:, 1], centered_points[:, 0])\n",
    "    # Aggregate colors\n",
    "    hue = theta / (2*np.pi) + .5\n",
    "    value = .2 + .8 * (r - r.min()) / (r.max() - r.min())\n",
    "    saturation = .8 * np.ones_like(r)\n",
    "    position_colors = [colorsys.hsv_to_rgb(h, s, v) for h, s, v in zip(hue, saturation, value)]\n",
    "\n",
    "    # Plot\n",
    "    scatter_kwargs = {'s': 5}\n",
    "    for i, (key, fname) in enumerate(method_results.items()):\n",
    "        method, modality, seed = key\n",
    "        if method in ('Measured', 'CellTRIP'): data = fname\n",
    "        else: data = np.loadtxt(fname)\n",
    "\n",
    "        ## Base color\n",
    "        base_color = np.array([.7, .7, .7, 1.])\n",
    "\n",
    "        ## Error colors\n",
    "        # Get errors\n",
    "        errors = ((data - target_points)**2).mean(axis=-1)\n",
    "        error_scale = 1\n",
    "        errors = (error_scale/r.std())*np.clip(errors, 0, (r.std()/error_scale))\n",
    "        error_color = np.array([1., 0., 0., 1.])\n",
    "        error_colors = error_color.reshape((1, -1)) * errors.reshape((-1, 1)) + base_color.reshape((1, -1)) * (1 - errors.reshape((-1, 1)))\n",
    "\n",
    "        # Create figure\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(8, 4), layout='constrained')\n",
    "        # Plot points\n",
    "        axs[0].scatter(*data.T, c=position_colors, **scatter_kwargs)\n",
    "        axs[1].scatter(*data.T, c=error_colors, **scatter_kwargs)\n",
    "        # Labels\n",
    "        axs[0].set_ylabel(method, color='black' if method != 'CellTRIP' else 'red')\n",
    "        axs[0].set_title('Positions')\n",
    "        axs[1].set_title('Error')\n",
    "        # Stylize\n",
    "        for ax in axs:\n",
    "            ax.set(xticklabels=[], xticks=[], yticklabels=[], yticks=[])\n",
    "            ax.spines[['right', 'top', 'bottom', 'left']].set_visible(False)\n",
    "\n",
    "        # Save plot\n",
    "        fname =                                     f'{args.run_id}'\n",
    "        if args.stage is not None: fname +=         f'_{args.stage:02}'\n",
    "        fname +=                                    f'_{config[\"data\"][\"dataset\"]}'\n",
    "        fname +=                                    f'_comparison_imputation_visualization.pdf'\n",
    "        fname +=                                    f'_{modality}_{method}'\n",
    "        if seed is not None: fname +=               f'_{seed}'\n",
    "        fname +=                                    f'.pdf'\n",
    "        fig.savefig(os.path.join(PLOT_FOLDER, fname), transparent=True, dpi=300)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCalculating feature effect size\n",
      "\t\tModality 0: Calb1 (1.65e-01), Slc17a7 (1.47e-01), Igfbp6 (1.00e-01)\n"
     ]
    }
   ],
   "source": [
    "# Perturbation significance analysis\n",
    "if 'perturbation' in args.analysis_key:\n",
    "    print('\\tCalculating feature effect size')\n",
    "    \n",
    "    # Get last idx for each stage\n",
    "    stages = memories['perturbation']['stages'].cpu().numpy()\n",
    "    unique_stages, unique_idx = np.unique(stages[::-1], return_index=True)\n",
    "    unique_idx = stages.shape[0] - unique_idx - 1\n",
    "    # unique_stages, unique_idx = unique_stages[::-1], unique_idx[::-1]\n",
    "\n",
    "    # Check memories\n",
    "    assert len(perturbation_feature_tuples) == len(unique_idx)-1, (\n",
    "        '`perturbation_features` and `memories` do not have the same '\n",
    "        'features, please run perturbation using `--force`')\n",
    "\n",
    "    # Compute effect sizes for each\n",
    "    effect_sizes = []\n",
    "    for stage, idx, pft in zip(unique_stages, unique_idx, [[]]+perturbation_feature_tuples):\n",
    "        # Get state\n",
    "        state = memories['perturbation']['states'][idx]\n",
    "\n",
    "        # Record steady state after convergence\n",
    "        if stage == 0:\n",
    "            steady_state = state\n",
    "            continue\n",
    "\n",
    "        # Move to next modality if needed, assumes triples advance modalities ortholinearly\n",
    "        while pft[0] + 1 > len(effect_sizes): effect_sizes.append([])\n",
    "\n",
    "        # Compute effect size\n",
    "        effect_size = (state[:, :env.dim] - steady_state[:, :env.dim]).square().sum(dim=-1).sqrt().mean(dim=-1).item()\n",
    "        effect_sizes[-1].append(effect_size)\n",
    "\n",
    "    # Print effect sizes\n",
    "    for i, (pfs, pfns, ess) in enumerate(zip(perturbation_features, perturbation_feature_names, effect_sizes)):\n",
    "        # Filter to top features\n",
    "        num_top = 3\n",
    "        top_idx = np.argsort(ess)[:-(num_top+1):-1]\n",
    "        pfs, pfns, ess = np.array(pfs)[top_idx], np.array(pfns)[top_idx], np.array(ess)[top_idx]\n",
    "\n",
    "        # CLI\n",
    "        print(f'\\t\\tModality {i}: ' + ', '.join([f'{pfn} ({es:.02e})' for pfn, es in zip(pfns, ess)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPerturbation visualization\n"
     ]
    }
   ],
   "source": [
    "if 'perturbation' in args.analysis_key:\n",
    "    print('\\tPerturbation visualization')\n",
    "\n",
    "    # Run for top features\n",
    "    num_top_features = 7\n",
    "    df = pd.DataFrame(\n",
    "        [(*pft, es) for pft, es in zip(perturbation_feature_tuples, sum(effect_sizes, []))],\n",
    "        columns=['Modality', 'Modal Index', 'Feature Name', 'Stage Num', 'Effect Size'])\n",
    "    df = df.sort_values('Effect Size', ascending=False)\n",
    "    df = df.groupby('Modality').head(num_top_features)\n",
    "\n",
    "    # Make plot\n",
    "    fig, axs = plt.subplots(len(env.modalities_to_return), num_top_features, figsize=(num_top_features*8, len(env.modalities_to_return)*8))\n",
    "    if len(axs.shape) < 2: axs = axs.reshape((1, -1))\n",
    "\n",
    "    modal_counts = defaultdict(lambda: 0)\n",
    "    for _, row in df.iterrows():\n",
    "        # Get ax\n",
    "        ax = axs[int(row['Modality']), modal_counts[row['Modality']]]\n",
    "        modal_counts[row['Modality']] += 1\n",
    "\n",
    "        # Get last idx for each stage\n",
    "        stages = memories['perturbation']['stages'].cpu().numpy()\n",
    "        unique_stages, unique_idx = np.unique(stages[::-1], return_index=True)\n",
    "        unique_idx = stages.shape[0] - unique_idx - 1\n",
    "\n",
    "        # Get perturbation subset\n",
    "        begin_idx = unique_idx[row['Stage Num']-1] + 1\n",
    "        end_idx = unique_idx[row['Stage Num']]\n",
    "        states = memories['perturbation']['states'].cpu().numpy()[begin_idx:end_idx+1]\n",
    "\n",
    "        # Pin to desired space\n",
    "        pinned_states = transform_pin['total'][0](states[:, :, :env.dim])\n",
    "\n",
    "        # Take means by regions\n",
    "        square_size = max([np.max(pinned_states[:, i]) - np.min(pinned_states[:, i]) for i in range(2)]) / 10.\n",
    "        xs, ys = np.meshgrid(\n",
    "            np.arange(np.min(pinned_states[:, :, 0]), np.max(pinned_states[:, :, 0]), square_size),\n",
    "            np.arange(np.min(pinned_states[:, :, 1]), np.max(pinned_states[:, :, 1]), square_size))\n",
    "        xs, ys = xs.flatten(), ys.flatten()\n",
    "        mean_states = []; total = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            state = pinned_states[0]\n",
    "            bottom_left = np.array([x, y])\n",
    "            top_right = bottom_left + square_size\n",
    "            mask = (state >= bottom_left) * (state < top_right)\n",
    "            mask = mask.prod(axis=-1).astype(bool)\n",
    "            if mask.sum() > 0:\n",
    "                mean_states.append(pinned_states[:, mask].mean(axis=1, keepdims=True))\n",
    "        mean_states = np.concatenate(mean_states, axis=1)\n",
    "\n",
    "        # Plot original data\n",
    "        for l in np.unique(labels):\n",
    "            ax.scatter(*pinned_states[0, labels == l].T, label=l, alpha=.2)\n",
    "\n",
    "        # Arrow params\n",
    "        states_to_use = mean_states\n",
    "\n",
    "        # Get most interesting arrows\n",
    "        total_movement = np.linalg.norm(states_to_use[1:] - states_to_use[:-1], ord=2, axis=-1).sum(axis=0)\n",
    "        num_top = 50\n",
    "        top_arrows = np.argsort(total_movement)[:-(num_top+1):-1]\n",
    "\n",
    "        # Plot movement\n",
    "        for i in top_arrows:\n",
    "            # Filter to non-small movements\n",
    "            threshold = 5e-3\n",
    "            moving_pos_mask = []\n",
    "            prev_pos = None\n",
    "            for pos in states_to_use[:, i]:\n",
    "                if prev_pos is None:\n",
    "                    moving_pos_mask.append(True)\n",
    "                    prev_pos = pos.copy()\n",
    "                    continue\n",
    "\n",
    "                # Calculate dist\n",
    "                dist = np.linalg.norm(pos - prev_pos, ord=2, axis=-1)\n",
    "\n",
    "                # Record\n",
    "                moving_pos_mask.append(dist > threshold)\n",
    "                if moving_pos_mask[-1]: prev_pos = pos\n",
    "\n",
    "            # Get moving states\n",
    "            moving_states = states_to_use[moving_pos_mask, i]\n",
    "\n",
    "            # Skip if too short\n",
    "            if moving_states.shape[0] < 2: continue\n",
    "\n",
    "            # Compute total movement\n",
    "            start_pos = moving_states[0]\n",
    "            end_pos = moving_states[-1]\n",
    "            diff_pos = end_pos - start_pos\n",
    "            r = (diff_pos**2).sum(axis=-1)**(1/2)\n",
    "            theta = np.arctan2(diff_pos[1], diff_pos[0])\n",
    "            # Aggregate colors\n",
    "            hue = theta / (2*np.pi) + .5\n",
    "            value = .4 + .2 * min(r / .1, 1.)\n",
    "            saturation = .8\n",
    "            color = colorsys.hsv_to_rgb(hue, saturation, value)\n",
    "\n",
    "            # Plot lines\n",
    "            ax.plot(*moving_states.T, color=color)\n",
    "\n",
    "            # Plot arrow heads\n",
    "            lookback_frames = max(int(.1*moving_states.shape[0]), 1)\n",
    "            start_pos = moving_states[-(lookback_frames+1)]\n",
    "            end_pos = moving_states[-1]\n",
    "            diff_pos = end_pos - start_pos\n",
    "            # origin = square_size * np.floor(start_pos / square_size) + square_size / 2\n",
    "            ax.arrow(*start_pos, *diff_pos, width=0, head_width=.05, color=color)\n",
    "\n",
    "        # Labels\n",
    "        ax.set_title(row['Feature Name'])\n",
    "        if modal_counts[row['Modality']] == 1: ax.set_ylabel(f'Modality {row[\"Modality\"]+1}')\n",
    "        if modal_counts[row['Modality']] == num_top_features and int(row['Modality']) == 0:\n",
    "            legend = ax.legend()\n",
    "            for lh in legend.legend_handles: lh.set_alpha(1)\n",
    "\n",
    "        # Styling\n",
    "        ax.set(xticklabels=[], xticks=[], yticklabels=[], yticks=[])\n",
    "        ax.spines[['right', 'top', 'bottom', 'left']].set_visible(False)\n",
    "\n",
    "    # Save plot\n",
    "    fname =                                     f'{args.run_id}'\n",
    "    if args.stage is not None: fname +=         f'_{args.stage:02}'\n",
    "    fname +=                                    f'_{config[\"data\"][\"dataset\"]}'\n",
    "    fname +=                                    f'_perturbation_velocity.pdf'\n",
    "    fig.savefig(os.path.join(PLOT_FOLDER, fname), transparent=True, dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPerturbation Comparison\n"
     ]
    }
   ],
   "source": [
    "if 'perturbation' in args.analysis_key:\n",
    "    print('\\tPerturbation Comparison')\n",
    "\n",
    "    # Get other methods\n",
    "    method_results = get_other_methods('F')\n",
    "    # Add CellTRIP\n",
    "    for i, ess in enumerate(effect_sizes): method_results[('CellTRIP', str(i+1), notebook_seed)] = effect_sizes[i]\n",
    "\n",
    "    # Aggregate results\n",
    "    feature_importance = None\n",
    "    for i, (key, fname) in enumerate(method_results.items()):\n",
    "        method, modality, seed = key\n",
    "        if method == 'CellTRIP': data = fname\n",
    "        else: data = np.loadtxt(fname)\n",
    "        data = np.array(data)\n",
    "\n",
    "        # Filter to requested features\n",
    "        try: idx = perturbation_features[(int(modality)-1)]\n",
    "        except: continue\n",
    "        if method != 'CellTRIP': data = data[idx]\n",
    "        # Scale\n",
    "        data /= data.sum()\n",
    "\n",
    "        # Format to df\n",
    "        df = pd.DataFrame({\n",
    "            'Method': method,\n",
    "            'Modality': modality,\n",
    "            'Seed': seed,\n",
    "            'Feature': perturbation_feature_names[(int(modality)-1)],\n",
    "            'Importance': data,\n",
    "        })\n",
    "        if feature_importance is None: feature_importance = df\n",
    "        else: feature_importance = pd.concat((feature_importance, df))\n",
    "\n",
    "    # Take average over all seeds\n",
    "    # feature_importance = feature_importance.groupby(['Method', 'Modality', 'Feature'])[['Importance']].mean().reset_index()\n",
    "\n",
    "    # Generate plots\n",
    "    for modality in env.modalities_to_return:\n",
    "        # Filter to modality\n",
    "        filtered_df = feature_importance.loc[feature_importance['Modality'] == str(modality+1)]\n",
    "        # Filter to top and sort based on CellTRIP\n",
    "        num_top = 40\n",
    "        sorted_features = filtered_df.loc[filtered_df['Method'] == 'CellTRIP'].sort_values('Importance', ascending=False)['Feature'].to_numpy()\n",
    "        filtered_df['_sort'] = filtered_df['Feature'].map(lambda f: np.argwhere(sorted_features == f))\n",
    "        filtered_df = filtered_df.sort_values('_sort')\n",
    "        filtered_df = filtered_df.loc[filtered_df['_sort'] < num_top]\n",
    "\n",
    "        # Generate plot\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 4), layout='constrained')\n",
    "        sns.barplot(\n",
    "            data=filtered_df,\n",
    "            x='Feature', y='Importance', hue='Method',\n",
    "            ax=ax)\n",
    "        \n",
    "        # Labels\n",
    "        ax.set_xticks(ax.get_xticks())\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=80, ha='center', va='baseline')\n",
    "        max_height = max([l.get_window_extent(renderer=ax.figure.canvas.get_renderer()).height for l in ax.get_xticklabels()])\n",
    "        fontsize = ax.get_xticklabels()[0].get_size()\n",
    "        pad = fontsize / 2 + max_height / 2\n",
    "        ax.tick_params(axis='x', pad=pad)\n",
    "\n",
    "        # Styling\n",
    "        ax.spines[['right', 'top', 'left']].set_visible(False)\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "        # Save plot\n",
    "        fname =                                     f'{args.run_id}'\n",
    "        if args.stage is not None: fname +=         f'_{args.stage:02}'\n",
    "        fname +=                                    f'_{config[\"data\"][\"dataset\"]}'\n",
    "        fname +=                                    f'_comparison_perturbation'\n",
    "        fname +=                                    f'_{modality+1}.pdf'\n",
    "        fig.savefig(os.path.join(PLOT_FOLDER, fname), transparent=True, dpi=300)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting dynamic visualizations\n"
     ]
    }
   ],
   "source": [
    "print('Plotting dynamic visualizations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T18:27:48.249939Z",
     "iopub.status.busy": "2025-01-15T18:27:48.249780Z",
     "iopub.status.idle": "2025-01-15T18:28:39.620056Z",
     "shell.execute_reply": "2025-01-15T18:28:39.619296Z"
    }
   },
   "outputs": [],
   "source": [
    "for ak in args.analysis_key:\n",
    "    if args.novid: break\n",
    "    print(f'\\tVideo {ak}')\n",
    "\n",
    "    # Prepare data\n",
    "    present = memories[ak]['present'].cpu()\n",
    "    states = memories[ak]['states'].cpu()\n",
    "    stages = memories[ak]['stages'].cpu()\n",
    "    rewards = memories[ak]['rewards'].cpu()\n",
    "    base_env = celltrip.environments.trajectory(*[torch.empty((0, 0)) for _ in range(len(modalities))], **config['env'])\n",
    "\n",
    "    # Testing for portions of large datasets\n",
    "    # sub_idx = np.random.choice(modalities[0].shape[0], 1_000, replace=False)\n",
    "    # modalities, labels, times = [m[sub_idx] for m in modalities], labels[sub_idx], times[sub_idx]\n",
    "    # present, states, rewards = present[:, sub_idx], states[:, sub_idx], rewards[:, sub_idx]\n",
    "\n",
    "    # Testing for larger dims\n",
    "    # states = torch.concatenate((states, states), dim=-1)\n",
    "    # base_env.dim *= 2\n",
    "\n",
    "    # Skip data\n",
    "    present, states, stages, rewards = present[::args.skip], states[::args.skip], stages[::args.skip], rewards[::args.skip]\n",
    "    # if states_3d is not None: states_3d = states_3d[::args.skip]\n",
    "\n",
    "    # Reduce dimensions\n",
    "    # TODO: Maybe add to transforms section?\n",
    "    if states.shape[-1] > 2*3 or args.force_reduction:\n",
    "        print('\\t\\tReducing state dimensionality')\n",
    "        # Get idx of last state in designated stage\n",
    "        stage_unique, stage_idx = np.unique(stages.numpy()[::-1], return_index=True)\n",
    "        stage_idx = stages.shape[0] - stage_idx - 1\n",
    "\n",
    "        # Choose reduction type\n",
    "        if args.reduction_type == 'umap':\n",
    "            import umap\n",
    "            fit_reducer = lambda data: umap.UMAP(n_components=3, n_jobs=1, random_state=notebook_seed).fit(data)\n",
    "            transform_reducer = lambda reducer, data: torch.Tensor(reducer.transform(data))\n",
    "        elif args.reduction_type == 'pca':\n",
    "            import sklearn.decomposition\n",
    "            fit_reducer = lambda data: sklearn.decomposition.PCA(n_components=3, random_state=notebook_seed).fit(data)\n",
    "            transform_reducer = lambda reducer, data: torch.Tensor(reducer.transform(data))\n",
    "        elif args.reduction_type is None or args.reduction_type == 'none':\n",
    "            initialize_reducer = lambda: None\n",
    "            transform_reducer = lambda reducer, data: data\n",
    "\n",
    "        # Get steady state\n",
    "        if ak in ('convergence', 'discovery', 'perturbation',):\n",
    "            reducer = fit_reducer(states[stage_idx[0]])\n",
    "            get_reducer = lambda stage: reducer\n",
    "        elif ak in ('temporal',):\n",
    "            # TODO: Maybe add handling for skipped stages?\n",
    "            get_reducer = lambda stage: fit_reducer(states[stage_idx[stage]])\n",
    "\n",
    "        # UMAP\n",
    "        get_desc = lambda stage: f'\\t\\t\\tProjecting ({stage}/{stage_unique.max()})'\n",
    "        states_3d = []; pbar = tqdm(total=states.shape[0]*states.shape[1], desc=get_desc(0), ascii=True, ncols=100)\n",
    "        for stage in stage_unique:\n",
    "            pbar.set_description(get_desc(stage))\n",
    "            stage_states = states[stages==stage].reshape((-1, states.shape[-1]))\n",
    "            for i in range(0, stage_states.shape[0], args.reduction_batch):\n",
    "                states_3d.append(transform_reducer(get_reducer(stage), stage_states[i:i+args.reduction_batch]))\n",
    "                pbar.update(stage_states[i:i+args.reduction_batch].shape[0])\n",
    "        pbar.close()\n",
    "        states_3d = torch.concatenate(states_3d, dim=0).reshape((*states.shape[:-1], 3))\n",
    "        states_3d = torch.concatenate((states_3d, torch.zeros_like(states_3d)), dim=-1)\n",
    "    else:\n",
    "        states_3d = None\n",
    "\n",
    "    # CLI\n",
    "    print('\\t\\tGenerating video')\n",
    "\n",
    "    # Parameters\n",
    "    interval = 1e3*env.delta/3  # Time between frames (3x speedup)\n",
    "    min_max_vel = 1e-2 if ak in ('convergence', 'discovery') else -1  # Stop at first frame all vels are below target. 0 for full play\n",
    "    frame_override = None  # Manually enter number of frames to draw\n",
    "    rotations_per_second = .1  # Camera azimuthal rotations per second\n",
    "    num_lines = 100\n",
    "    if ak == 'temporal': num_lines *= len(temporal['stages'])**2\n",
    "\n",
    "    # Create plot based on key\n",
    "    # NOTE: Standard 1-padding all around and between figures\n",
    "    # NOTE: Left, bottom, width, height\n",
    "    if ak in ('convergence', 'discovery'):\n",
    "        figsize = (15, 10)\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        axs = [\n",
    "            fig.add_axes([1 /figsize[0], 1 /figsize[1], 8 /figsize[0], 8 /figsize[1]], projection='3d'),\n",
    "            fig.add_axes([10 /figsize[0], 5.5 /figsize[1], 4 /figsize[0], 3.5 /figsize[1]]),\n",
    "            fig.add_axes([10 /figsize[0], 1 /figsize[1], 4 /figsize[0], 3.5 /figsize[1]]),\n",
    "        ]\n",
    "        views = [\n",
    "            celltrip.utilities.View3D,\n",
    "            celltrip.utilities.ViewTemporalScatter,\n",
    "            celltrip.utilities.ViewSilhouette,\n",
    "        ]\n",
    "\n",
    "    elif ak == 'temporal':\n",
    "        figsize = (15, 10)\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        axs = [\n",
    "            fig.add_axes([1 /figsize[0], 1 /figsize[1], 8 /figsize[0], 8 /figsize[1]], projection='3d'),\n",
    "            fig.add_axes([10 /figsize[0], 5.5 /figsize[1], 4 /figsize[0], 3.5 /figsize[1]]),\n",
    "            fig.add_axes([10 /figsize[0], 1 /figsize[1], 4 /figsize[0], 3.5 /figsize[1]]),\n",
    "        ]\n",
    "        views = [\n",
    "            celltrip.utilities.View3D,\n",
    "            celltrip.utilities.ViewTemporalScatter,\n",
    "            celltrip.utilities.ViewTemporalDiscrepancy,\n",
    "        ]\n",
    "\n",
    "    elif ak in ('perturbation',):\n",
    "        figsize = (20, 10)\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        axs = [\n",
    "            fig.add_axes([1 /figsize[0], 1 /figsize[1], 8 /figsize[0], 8 /figsize[1]], projection='3d'),\n",
    "            fig.add_axes([10 /figsize[0], 5.5 /figsize[1], 8 /figsize[0], 3.5 /figsize[1]]),\n",
    "            fig.add_axes([10 /figsize[0], 1 /figsize[1], 3.5 /figsize[0], 3.5 /figsize[1]]),\n",
    "            fig.add_axes([14.5 /figsize[0], 1 /figsize[1], 3.5 /figsize[0], 3.5 /figsize[1]]),\n",
    "        ]\n",
    "        views = [\n",
    "            celltrip.utilities.View3D,\n",
    "            celltrip.utilities.ViewPerturbationEffect,\n",
    "            celltrip.utilities.ViewTemporalScatter,\n",
    "            celltrip.utilities.ViewSilhouette,\n",
    "        ]\n",
    "\n",
    "    # Initialize views\n",
    "    arguments = {\n",
    "        # Data\n",
    "        'present': present,\n",
    "        'states': states,\n",
    "        'states_3d': states_3d,\n",
    "        'stages': stages,\n",
    "        'rewards': rewards,\n",
    "        'modalities': modalities,\n",
    "        'labels': labels,\n",
    "        # Data params\n",
    "        'dim': base_env.dim,\n",
    "        'modal_targets': base_env.reward_distance_target,\n",
    "        'temporal_stages': temporal['stages'],\n",
    "        'perturbation_features': perturbation_features,\n",
    "        'perturbation_feature_names': perturbation_feature_names,\n",
    "        'partitions': times if ak in ('temporal',) else None,\n",
    "        # Arguments\n",
    "        'interval': interval,\n",
    "        'skip': args.skip,\n",
    "        'seed': notebook_seed,\n",
    "        # Styling\n",
    "        'num_lines': num_lines,\n",
    "        'ms': 5,  # 3\n",
    "        'lw': 1,\n",
    "    }\n",
    "    views = [view(**arguments, ax=ax) for view, ax in zip(views, axs)]\n",
    "\n",
    "    # Compile animation\n",
    "    frames = states[..., env.dim:env.dim+3].square().sum(dim=-1).sqrt().max(dim=-1).values < min_max_vel\n",
    "    frames = np.array([(frames[i] or frames[i+1]) if i != len(frames)-1 else frames[i] for i in range(len(frames))])  # Disregard interrupted sections of low movement\n",
    "    frames = np.argwhere(frames)\n",
    "    frames = frames[0, 0].item()+1 if len(frames) > 0 else states.shape[0]\n",
    "    frames = frames if frame_override is None else frame_override\n",
    "\n",
    "    # Update function\n",
    "    pbar = tqdm(ascii=True, total=frames+1, desc='\\t\\t\\tRendering', ncols=100)  # CLI, runs frame 0 twice\n",
    "    def update(frame):\n",
    "        # Update views\n",
    "        for view in views:\n",
    "            view.update(frame)\n",
    "\n",
    "        # CLI\n",
    "        update_timestep = 1\n",
    "        if frame % update_timestep == 0:\n",
    "            pbar.update(update_timestep)\n",
    "\n",
    "    # Test individual frames\n",
    "    # for frame in range(frames):\n",
    "    #     update(frame)\n",
    "    #     # print()\n",
    "    #     # print('saving')\n",
    "    #     fig.savefig(os.path.join('temp/plots', f'frame_{frame}.png'), dpi=300)\n",
    "    #     plt.close(fig)\n",
    "    #     break\n",
    "\n",
    "    # Initialize animation\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig=fig,\n",
    "        func=update,\n",
    "        frames=frames,\n",
    "        interval=interval,\n",
    "    )\n",
    "\n",
    "    # Display animation as it renders\n",
    "    # plt.show()\n",
    "\n",
    "    # Display complete animation\n",
    "    # from IPython.display import HTML\n",
    "    # HTML(ani.to_jshtml())\n",
    "\n",
    "    # Save animation\n",
    "    # NOTE: Requires `sudo apt-get install ffmpeg`\n",
    "    file_type = 'mp4' if not args.gif else 'gif'\n",
    "    if file_type == 'mp4': writer = animation.FFMpegWriter(fps=int(1e3/interval), extra_args=['-vcodec', 'libx264'], bitrate=8e3)  # Faster\n",
    "    elif file_type == 'gif': writer = animation.FFMpegWriter(fps=int(1e3/interval))  # Slower\n",
    "    fname =                                     f'{args.run_id}'\n",
    "    if args.stage is not None: fname +=         f'_{args.stage:02}'\n",
    "    fname +=                                    f'_{config[\"data\"][\"dataset\"]}'\n",
    "    fname +=                                    f'_{ak}'\n",
    "    fname +=                                    f'.{file_type}'\n",
    "    ani.save(os.path.join(PLOT_FOLDER, fname), writer=writer, dpi=300)\n",
    "\n",
    "    # CLI\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Done\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
