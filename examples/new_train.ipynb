{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Set env vars\n",
    "os.environ['RAY_DEDUP_LOGS'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "# Enable text output in notebooks\n",
    "import tqdm.auto\n",
    "import tqdm.notebook\n",
    "tqdm.notebook.tqdm = tqdm.auto.tqdm\n",
    "\n",
    "import celltrip\n",
    "import data\n",
    "\n",
    "# Set params\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data/')\n",
    "MODEL_FOLDER = os.path.join(BASE_FOLDER, 'models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High priority\n",
    "  - Optimize cancels to only cancel non-running\n",
    "  - Implement stages\n",
    "  - Add partitioning\n",
    "  - Make ray init command for bash and add to README\n",
    "- Medium Priority\n",
    "  - Add seeding\n",
    "  - Add state manager to env and then parallelize in analysis, maybe make `analyze` function\n",
    "  - Decide on split_state reproducibility\n",
    "  - Add parallelism on max_batch and update. With update, encase whole epoch as ray function so splitting occurs within ray function, using ray.remote inline API to allow for non-ray usage. Then, adjustable policy weight sync (i.e. 1 epoch, 10 epochs)\n",
    "- Low Priority\n",
    "  - Allow memory to pre-process keys and persistent storage\n",
    "  - Add hook for wandb, ex.\n",
    "  - Move preprocessing to manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities, types, features = data.load_data('MMD-MA', DATA_FOLDER)\n",
    "ppc = celltrip.utilities.Preprocessing(pca_dim=128, device=DEVICE)\n",
    "processed_modalities, features = ppc.fit_transform(modalities, features)\n",
    "# modalities = ppc.cast(processed_modalities)\n",
    "modalities = [m.astype(np.float32) for m in processed_modalities]\n",
    "# modalities = [np.concatenate([m for _ in range(10000)], axis=0) for m in modalities]\n",
    "# modalities = [m[:100] for m in modalities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behavioral functions\n",
    "dim = 3\n",
    "policy_init = lambda modalities: celltrip.models.PPO(\n",
    "    positional_dim=2*dim,\n",
    "    modal_dims=[m.shape[1] for m in modalities],\n",
    "    output_dim=dim,\n",
    "    # BACKWARDS\n",
    "    # epochs=5,\n",
    "    # memory_prune=0,\n",
    "    update_load_level='batch',\n",
    "    update_cast_level='minibatch',\n",
    "    update_batch=1e4,\n",
    "    update_minibatch=3e3,\n",
    "    # SAMPLING\n",
    "    # max_batch=100,\n",
    "    max_nodes=100,\n",
    "    # DEVICE\n",
    "    device='cpu')\n",
    "# policy = policy_init(modalities)\n",
    "# policy_init = lambda _: policy\n",
    "env_init = lambda policy, modalities: celltrip.environments.EnvironmentBase(\n",
    "    *modalities,\n",
    "    dim=dim,\n",
    "    # max_timesteps=1e2,\n",
    "    penalty_bound=1,\n",
    "    device=policy.device)\n",
    "memory_init = lambda policy: celltrip.models.AdvancedMemoryBuffer(\n",
    "    sum(policy.modal_dims),\n",
    "    split_args=policy.split_args)\n",
    "\n",
    "# Initialize ray and distributed\n",
    "ray.shutdown()\n",
    "ray.init(\n",
    "    resources={'VRAM': torch.cuda.get_device_properties(0).total_memory},\n",
    "    dashboard_host='0.0.0.0')\n",
    "dm = celltrip.training.DistributedManager(\n",
    "    modalities,\n",
    "    policy_init=policy_init,\n",
    "    env_init=env_init,\n",
    "    memory_init=memory_init)\n",
    "\n",
    "# Train loop iter\n",
    "max_rollout_futures = 20\n",
    "num_updates = 0; calibrated = False\n",
    "while True:\n",
    "    # Retrieve active futures\n",
    "    futures = dm.get_futures()\n",
    "    num_futures = len(dm.get_all_futures())\n",
    "\n",
    "    # CLI\n",
    "    # print('; '.join([f'{k} ({len(v)})' for k, v in futures.items()]))\n",
    "    # print(ray.available_resources())\n",
    "\n",
    "    ## Check for futures to add\n",
    "    # Check memory and apply update if needed \n",
    "    if len(futures['update']) == 0 and dm.get_memory_len() >= int(1e6):\n",
    "        # assert False\n",
    "        dm.cancel()  # Cancel all non-running (TODO)\n",
    "        dm.update()\n",
    "\n",
    "    # Add rollouts if no update future and below max queued futures\n",
    "    elif len(futures['update']) == 0 and num_futures < max_rollout_futures:\n",
    "        dm.rollout( max_rollout_futures - num_futures )\n",
    "\n",
    "    ## Check for completed futures\n",
    "    # Completed rollouts\n",
    "    if len(ray.wait(futures['rollout'], timeout=0)[0]) > 0:\n",
    "        # Calibrate if needed\n",
    "        all_variants_run = True  # TODO: Set to true if all partitions have been run\n",
    "        if dm.resources['rollout']['core']['memory'] == 0 and all_variants_run:\n",
    "            dm.calibrate()\n",
    "            print(\n",
    "                f'Required rollout'\n",
    "                f' memory ({dm.resources[\"rollout\"][\"core\"][\"memory\"] / 2**30:.2f} GiB)'\n",
    "                f' and VRAM ({dm.resources[\"rollout\"][\"custom\"][\"VRAM\"] / 2**30:.2f} GiB)')\n",
    "            dm.cancel(); time.sleep(1)  # Cancel all non-running (TODO)\n",
    "            dm.policy_manager.release_locks.remote()\n",
    "        # Clean if calibrated\n",
    "        if dm.resources['rollout']['core']['memory'] != 0: dm.clean('rollout')\n",
    "\n",
    "    # Completed updates\n",
    "    if len(ray.wait(futures['update'], timeout=0)[0]) > 0:\n",
    "        num_updates += 1\n",
    "        # Calibrate if needed\n",
    "        if dm.resources['update']['core']['memory'] == 0:\n",
    "            dm.calibrate()\n",
    "            print(f'Required update'\n",
    "                f' memory ({dm.resources[\"update\"][\"core\"][\"memory\"] / 2**30:.3f} GiB)'\n",
    "                f' and VRAM ({dm.resources[\"update\"][\"custom\"][\"VRAM\"] / 2**30:.3f} GiB)')\n",
    "        dm.clean('update')\n",
    "\n",
    "    # Wait for a new completion\n",
    "    num_futures = len(dm.get_all_futures())\n",
    "    if num_futures > 0:\n",
    "        num_completed_futures = len(dm.wait(num_returns=num_futures, timeout=0))\n",
    "        if num_completed_futures != num_futures: dm.wait(num_returns=num_completed_futures+1)\n",
    "\n",
    "    # Escape\n",
    "    if num_updates >= 50: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cancel\n",
    "# dm.cancel()\n",
    "# dm.clean()\n",
    "\n",
    "# # Get policy\n",
    "# device = DEVICE\n",
    "# policy = policy_init(modalities).to(device)\n",
    "# celltrip.training.set_policy_state(policy, ray.get(dm.policy_manager.get_policy_state.remote()))\n",
    "\n",
    "# # Get memory\n",
    "# memory = memory_init(policy)\n",
    "# memory.append_memory(\n",
    "#     *ray.get(dm.policy_manager.get_memory_storage.remote()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get state of job from ObjectRef\n",
    "# import ray.util.state\n",
    "# object_id = dm.futures['simulation'][0].hex()\n",
    "# object_state = ray.util.state.get_objects(object_id)[0]\n",
    "# object_state.task_status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
