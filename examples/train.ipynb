{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T20:00:03.500503Z",
     "iopub.status.busy": "2024-05-29T20:00:03.500155Z",
     "iopub.status.idle": "2024-05-29T20:00:03.523964Z",
     "shell.execute_reply": "2024-05-29T20:00:03.523523Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checks\n",
    "# Check that rewards are normalized after (?) advantage\n",
    "\n",
    "## Improvements\n",
    "# Fix off-center positioning in large environments\n",
    "# Revise distance reward - Maybe add cell attraction (all should be close to each other) and repulsion (repulsion based on distance in modality)\n",
    "# Revise velocity and action penalties to encourage early cell-type separation (i.e. sqrt of vec length or similar)\n",
    "# Try using running average early stopping\n",
    "# Add parallel envs of different sizes, with different data to help generality\n",
    "\n",
    "## QOL\n",
    "# Save every time early stopping occurs\n",
    "\n",
    "## Runs\n",
    "# Try full real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T20:00:03.526092Z",
     "iopub.status.busy": "2024-05-29T20:00:03.525951Z",
     "iopub.status.idle": "2024-05-29T20:00:03.539229Z",
     "shell.execute_reply": "2024-05-29T20:00:03.538751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=train.ipynb\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env WANDB_NOTEBOOK_NAME train.ipynb\n",
    "%env WANDB_SILENT true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T20:00:03.568665Z",
     "iopub.status.busy": "2024-05-29T20:00:03.568449Z",
     "iopub.status.idle": "2024-05-29T20:00:16.211832Z",
     "shell.execute_reply": "2024-05-29T20:00:16.211370Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import inept\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "# Set params\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DATA_FOLDER = os.path.join(os.path.abspath(''), '../data')\n",
    "MODEL_FOLDER = os.path.join(os.path.abspath(''), 'temp/trained_models')\n",
    "\n",
    "# Script arguments\n",
    "# import sys\n",
    "# arg1 = int(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T20:00:16.214001Z",
     "iopub.status.busy": "2024-05-29T20:00:16.213845Z",
     "iopub.status.idle": "2024-05-29T20:00:16.231822Z",
     "shell.execute_reply": "2024-05-29T20:00:16.231385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Original paper (pg 24)\n",
    "# https://arxiv.org/pdf/1909.07528.pdf\n",
    "\n",
    "# Original blog\n",
    "# https://openai.com/research/emergent-tool-use\n",
    "\n",
    "# Gym\n",
    "# https://gymnasium.farama.org/\n",
    "\n",
    "# Slides\n",
    "# https://glouppe.github.io/info8004-advanced-machine-learning/pdf/pleroy-hide-and-seek.pdf\n",
    "\n",
    "# PPO implementation\n",
    "# https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py#L38\n",
    "\n",
    "# Residual SA\n",
    "# https://github.com/openai/multi-agent-emergence-environments/blob/bafaf1e11e6398624116761f91ae7c93b136f395/ma_policy/layers.py#L89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T20:00:16.233732Z",
     "iopub.status.busy": "2024-05-29T20:00:16.233580Z",
     "iopub.status.idle": "2024-05-29T20:00:16.263446Z",
     "shell.execute_reply": "2024-05-29T20:00:16.263001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if DEVICE == 'cuda': torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "note_kwargs = {'seed': seed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T20:00:16.265548Z",
     "iopub.status.busy": "2024-05-29T20:00:16.265366Z",
     "iopub.status.idle": "2024-05-29T20:00:23.096783Z",
     "shell.execute_reply": "2024-05-29T20:00:23.096324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "dataset_name = 'BrainChromatin'\n",
    "if dataset_name == 'BrainChromatin':\n",
    "    M1 = pd.read_csv(os.path.join(DATA_FOLDER, 'brainchromatin/multiome_rna_counts.tsv'), delimiter='\\t', nrows=2_000).transpose()  # TODO: Raise number of features\n",
    "    M2 = pd.read_csv(os.path.join(DATA_FOLDER, 'brainchromatin/multiome_atac_gene_activities.tsv'), delimiter='\\t', nrows=2_000).transpose()  # TODO: Raise number of features\n",
    "    M2 = M2.transpose()[M1.index].transpose()\n",
    "    meta = pd.read_csv(os.path.join(DATA_FOLDER, 'brainchromatin/multiome_cell_metadata.txt'), delimiter='\\t')\n",
    "    meta_names = pd.read_csv(os.path.join(DATA_FOLDER, 'brainchromatin/multiome_cluster_names.txt'), delimiter='\\t')\n",
    "    meta_names = meta_names[meta_names['Assay'] == 'Multiome ATAC']\n",
    "    meta = pd.merge(meta, meta_names, left_on='ATAC_cluster', right_on='Cluster.ID', how='left')\n",
    "    meta.index = meta['Cell.ID']\n",
    "    T1 = T2 = np.array(meta.transpose()[M1.index].transpose()['Cluster.Name'])\n",
    "    F1, F2 = M1.columns, M2.columns\n",
    "    M1, M2 = M1.to_numpy(), M2.to_numpy()\n",
    "\n",
    "    del meta, meta_names\n",
    "\n",
    "elif dataset_name == 'scGEM':\n",
    "    M1 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/scGEM/GeneExpression.txt'), delimiter=' ', header=None).to_numpy()\n",
    "    M2 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/scGEM/DNAmethylation.txt'), delimiter=' ', header=None).to_numpy()\n",
    "    T1 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/scGEM/type1.txt'), delimiter=' ', header=None).to_numpy()\n",
    "    T2 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/scGEM/type2.txt'), delimiter=' ', header=None).to_numpy()\n",
    "    F1 = np.loadtxt(os.path.join(DATA_FOLDER, 'UnionCom/scGEM/gex_names.txt'), dtype='str')\n",
    "    F2 = np.loadtxt(os.path.join(DATA_FOLDER, 'UnionCom/scGEM/dm_names.txt'), dtype='str')\n",
    "\n",
    "# MMD-MA data\n",
    "elif dataset_name == 'MMD-MA':\n",
    "    M1 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/MMD/s1_mapped1.txt'), delimiter='\\t', header=None).to_numpy()\n",
    "    M2 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/MMD/s1_mapped2.txt'), delimiter='\\t', header=None).to_numpy()\n",
    "    T1 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/MMD/s1_type1.txt'), delimiter='\\t', header=None).to_numpy()\n",
    "    T2 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/MMD/s1_type2.txt'), delimiter='\\t', header=None).to_numpy()\n",
    "\n",
    "# Random data\n",
    "elif dataset_name == 'Random':\n",
    "    num_nodes = 100\n",
    "    M1 = torch.rand((num_nodes, 8), device=DEVICE)\n",
    "    M2 = torch.rand((num_nodes, 16), device=DEVICE)\n",
    "\n",
    "else: assert False, 'No matching dataset found.'\n",
    "\n",
    "# Parameters\n",
    "num_nodes = 100  # M1.shape[0]\n",
    "\n",
    "# Modify data\n",
    "M1, M2 = inept.utilities.normalize(M1, M2)  # Normalize\n",
    "# M1, M2 = inept.utilities.pca_features(M1, M2, num_features=(16, 16))  # PCA features\n",
    "M1, M2, T1, T2 = inept.utilities.subsample_nodes(M1, M2, T1, T2, num_nodes=num_nodes)  # Subsample nodes\n",
    "# M1, M2 = inept.utilities.subsample_features(M1, M2, num_features=(16, 16))  # Subsample features\n",
    "\n",
    "# Cast types\n",
    "M1 = torch.tensor(M1, dtype=torch.float32, device=DEVICE)\n",
    "M2 = torch.tensor(M2, dtype=torch.float32, device=DEVICE)\n",
    "modalities = (M1, M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T20:00:23.099036Z",
     "iopub.status.busy": "2024-05-29T20:00:23.098880Z",
     "iopub.status.idle": "2024-05-29T20:00:23.119483Z",
     "shell.execute_reply": "2024-05-29T20:00:23.119059Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "data_kwargs = {\n",
    "    'dataset': dataset_name,\n",
    "    'num_nodes': num_nodes,\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "env_kwargs = {\n",
    "    'dim': 2,  # x, y, vx, vy\n",
    "    'pos_bound': 5,\n",
    "    'pos_rand_bound': 1,\n",
    "    'vel_bound': 1,\n",
    "    'delta': .1,\n",
    "    # 'reward_distance': 0,\n",
    "    # 'reward_origin': 0,\n",
    "    # 'penalty_bound': 0,\n",
    "    # 'penalty_velocity': 0,\n",
    "    # 'penalty_action': 0,\n",
    "    'reward_distance_type': 'euclidean',\n",
    "}\n",
    "\n",
    "# Environment weight stages\n",
    "stages_kwargs = {\n",
    "    'env': (\n",
    "        # Stage 0\n",
    "        {'penalty_bound': 1},\n",
    "        # Stage 1\n",
    "        {'reward_origin': 1},\n",
    "        # Stage 2\n",
    "        {'penalty_velocity': 1, 'penalty_action': 1},\n",
    "        # Stage 3\n",
    "        {'reward_origin': 0, 'reward_distance': 1},\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "max_ep_timesteps = 1e3  # Normal: 2e2\n",
    "max_timesteps = 5e3 * max_ep_timesteps\n",
    "update_timesteps = 5 * max_ep_timesteps  # Normal: 4e3\n",
    "train_kwargs = {\n",
    "    'max_ep_timesteps': max_ep_timesteps,\n",
    "    'max_timesteps': max_timesteps,\n",
    "    'update_timesteps': update_timesteps,\n",
    "}\n",
    "\n",
    "# Policy parameters\n",
    "update_minibatch = int( 1e4 * (2000 / sum(M.shape[1] for M in modalities)) * (20 / data_kwargs[\"num_nodes\"]) )  # Optimized for 1080 Ti\n",
    "update_max_batch = update_minibatch  # int( 2e4 )\n",
    "policy_kwargs = {\n",
    "    # Main arguments\n",
    "    'num_features_per_node': 2*env_kwargs['dim'],\n",
    "    'modal_sizes': [M.shape[1] for M in modalities],\n",
    "    'output_dim': env_kwargs['dim'],\n",
    "    'action_std_init': .6,\n",
    "    'action_std_decay': .05,\n",
    "    'action_std_min': .1,\n",
    "    'epochs': 80,\n",
    "    'epsilon_clip': .2,\n",
    "    'memory_gamma': .99,\n",
    "    'actor_lr': 3e-4,\n",
    "    'critic_lr': 1e-3,\n",
    "    'lr_gamma': 1,\n",
    "    'update_minibatch': min( update_minibatch, update_max_batch ),  # If too high, the kernel will crash (can also often crash machine)\n",
    "    'update_max_batch': update_max_batch,  # All memories: int(train_kwargs['update_timesteps'] * data_kwargs[\"num_nodes\"])\n",
    "    'device': DEVICE,\n",
    "    # Layer arguments\n",
    "    'embed_dim': 64,\n",
    "    'feature_embed_dim': 32,\n",
    "}\n",
    "\n",
    "# Early stopping parameters\n",
    "es_kwargs = {\n",
    "    # Global parameters\n",
    "    'method': 'average',\n",
    "    'buffer': 6 * int(train_kwargs['update_timesteps'] / train_kwargs['max_ep_timesteps']),  # 6 training cycles\n",
    "    'delta': .01,\n",
    "    'decreasing': False,\n",
    "    # `average` method parameters\n",
    "    'window_size': 3 * int(train_kwargs['update_timesteps'] / train_kwargs['max_ep_timesteps']),  # 3 training cycles\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T20:00:23.121660Z",
     "iopub.status.busy": "2024-05-29T20:00:23.121395Z",
     "iopub.status.idle": "2024-05-30T03:22:32.140031Z",
     "shell.execute_reply": "2024-05-30T03:22:32.139215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training\n",
      "Subsampling 1000 states with minibatches of size 1000 from 500000 total.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.54066 on episode 5 and timestep 5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.427368 on episode 10 and timestep 10000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.366008 on episode 15 and timestep 15000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.228196 on episode 20 and timestep 20000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.376924 on episode 25 and timestep 25000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.175704 on episode 30 and timestep 30000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.007472 on episode 35 and timestep 35000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.737512 on episode 40 and timestep 40000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.032336 on episode 45 and timestep 45000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.693968 on episode 50 and timestep 50000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.901836 on episode 55 and timestep 55000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.713328 on episode 60 and timestep 60000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.732932 on episode 65 and timestep 65000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.565408 on episode 70 and timestep 70000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.818816 on episode 75 and timestep 75000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.36382 on episode 80 and timestep 80000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.243916 on episode 85 and timestep 85000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.0531 on episode 90 and timestep 90000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.034008 on episode 95 and timestep 95000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.023912 on episode 100 and timestep 100000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.01802 on episode 105 and timestep 105000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.014884 on episode 110 and timestep 110000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.013796 on episode 115 and timestep 115000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.014464 on episode 120 and timestep 120000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.007404 on episode 125 and timestep 125000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.004596 on episode 130 and timestep 130000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.003456 on episode 135 and timestep 135000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.002328 on episode 140 and timestep 140000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.001536 on episode 145 and timestep 145000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.000656 on episode 150 and timestep 150000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.000728 on episode 155 and timestep 155000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -8.8e-05 on episode 160 and timestep 160000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.00026 on episode 165 and timestep 165000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.11570096064680815 on episode 170 and timestep 170000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.10258315300875902 on episode 175 and timestep 175000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.08739825301133096 on episode 180 and timestep 180000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.05665520005464554 on episode 185 and timestep 185000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.03601942310194671 on episode 190 and timestep 190000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.023815207916654646 on episode 195 and timestep 195000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.003764595066934824 on episode 200 and timestep 200000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.008319394479781389 on episode 205 and timestep 205000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.013820699608340859 on episode 210 and timestep 210000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.01836525334794633 on episode 215 and timestep 215000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.0234605907426998 on episode 220 and timestep 220000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.03139412025847286 on episode 225 and timestep 225000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.03460393983583152 on episode 230 and timestep 230000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.03855696997383609 on episode 235 and timestep 235000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.04020594506212324 on episode 240 and timestep 240000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.043574800506174564 on episode 245 and timestep 245000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.04476795309132338 on episode 250 and timestep 250000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.04254899021672271 on episode 255 and timestep 255000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.053321021058563144 on episode 260 and timestep 260000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.05139799759559892 on episode 265 and timestep 265000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.053360716918645426 on episode 270 and timestep 270000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.05432648567392491 on episode 275 and timestep 275000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.05696174816912972 on episode 280 and timestep 280000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.05374877974059619 on episode 285 and timestep 285000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.05829406799527444 on episode 290 and timestep 290000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.05888016659829393 on episode 295 and timestep 295000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.059957179331095886 on episode 300 and timestep 300000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.057937664813505485 on episode 305 and timestep 305000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.05598663475413853 on episode 310 and timestep 310000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5148614919857871 on episode 315 and timestep 315000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5081089103398909 on episode 320 and timestep 320000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.501293186148361 on episode 325 and timestep 325000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5047824757470488 on episode 330 and timestep 330000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4985051459697989 on episode 335 and timestep 335000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4902715367288711 on episode 340 and timestep 340000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4854798003756078 on episode 345 and timestep 345000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4846533656806806 on episode 350 and timestep 350000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.48340760166663793 on episode 355 and timestep 355000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4819550950154181 on episode 360 and timestep 360000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4782591398533841 on episode 365 and timestep 365000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.47691344832403026 on episode 370 and timestep 370000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.47521700166680686 on episode 375 and timestep 375000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.47386891810715337 on episode 380 and timestep 380000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.47606758102232566 on episode 385 and timestep 385000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4761281356226748 on episode 390 and timestep 390000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4735426171301268 on episode 395 and timestep 395000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4732152965267971 on episode 400 and timestep 400000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.503548025891373 on episode 405 and timestep 405000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.6253156348157514 on episode 410 and timestep 410000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.6205414692423825 on episode 415 and timestep 415000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.6083087774239926 on episode 420 and timestep 420000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.6049807994570835 on episode 425 and timestep 425000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5933496502458169 on episode 430 and timestep 430000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.583857180426142 on episode 435 and timestep 435000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5785446841778348 on episode 440 and timestep 440000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.569338977948314 on episode 445 and timestep 445000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5654857559293265 on episode 450 and timestep 450000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5614679762772288 on episode 455 and timestep 455000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5538476393241525 on episode 460 and timestep 460000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5486622803654242 on episode 465 and timestep 465000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5419943129895325 on episode 470 and timestep 470000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5385906260346641 on episode 475 and timestep 475000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5296250962687983 on episode 480 and timestep 480000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5277920683446412 on episode 485 and timestep 485000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.529106513174777 on episode 490 and timestep 490000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5237140378542526 on episode 495 and timestep 495000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5200457376363153 on episode 500 and timestep 500000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5225533022466696 on episode 505 and timestep 505000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5217887052540626 on episode 510 and timestep 510000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5158428374735299 on episode 515 and timestep 515000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5160592146342259 on episode 520 and timestep 520000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5143750173293409 on episode 525 and timestep 525000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5126583480325435 on episode 530 and timestep 530000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5142203527926607 on episode 535 and timestep 535000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.5499999999999999 on episode 537 and timestep 537000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4632151129331398 on episode 540 and timestep 540000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4290758101294317 on episode 545 and timestep 545000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4293705092679708 on episode 550 and timestep 550000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.429771243119462 on episode 555 and timestep 555000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4285177794568837 on episode 560 and timestep 560000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4275622973761333 on episode 565 and timestep 565000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.43005398196770356 on episode 570 and timestep 570000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4273201193430388 on episode 575 and timestep 575000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4262102257307524 on episode 580 and timestep 580000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.49999999999999994 on episode 581 and timestep 581000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3645127527178382 on episode 585 and timestep 585000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.34981887868900435 on episode 590 and timestep 590000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3522195074372339 on episode 595 and timestep 595000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.35300941866503427 on episode 600 and timestep 600000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3477827548752659 on episode 605 and timestep 605000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3517837985286681 on episode 610 and timestep 610000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3509270669821132 on episode 615 and timestep 615000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3506481035254446 on episode 620 and timestep 620000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3513816625865364 on episode 625 and timestep 625000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 6\n",
      "Decaying std to 0.44999999999999996 on episode 625 and timestep 625000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2831705946614091 on episode 630 and timestep 630000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.28310268454522686 on episode 635 and timestep 635000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.28419906556366875 on episode 640 and timestep 640000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.28245321121667244 on episode 645 and timestep 645000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.28430728142733364 on episode 650 and timestep 650000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2828289310499787 on episode 655 and timestep 655000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2818364120498789 on episode 660 and timestep 660000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2835964505551198 on episode 665 and timestep 665000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 7\n",
      "Decaying std to 0.39999999999999997 on episode 669 and timestep 669000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.27133745546980426 on episode 670 and timestep 670000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.22375874592233524 on episode 675 and timestep 675000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.22257423246608019 on episode 680 and timestep 680000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.22204693396558084 on episode 685 and timestep 685000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.22166390583817186 on episode 690 and timestep 690000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.22257334948153473 on episode 695 and timestep 695000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.22311602327622318 on episode 700 and timestep 700000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.22317602742725187 on episode 705 and timestep 705000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.22382956658322412 on episode 710 and timestep 710000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.35 on episode 713 and timestep 713000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.20076919110929267 on episode 715 and timestep 715000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.1714653199682168 on episode 720 and timestep 720000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.17182464384631682 on episode 725 and timestep 725000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.16908968203825597 on episode 730 and timestep 730000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.16875157179720937 on episode 735 and timestep 735000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.16962338136892324 on episode 740 and timestep 740000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.16989796843554028 on episode 745 and timestep 745000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.17102557481500044 on episode 750 and timestep 750000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.17089648234004295 on episode 755 and timestep 755000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.3 on episode 757 and timestep 757000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.14261231198215926 on episode 760 and timestep 760000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.12479208914985174 on episode 765 and timestep 765000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.12471232143890823 on episode 770 and timestep 770000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.12355054188129595 on episode 775 and timestep 775000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.12524160984049326 on episode 780 and timestep 780000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.12299275131304103 on episode 785 and timestep 785000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.12407219673364236 on episode 790 and timestep 790000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.12449626236590879 on episode 795 and timestep 795000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.12573313721128648 on episode 800 and timestep 800000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.25 on episode 801 and timestep 801000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.09380606601747993 on episode 805 and timestep 805000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.08783048389222854 on episode 810 and timestep 810000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.08811354956748689 on episode 815 and timestep 815000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.08922434137103319 on episode 820 and timestep 820000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.0875540706172887 on episode 825 and timestep 825000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.08894683096827878 on episode 830 and timestep 830000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.08689919884208604 on episode 835 and timestep 835000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.08693671220168857 on episode 840 and timestep 840000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.08905558798947258 on episode 845 and timestep 845000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n",
      "Advancing training to stage 11\n",
      "Decaying std to 0.2 on episode 845 and timestep 845000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.05465780347325913 on episode 850 and timestep 850000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.05578872013186259 on episode 855 and timestep 855000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.05590849679404192 on episode 860 and timestep 860000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.054960764928805396 on episode 865 and timestep 865000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.05389241654601143 on episode 870 and timestep 870000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.05433718180977933 on episode 875 and timestep 875000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.0553662420144005 on episode 880 and timestep 880000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.05537830573898862 on episode 885 and timestep 885000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.15000000000000002 on episode 889 and timestep 889000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.05167800684563273 on episode 890 and timestep 890000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.028622414808721602 on episode 895 and timestep 895000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.03043119059550322 on episode 900 and timestep 900000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.030569271234324714 on episode 905 and timestep 905000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.031160431341165164 on episode 910 and timestep 910000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.03010945486133019 on episode 915 and timestep 915000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.03210623789521736 on episode 920 and timestep 920000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.03171856929222687 on episode 925 and timestep 925000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.030875544422589954 on episode 930 and timestep 930000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.10000000000000002 on episode 933 and timestep 933000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.02412624672257092 on episode 935 and timestep 935000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.011813218912250293 on episode 940 and timestep 940000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.015295426602479793 on episode 945 and timestep 945000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.013085396294426845 on episode 950 and timestep 950000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.014581249723119737 on episode 955 and timestep 955000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.01417408926799234 on episode 960 and timestep 960000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.013266835332582487 on episode 965 and timestep 965000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.014185800197246997 on episode 970 and timestep 970000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.014599908882804083 on episode 975 and timestep 975000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing training to stage 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.1 on episode 977 and timestep 977000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.013887968614676482 on episode 980 and timestep 980000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.014977739237843584 on episode 985 and timestep 985000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.014434134868327678 on episode 990 and timestep 990000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.013095085220360117 on episode 995 and timestep 995000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.015330101614758984 on episode 1000 and timestep 1000000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.013439631169379252 on episode 1005 and timestep 1005000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.015392415418416705 on episode 1010 and timestep 1010000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.013930599324360563 on episode 1015 and timestep 1015000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.014090815707665577 on episode 1020 and timestep 1020000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.01 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending early on episode 1021 and timestep 1021000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reset Environment: 3.6189038030119036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Setup: 34.459714092356705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate Actions: 10679.664114404415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Environment: 858.6425333003565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record Rewards: 228.97438001811037\n",
      "Record Stats: 1.9380471019876495\n",
      "Early Stopping: 254.60155569203897\n",
      "Update Policy: 14367.916906778002\n",
      "Total: 26429.81615519028\n"
     ]
    }
   ],
   "source": [
    "# Tracking parameters\n",
    "# Use `watch -d -n 0.5 nvidia-smi` to watch CUDA memory usage\n",
    "# Use `top` to watch system memory usage\n",
    "use_wandb = True\n",
    "\n",
    "# Initialize classes\n",
    "env = inept.environments.trajectory(*modalities, **env_kwargs, **stages_kwargs['env'][0], device=DEVICE)  # Set to first stage\n",
    "policy = inept.models.PPO(**policy_kwargs)\n",
    "early_stopping = inept.utilities.EarlyStopping(**es_kwargs)\n",
    "\n",
    "# Initialize wandb\n",
    "if use_wandb: wandb.init(\n",
    "    project='INEPT',\n",
    "    config={\n",
    "        **{'note/'+k:v for k, v in note_kwargs.items()},\n",
    "        **{'data/'+k:v for k, v in data_kwargs.items()},\n",
    "        **{'env/'+k:v for k, v in env_kwargs.items()},\n",
    "        **{'stages/'+k:v for k, v in stages_kwargs.items()},\n",
    "        **{'policy/'+k:v for k, v in policy_kwargs.items()},\n",
    "        **{'train/'+k:v for k, v in train_kwargs.items()},\n",
    "        **{'es/'+k:v for k, v in es_kwargs.items()},\n",
    "    },\n",
    ")\n",
    "\n",
    "# Initialize logging vars\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "timer = inept.utilities.time_logger(discard_first_sample=True)\n",
    "timestep = 0; episode = 1; stage = 0\n",
    "\n",
    "# CLI\n",
    "print('Beginning training')\n",
    "print(f'Subsampling {policy_kwargs[\"update_max_batch\"]} states with minibatches of size {policy_kwargs[\"update_minibatch\"]} from {int(train_kwargs[\"update_timesteps\"] * data_kwargs[\"num_nodes\"])} total.')\n",
    "\n",
    "# Simulation loop\n",
    "while timestep < train_kwargs['max_timesteps']:\n",
    "    # Reset environment\n",
    "    env.reset()\n",
    "    timer.log('Reset Environment')\n",
    "\n",
    "    # Start episode\n",
    "    ep_timestep = 0; ep_reward = 0; ep_itemized_reward = defaultdict(lambda: 0)\n",
    "    while ep_timestep < train_kwargs['max_ep_timesteps']:\n",
    "        with torch.no_grad():\n",
    "            # Get current state\n",
    "            state = env.get_state(include_modalities=True)\n",
    "            timer.log('Environment Setup')\n",
    "\n",
    "            # Get actions from policy\n",
    "            actions = policy.act_macro(state, keys=list(range(num_nodes))).detach()\n",
    "            timer.log('Calculate Actions')\n",
    "\n",
    "            # Step environment and get reward\n",
    "            rewards, finished, itemized_rewards = env.step(actions, return_rewards=True)\n",
    "            finished = finished or (ep_timestep == train_kwargs['max_ep_timesteps']-1)  # Maybe move logic inside env?\n",
    "            timer.log('Step Environment')\n",
    "\n",
    "            # Record rewards for policy\n",
    "            policy.memory.record(\n",
    "                rewards=rewards.cpu().tolist(),\n",
    "                is_terminals=finished,\n",
    "            )\n",
    "\n",
    "            # Record rewards for logging\n",
    "            ep_reward = ep_reward + rewards.cpu().mean()\n",
    "            for k, v in itemized_rewards.items():\n",
    "                ep_itemized_reward[k] += v.cpu().mean()\n",
    "            timer.log('Record Rewards')\n",
    "\n",
    "        # Iterate\n",
    "        timestep += 1\n",
    "        ep_timestep += 1\n",
    "\n",
    "        # Update model\n",
    "        if timestep % train_kwargs['update_timesteps'] == 0:\n",
    "            # assert False\n",
    "            print(f'Updating model with average reward {np.mean(policy.memory.storage[\"rewards\"])} on episode {episode} and timestep {timestep}', end='')\n",
    "            policy.update()\n",
    "            print(f' ({torch.cuda.max_memory_allocated() / 1024**3:.2f} GB CUDA)')\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            timer.log('Update Policy')\n",
    "\n",
    "        # Escape if finished\n",
    "        if finished: break\n",
    "\n",
    "    # Upload stats\n",
    "    ep_reward = (ep_reward / ep_timestep).item()\n",
    "    update = int(timestep / train_kwargs['update_timesteps'])\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            **{\n",
    "            # Measurements\n",
    "            'end_timestep': timestep,\n",
    "            'episode': episode,\n",
    "            'update': update,\n",
    "            'stage': stage,\n",
    "            # Parameters\n",
    "            'action_std': policy.action_std,\n",
    "            # Outputs\n",
    "            'average_reward': ep_reward,\n",
    "            },\n",
    "            **{'rewards/'+k: (v / ep_timestep).item() for k, v in ep_itemized_reward.items()},\n",
    "        })\n",
    "    timer.log('Record Stats')\n",
    "\n",
    "    # Decay model std\n",
    "    if early_stopping(ep_reward) or timestep >= train_kwargs['max_timesteps']:\n",
    "        # Save model\n",
    "        wgt_file = os.path.join(MODEL_FOLDER, f'policy_{stage:02}.wgt')\n",
    "        torch.save(policy.state_dict(), wgt_file)  # Save just weights\n",
    "        if use_wandb: wandb.save(wgt_file)\n",
    "        mdl_file = os.path.join(MODEL_FOLDER, f'policy_{stage:02}.mdl')\n",
    "        torch.save(policy, mdl_file)  # Save whole model\n",
    "        if use_wandb: wandb.save(mdl_file)\n",
    "\n",
    "        # End if maximum timesteps reached\n",
    "        if timestep >= train_kwargs['max_timesteps']:\n",
    "            print('Maximal timesteps reached')\n",
    "\n",
    "        # End if at minimum `action_std`\n",
    "        if policy.action_std <= policy.action_std_min:\n",
    "            print(f'Ending early on episode {episode} and timestep {timestep}')\n",
    "            break\n",
    "\n",
    "        # Activate next stage or decay\n",
    "        stage += 1\n",
    "        # CLI\n",
    "        print(f'Advancing training to stage {stage}')\n",
    "        if stage < len(stages_kwargs['env']):\n",
    "            # Activate next stage\n",
    "            env.set_rewards(stages_kwargs['env'][stage])\n",
    "        else:\n",
    "            # Decay policy randomness\n",
    "            policy.decay_action_std()\n",
    "            # CLI\n",
    "            print(f'Decaying std to {policy.action_std} on episode {episode} and timestep {timestep}')\n",
    "\n",
    "        # Reset early stopping\n",
    "        early_stopping.reset()\n",
    "    timer.log('Early Stopping')\n",
    "\n",
    "    # Iterate\n",
    "    episode += 1\n",
    "\n",
    "# CLI Timer\n",
    "print()\n",
    "timer.aggregate('sum')\n",
    "\n",
    "# Finish wandb\n",
    "if use_wandb: wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
