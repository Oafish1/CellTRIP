{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T08:02:08.653765Z",
     "iopub.status.busy": "2024-04-14T08:02:08.653537Z",
     "iopub.status.idle": "2024-04-14T08:02:08.657347Z",
     "shell.execute_reply": "2024-04-14T08:02:08.656785Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# More efficient state storage (store modalities only once, etc.)\n",
    "# Add FCL between raw features and the features appended to vector\n",
    "# Make distance reward env-size and dataset-agnostic (i.e. spawn nodes in range 0-1 (or at origin), normalize dist per dataset (maybe by average dist))\n",
    "# Try MMD-MA with all features\n",
    "\n",
    "# Fix off-center positioning in large environments (kinda solved with post-centering?)\n",
    "# Try using running average early stopping\n",
    "# Save checkpoint models\n",
    "# Try with 100s of nodes\n",
    "# Try transfer learning with stages like stay at origin -> regular dist -> etc.\n",
    "# Upload episode playback data to wandb\n",
    "# Add parallel envs of different sizes, with different data to help generality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T08:02:08.660109Z",
     "iopub.status.busy": "2024-04-14T08:02:08.659888Z",
     "iopub.status.idle": "2024-04-14T08:02:08.675253Z",
     "shell.execute_reply": "2024-04-14T08:02:08.674832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=train.ipynb\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env WANDB_NOTEBOOK_NAME train.ipynb\n",
    "%env WANDB_SILENT true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T08:02:08.701423Z",
     "iopub.status.busy": "2024-04-14T08:02:08.701216Z",
     "iopub.status.idle": "2024-04-14T08:02:10.054711Z",
     "shell.execute_reply": "2024-04-14T08:02:10.054168Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import inept\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "# Set params\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DATA_FOLDER = os.path.join(os.path.abspath(''), '../data')\n",
    "MODEL_FOLDER = os.path.join(os.path.abspath(''), 'temp/trained_models')\n",
    "\n",
    "# Script arguments\n",
    "# import sys\n",
    "# arg1 = int(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T08:02:10.057085Z",
     "iopub.status.busy": "2024-04-14T08:02:10.056775Z",
     "iopub.status.idle": "2024-04-14T08:02:10.071207Z",
     "shell.execute_reply": "2024-04-14T08:02:10.070818Z"
    }
   },
   "outputs": [],
   "source": [
    "# Original paper (pg 24)\n",
    "# https://arxiv.org/pdf/1909.07528.pdf\n",
    "\n",
    "# Original blog\n",
    "# https://openai.com/research/emergent-tool-use\n",
    "\n",
    "# Gym\n",
    "# https://gymnasium.farama.org/\n",
    "\n",
    "# Slides\n",
    "# https://glouppe.github.io/info8004-advanced-machine-learning/pdf/pleroy-hide-and-seek.pdf\n",
    "\n",
    "# PPO implementation\n",
    "# https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py#L38\n",
    "\n",
    "# Residual SA\n",
    "# https://github.com/openai/multi-agent-emergence-environments/blob/bafaf1e11e6398624116761f91ae7c93b136f395/ma_policy/layers.py#L89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T08:02:10.073026Z",
     "iopub.status.busy": "2024-04-14T08:02:10.072872Z",
     "iopub.status.idle": "2024-04-14T08:02:10.089424Z",
     "shell.execute_reply": "2024-04-14T08:02:10.089016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if DEVICE == 'cuda': torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "note_kwargs = {'seed': seed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T08:02:10.091441Z",
     "iopub.status.busy": "2024-04-14T08:02:10.091289Z",
     "iopub.status.idle": "2024-04-14T08:02:10.427549Z",
     "shell.execute_reply": "2024-04-14T08:02:10.427056Z"
    }
   },
   "outputs": [],
   "source": [
    "# MMD-MA data\n",
    "num_nodes = 50\n",
    "data_kwargs = {\n",
    "    'dataset': 'MMD-MA',\n",
    "    'num_nodes': num_nodes,\n",
    "}\n",
    "M1 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/MMD/s1_mapped1.txt'), delimiter='\\t', header=None).to_numpy()\n",
    "M2 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/MMD/s1_mapped2.txt'), delimiter='\\t', header=None).to_numpy()\n",
    "T1 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/MMD/s1_type1.txt'), delimiter='\\t', header=None).to_numpy()\n",
    "T2 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/MMD/s1_type2.txt'), delimiter='\\t', header=None).to_numpy()\n",
    "# Subsample\n",
    "idx = np.random.choice(M1.shape[0], num_nodes, replace=False)\n",
    "idx_1 = np.random.choice(M1.shape[1], 16, replace=False)\n",
    "idx_2 = np.random.choice(M2.shape[1], 16, replace=False)\n",
    "M1 = torch.tensor(M1[idx][:, idx_1], dtype=torch.float32, device=DEVICE)\n",
    "M2 = torch.tensor(M2[idx][:, idx_2], dtype=torch.float32, device=DEVICE)\n",
    "T1 = torch.tensor(T1[idx], dtype=torch.long, device=DEVICE)\n",
    "T2 = torch.tensor(T2[idx], dtype=torch.long, device=DEVICE)\n",
    "modalities = (M1, M2)\n",
    "\n",
    "# Random data\n",
    "# num_nodes = 20\n",
    "# data_kwargs = {\n",
    "#     'dataset': 'Random',\n",
    "#     'num_nodes': num_nodes,\n",
    "# }\n",
    "# M1 = torch.rand((num_nodes, 8), device=DEVICE)\n",
    "# M2 = torch.rand((num_nodes, 16), device=DEVICE)\n",
    "# modalities = (M1, M2)\n",
    "\n",
    "# Environment\n",
    "# x, y, vx, vy\n",
    "num_dims = 2\n",
    "env_kwargs = {\n",
    "    'dim': num_dims,\n",
    "    'pos_bound': 2,\n",
    "    'vel_bound': 1,\n",
    "    'delta': .1,\n",
    "    'reward_distance': 10,\n",
    "    # 'reward_origin': 1,\n",
    "    'penalty_bound': 1,\n",
    "    'penalty_velocity': 1,\n",
    "    'penalty_action': 1,\n",
    "    'reward_distance_type': 'euclidean',\n",
    "}\n",
    "env = inept.environments.trajectory(*modalities, **env_kwargs, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T08:02:10.429900Z",
     "iopub.status.busy": "2024-04-14T08:02:10.429743Z",
     "iopub.status.idle": "2024-04-14T10:02:13.715072Z",
     "shell.execute_reply": "2024-04-14T10:02:13.714603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training\n",
      "Subsampling 8000 states with minibatches of size 8000 from 200000 total.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.3917209325961886 on episode 20 and timestep 4000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.3151680687619234 on episode 40 and timestep 8000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.2382298703477916 on episode 60 and timestep 12000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.152881999494515 on episode 80 and timestep 16000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.0521632170345996 on episode 100 and timestep 20000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.9886831611302821 on episode 120 and timestep 24000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.9196926702908398 on episode 140 and timestep 28000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.8428780927985988 on episode 160 and timestep 32000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.7481078351171023 on episode 180 and timestep 36000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.6709775609658979 on episode 200 and timestep 40000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5877676838175167 on episode 220 and timestep 44000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5035580485002283 on episode 240 and timestep 48000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4576468310356146 on episode 260 and timestep 52000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4236286136259636 on episode 280 and timestep 56000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3939922519378917 on episode 300 and timestep 60000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3795537867309595 on episode 320 and timestep 64000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3626377503960341 on episode 340 and timestep 68000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3436570979552246 on episode 360 and timestep 72000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3415568515166969 on episode 380 and timestep 76000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.32636265260663744 on episode 400 and timestep 80000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3267465251421464 on episode 420 and timestep 84000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.31719898975027117 on episode 440 and timestep 88000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.5499999999999999 on episode 441 and timestep 88200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.25575230960073575 on episode 460 and timestep 92000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.24725818082189965 on episode 480 and timestep 96000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.24865575973629123 on episode 500 and timestep 100000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.49999999999999994 on episode 512 and timestep 102400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2250454308072952 on episode 520 and timestep 104000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.186052550865803 on episode 540 and timestep 108000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.18970456164538352 on episode 560 and timestep 112000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.19333805921990818 on episode 580 and timestep 116000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.44999999999999996 on episode 581 and timestep 116200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.13412317971113633 on episode 600 and timestep 120000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.13373263429741492 on episode 620 and timestep 124000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.13411498940484184 on episode 640 and timestep 128000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.39999999999999997 on episode 643 and timestep 128600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.09749904959264385 on episode 660 and timestep 132000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.08214718012809062 on episode 680 and timestep 136000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.07973560899763427 on episode 700 and timestep 140000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.08914491416471748 on episode 720 and timestep 144000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.08232390026858957 on episode 740 and timestep 148000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.08274717580577656 on episode 760 and timestep 152000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.35 on episode 771 and timestep 154200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.0667399663067986 on episode 780 and timestep 156000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.0406938525407284 on episode 800 and timestep 160000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.03858389412840908 on episode 820 and timestep 164000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.036193509088624884 on episode 840 and timestep 168000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.3 on episode 845 and timestep 169000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.013247834154431839 on episode 860 and timestep 172000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.0007127597328264528 on episode 880 and timestep 176000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.0040380415998672835 on episode 900 and timestep 180000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.25 on episode 914 and timestep 182800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.005193568981504359 on episode 920 and timestep 184000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.020094097247816245 on episode 940 and timestep 188000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.029007319567891827 on episode 960 and timestep 192000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.02079581021755872 on episode 980 and timestep 196000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.2 on episode 983 and timestep 196600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.04703338525587955 on episode 1000 and timestep 200000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.051210108835136416 on episode 1020 and timestep 204000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.051174396016751125 on episode 1040 and timestep 208000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.051247948595122925 on episode 1060 and timestep 212000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.15000000000000002 on episode 1067 and timestep 213400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.0584429944379257 on episode 1080 and timestep 216000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.0729569145752331 on episode 1100 and timestep 220000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.06415593391028912 on episode 1120 and timestep 224000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.07593103827403405 on episode 1140 and timestep 228000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.07698732997552582 on episode 1160 and timestep 232000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.06254915316787839 on episode 1180 and timestep 236000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.10000000000000002 on episode 1195 and timestep 239000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.07451648730699564 on episode 1200 and timestep 240000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.08041215548045433 on episode 1220 and timestep 244000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.08196720699148116 on episode 1240 and timestep 248000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.08163264575361687 on episode 1260 and timestep 252000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.08303656767645534 on episode 1280 and timestep 256000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.1 on episode 1286 and timestep 257200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.07996932511244811 on episode 1300 and timestep 260000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.08729848490672615 on episode 1320 and timestep 264000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward 0.09342098058841584 on episode 1340 and timestep 268000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (3.79 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending early on episode 1356 and timestep 271200\n",
      "\n",
      "Reset Environment: 0.16404553500251495\n",
      "Environment Setup: 2973.672982980039\n",
      "Calculate Actions: 1695.3234914082823\n",
      "Step Environment: 195.54342375727356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record Rewards: 186.03955030623183\n",
      "Record Stats: 0.44416088401703746\n",
      "Early Stopping: 0.01824922999367118\n",
      "Update Policy: 2087.196198100981\n",
      "Total: 7138.4021022018205\n"
     ]
    }
   ],
   "source": [
    "# Policy parameters\n",
    "input_dims = 2*num_dims+sum([m.shape[1] for m in modalities])\n",
    "update_minibatch = int( 4e4 * (10 / num_nodes) )\n",
    "update_max_batch = 1 * update_minibatch  # Only run one minibatch, empirically the benefit is minimal compared to time loss\n",
    "policy_kwargs = {\n",
    "    'num_features_per_node': input_dims,\n",
    "    'output_dim': num_dims,\n",
    "    'action_std_init': .6,\n",
    "    'action_std_decay': .05,\n",
    "    'action_std_min': .1,\n",
    "    'actor_lr': 3e-4,\n",
    "    'critic_lr': 1e-3,\n",
    "    'lr_gamma': 1,\n",
    "    'update_minibatch': update_minibatch,  # Based on no minibatches needed with 10 nodes at 4k update timesteps\n",
    "    'update_max_batch': update_max_batch,  # Try making larger, e.g. 20x minibatches\n",
    "    'device': DEVICE,\n",
    "}\n",
    "policy = inept.models.PPO(**policy_kwargs)\n",
    "\n",
    "# Training parameters\n",
    "max_ep_timesteps = 2e2  # 2e2\n",
    "max_timesteps = 1e6\n",
    "update_timesteps = 4e3  # 20 * max_ep_timesteps\n",
    "train_kwargs = {\n",
    "    'max_ep_timesteps': max_ep_timesteps,\n",
    "    'max_timesteps': max_timesteps,\n",
    "    'update_timesteps': update_timesteps,\n",
    "}\n",
    "\n",
    "# Early stopping parameters\n",
    "es_kwargs = {\n",
    "    'buffer': 3 * int(update_timesteps / max_ep_timesteps),\n",
    "    'delta': .01,\n",
    "}\n",
    "early_stopping = inept.utilities.EarlyStopping(**es_kwargs)\n",
    "\n",
    "# Initialize wandb\n",
    "use_wandb = True\n",
    "if use_wandb: wandb.init(\n",
    "    project='INEPT',\n",
    "    config={\n",
    "        **{'note/'+k:v for k, v in note_kwargs.items()},\n",
    "        **{'data/'+k:v for k, v in data_kwargs.items()},\n",
    "        **{'env/'+k:v for k, v in env_kwargs.items()},\n",
    "        **{'policy/'+k:v for k, v in policy_kwargs.items()},\n",
    "        **{'train/'+k:v for k, v in train_kwargs.items()},\n",
    "        **{'es/'+k:v for k, v in es_kwargs.items()},\n",
    "    },\n",
    ")\n",
    "\n",
    "# Initialize logging vars\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "timer = inept.utilities.time_logger(discard_first_sample=True)\n",
    "timestep = 0; episode = 1\n",
    "\n",
    "# CLI\n",
    "print('Beginning training')\n",
    "print(f'Subsampling {update_max_batch} states with minibatches of size {update_minibatch} from {int(update_timesteps * num_nodes)} total.')\n",
    "\n",
    "# Simulation loop\n",
    "while timestep < max_timesteps:\n",
    "    # Reset environment\n",
    "    env.reset()\n",
    "    # env.reward_scales['reward_origin'] = episode / 1e3\n",
    "    timer.log('Reset Environment')\n",
    "\n",
    "    # Start episode\n",
    "    ep_timestep = 0; ep_reward = 0; ep_itemized_reward = defaultdict(lambda: 0)\n",
    "    while ep_timestep < max_ep_timesteps:\n",
    "        with torch.no_grad():\n",
    "            # Get current state\n",
    "            state = env.get_state(include_modalities=True)\n",
    "\n",
    "            # Get self features for each node\n",
    "            self_entity = state\n",
    "\n",
    "            # Get node features for each state\n",
    "            idx = torch.zeros((num_nodes, num_nodes), dtype=torch.bool)\n",
    "            for i, j in itertools.product(*[range(x) for x in idx.shape]):\n",
    "                idx[i, j] = i!=j\n",
    "            node_entities = state.unsqueeze(0).expand(num_nodes, *state.shape)\n",
    "            node_entities = node_entities[idx].reshape(num_nodes, num_nodes-1, input_dims)\n",
    "            timer.log('Environment Setup')\n",
    "\n",
    "            # Get actions from policy\n",
    "            actions = policy.act(self_entity, node_entities).detach()\n",
    "            timer.log('Calculate Actions')\n",
    "\n",
    "            # Step environment and get reward\n",
    "            rewards, finished, itemized_rewards = env.step(actions, return_rewards=True)\n",
    "            timer.log('Step Environment')\n",
    "\n",
    "            # Record rewards\n",
    "            for key in range(num_nodes):\n",
    "                policy.memory.rewards.append(rewards[key].item())  # Could just add lists\n",
    "                policy.memory.is_terminals.append(finished)\n",
    "            ep_reward = ep_reward + rewards.cpu().mean()\n",
    "            for k, v in itemized_rewards.items():\n",
    "                ep_itemized_reward[k] += v.cpu().mean()\n",
    "            timer.log('Record Rewards')\n",
    "\n",
    "        # Iterate\n",
    "        timestep += 1\n",
    "        ep_timestep += 1\n",
    "\n",
    "        # Update model\n",
    "        if timestep % update_timesteps == 0:\n",
    "            print(f'Updating model with average reward {np.mean(policy.memory.rewards)} on episode {episode} and timestep {timestep}', end='')\n",
    "            policy.update()\n",
    "            print(f' ({torch.cuda.max_memory_allocated() / 1024**3:.2f} GB CUDA)')\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            timer.log('Update Policy')\n",
    "\n",
    "        # Escape if finished\n",
    "        if finished: break\n",
    "\n",
    "    # Upload stats\n",
    "    ep_reward = (ep_reward / ep_timestep).item()\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            **{\n",
    "            'episode': episode,\n",
    "            'update': int(timestep / update_timesteps),\n",
    "            'end_timestep': timestep,\n",
    "            'average_reward': ep_reward,\n",
    "            'action_std': policy.action_std,\n",
    "            },\n",
    "            **{'rewards/'+k: (v / ep_timestep).item() for k, v in ep_itemized_reward.items()},\n",
    "        })\n",
    "    timer.log('Record Stats')\n",
    "\n",
    "    # Decay model std\n",
    "    if early_stopping(ep_reward):\n",
    "        # End if already at minimum\n",
    "        if policy.action_std <= policy.action_std_min:\n",
    "            print(f'Ending early on episode {episode} and timestep {timestep}')\n",
    "            break\n",
    "\n",
    "        # Decay and reset early stop\n",
    "        policy.decay_action_std()\n",
    "        early_stopping.reset()\n",
    "\n",
    "        # CLI\n",
    "        print(f'Decaying std to {policy.action_std} on episode {episode} and timestep {timestep}')\n",
    "    timer.log('Early Stopping')\n",
    "\n",
    "    # Iterate\n",
    "    episode += 1\n",
    "\n",
    "# CLI Timer\n",
    "print()\n",
    "timer.aggregate('sum')\n",
    "\n",
    "# Save model\n",
    "wgt_file = os.path.join(MODEL_FOLDER, 'policy.wgt')\n",
    "torch.save(policy.state_dict(), wgt_file)  # Save just weights\n",
    "if use_wandb: wandb.save(wgt_file)\n",
    "mdl_file = os.path.join(MODEL_FOLDER, 'policy.mdl')\n",
    "torch.save(policy, mdl_file)  # Save whole model\n",
    "if use_wandb: wandb.save(mdl_file)\n",
    "\n",
    "# Finish wandb\n",
    "if use_wandb: wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
