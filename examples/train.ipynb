{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:01:34.769509Z",
     "iopub.status.busy": "2024-05-23T06:01:34.769322Z",
     "iopub.status.idle": "2024-05-23T06:01:34.772446Z",
     "shell.execute_reply": "2024-05-23T06:01:34.771968Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checks\n",
    "# Check that rewards are normalized after (?) advantage\n",
    "\n",
    "## Improvements\n",
    "# Fix off-center positioning in large environments\n",
    "# Revise distance reward\n",
    "# Try using running average early stopping\n",
    "# Add parallel envs of different sizes, with different data to help generality\n",
    "\n",
    "## QOL\n",
    "# Save checkpoint models\n",
    "\n",
    "## Runs\n",
    "# Try full real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:01:34.774610Z",
     "iopub.status.busy": "2024-05-23T06:01:34.774420Z",
     "iopub.status.idle": "2024-05-23T06:01:34.789218Z",
     "shell.execute_reply": "2024-05-23T06:01:34.788792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=train.ipynb\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env WANDB_NOTEBOOK_NAME train.ipynb\n",
    "%env WANDB_SILENT true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:01:34.817508Z",
     "iopub.status.busy": "2024-05-23T06:01:34.817109Z",
     "iopub.status.idle": "2024-05-23T06:01:36.585936Z",
     "shell.execute_reply": "2024-05-23T06:01:36.585476Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import inept\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "# Set params\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DATA_FOLDER = os.path.join(os.path.abspath(''), '../data')\n",
    "MODEL_FOLDER = os.path.join(os.path.abspath(''), 'temp/trained_models')\n",
    "\n",
    "# Script arguments\n",
    "# import sys\n",
    "# arg1 = int(sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:01:36.588154Z",
     "iopub.status.busy": "2024-05-23T06:01:36.587984Z",
     "iopub.status.idle": "2024-05-23T06:01:36.605942Z",
     "shell.execute_reply": "2024-05-23T06:01:36.605530Z"
    }
   },
   "outputs": [],
   "source": [
    "# Original paper (pg 24)\n",
    "# https://arxiv.org/pdf/1909.07528.pdf\n",
    "\n",
    "# Original blog\n",
    "# https://openai.com/research/emergent-tool-use\n",
    "\n",
    "# Gym\n",
    "# https://gymnasium.farama.org/\n",
    "\n",
    "# Slides\n",
    "# https://glouppe.github.io/info8004-advanced-machine-learning/pdf/pleroy-hide-and-seek.pdf\n",
    "\n",
    "# PPO implementation\n",
    "# https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py#L38\n",
    "\n",
    "# Residual SA\n",
    "# https://github.com/openai/multi-agent-emergence-environments/blob/bafaf1e11e6398624116761f91ae7c93b136f395/ma_policy/layers.py#L89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:01:36.607866Z",
     "iopub.status.busy": "2024-05-23T06:01:36.607701Z",
     "iopub.status.idle": "2024-05-23T06:01:36.625943Z",
     "shell.execute_reply": "2024-05-23T06:01:36.625534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if DEVICE == 'cuda': torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "note_kwargs = {'seed': seed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:01:36.628068Z",
     "iopub.status.busy": "2024-05-23T06:01:36.627897Z",
     "iopub.status.idle": "2024-05-23T06:01:36.953043Z",
     "shell.execute_reply": "2024-05-23T06:01:36.952406Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "dataset_name = 'MMD-MA'\n",
    "if dataset_name == 'BrainChromatin':\n",
    "    M1 = pd.read_csv(os.path.join(DATA_FOLDER, 'brainchromatin/multiome_rna_counts.tsv'), delimiter='\\t', nrows=1_000).transpose()  # TODO: Raise number of features\n",
    "    M2 = pd.read_csv(os.path.join(DATA_FOLDER, 'brainchromatin/multiome_atac_gene_activities.tsv'), delimiter='\\t', nrows=1_000).transpose()  # TODO: Raise number of features\n",
    "    M2 = M2.transpose()[M1.index].transpose()\n",
    "    meta = pd.read_csv(os.path.join(DATA_FOLDER, 'brainchromatin/multiome_cell_metadata.txt'), delimiter='\\t')\n",
    "    meta_names = pd.read_csv(os.path.join(DATA_FOLDER, 'brainchromatin/multiome_cluster_names.txt'), delimiter='\\t')\n",
    "    meta_names = meta_names[meta_names['Assay'] == 'Multiome ATAC']\n",
    "    meta = pd.merge(meta, meta_names, left_on='ATAC_cluster', right_on='Cluster.ID', how='left')\n",
    "    meta.index = meta['Cell.ID']\n",
    "    T1 = T2 = np.array(meta.transpose()[M1.index].transpose()['Cluster.Name'])\n",
    "    F1, F2 = M1.columns, M2.columns\n",
    "    M1, M2 = M1.to_numpy(), M2.to_numpy()\n",
    "\n",
    "    del meta, meta_names\n",
    "\n",
    "elif dataset_name == 'scGEM':\n",
    "    M1 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/scGEM/GeneExpression.txt'), delimiter=' ', header=None).to_numpy()\n",
    "    M2 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/scGEM/DNAmethylation.txt'), delimiter=' ', header=None).to_numpy()\n",
    "    T1 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/scGEM/type1.txt'), delimiter=' ', header=None).to_numpy()\n",
    "    T2 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/scGEM/type2.txt'), delimiter=' ', header=None).to_numpy()\n",
    "    F1 = np.loadtxt(os.path.join(DATA_FOLDER, 'UnionCom/scGEM/gex_names.txt'), dtype='str')\n",
    "    F2 = np.loadtxt(os.path.join(DATA_FOLDER, 'UnionCom/scGEM/dm_names.txt'), dtype='str')\n",
    "\n",
    "# MMD-MA data\n",
    "elif dataset_name == 'MMD-MA':\n",
    "    M1 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/MMD/s1_mapped1.txt'), delimiter='\\t', header=None).to_numpy()\n",
    "    M2 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/MMD/s1_mapped2.txt'), delimiter='\\t', header=None).to_numpy()\n",
    "    T1 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/MMD/s1_type1.txt'), delimiter='\\t', header=None).to_numpy()\n",
    "    T2 = pd.read_csv(os.path.join(DATA_FOLDER, 'UnionCom/MMD/s1_type2.txt'), delimiter='\\t', header=None).to_numpy()\n",
    "\n",
    "# Random data\n",
    "elif dataset_name == 'Random':\n",
    "    num_nodes = 100\n",
    "    M1 = torch.rand((num_nodes, 8), device=DEVICE)\n",
    "    M2 = torch.rand((num_nodes, 16), device=DEVICE)\n",
    "\n",
    "else: assert False, 'No matching dataset found.'\n",
    "\n",
    "# Parameters\n",
    "num_nodes = 50  # M1.shape[0]\n",
    "\n",
    "# Modify data\n",
    "M1, M2 = inept.utilities.normalize(M1, M2)  # Normalize\n",
    "# M1, M2 = inept.utilities.pca_features(M1, M2, num_features=(16, 16))  # PCA features\n",
    "M1, M2, T1, T2 = inept.utilities.subsample_nodes(M1, M2, T1, T2, num_nodes=num_nodes)  # Subsample nodes\n",
    "# M1, M2 = inept.utilities.subsample_features(M1, M2, num_features=(16, 16))  # Subsample features\n",
    "\n",
    "# Cast types\n",
    "M1 = torch.tensor(M1, dtype=torch.float32, device=DEVICE)\n",
    "M2 = torch.tensor(M2, dtype=torch.float32, device=DEVICE)\n",
    "modalities = (M1, M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:01:36.955521Z",
     "iopub.status.busy": "2024-05-23T06:01:36.955346Z",
     "iopub.status.idle": "2024-05-23T06:01:36.974919Z",
     "shell.execute_reply": "2024-05-23T06:01:36.974484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "data_kwargs = {\n",
    "    'dataset': dataset_name,\n",
    "    'num_nodes': num_nodes,\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "env_kwargs = {\n",
    "    'dim': 2,  # x, y, vx, vy\n",
    "    'pos_bound': 5,\n",
    "    'pos_rand_bound': 1,\n",
    "    'vel_bound': 1,\n",
    "    'delta': .1,\n",
    "    'reward_distance': 10,\n",
    "    'penalty_bound': 1,\n",
    "    'penalty_velocity': 1,\n",
    "    'penalty_action': 1,\n",
    "    'reward_distance_type': 'euclidean',\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_kwargs = {\n",
    "    'max_ep_timesteps': 3e2,\n",
    "    'max_timesteps': 1e6,\n",
    "    'update_timesteps': 4e3,\n",
    "}\n",
    "\n",
    "# Policy parameters\n",
    "policy_kwargs = {\n",
    "    'modal_sizes': [M.shape[1] for M in modalities],\n",
    "    'num_features_per_node': 2*env_kwargs['dim'],\n",
    "    'output_dim': env_kwargs['dim'],\n",
    "    'action_std_init': .6,\n",
    "    'action_std_decay': .05,\n",
    "    'action_std_min': .1,\n",
    "    'actor_lr': 3e-4,\n",
    "    'critic_lr': 1e-3,\n",
    "    'lr_gamma': 1,\n",
    "    'update_minibatch': int(2e3),  # If too high, the kernel will crash (can also often crash machine)\n",
    "    'update_max_batch': int(2e3),  # int(train_kwargs['update_timesteps'] * data_kwargs[\"num_nodes\"])\n",
    "    'device': DEVICE,\n",
    "}\n",
    "\n",
    "# Early stopping parameters\n",
    "es_kwargs = {\n",
    "    'buffer': 3 * int(train_kwargs['update_timesteps'] / train_kwargs['max_ep_timesteps']),  # 3 training cycles\n",
    "    'delta': .01,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T06:01:36.977230Z",
     "iopub.status.busy": "2024-05-23T06:01:36.976920Z",
     "iopub.status.idle": "2024-05-23T07:42:25.029982Z",
     "shell.execute_reply": "2024-05-23T07:42:25.029529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training\n",
      "Subsampling 2000 states with minibatches of size 2000 from 200000 total.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.4661225230419554 on episode 14 and timestep 4000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.3995731101571152 on episode 27 and timestep 8000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.2955645820164354 on episode 40 and timestep 12000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.2820697113142476 on episode 54 and timestep 16000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.0854975688864725 on episode 67 and timestep 20000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -1.0145903051113552 on episode 80 and timestep 24000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.9177235155997833 on episode 94 and timestep 28000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.8252967280298774 on episode 107 and timestep 32000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.791715454409087 on episode 120 and timestep 36000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.7602193795348701 on episode 134 and timestep 40000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.6834847722081832 on episode 147 and timestep 44000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.6926262863448326 on episode 160 and timestep 48000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.6404163023043669 on episode 174 and timestep 52000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5653562939555896 on episode 187 and timestep 56000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5359236595778424 on episode 200 and timestep 60000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5369159328065534 on episode 214 and timestep 64000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5724982478319796 on episode 227 and timestep 68000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5509149498046501 on episode 240 and timestep 72000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5558103483584488 on episode 254 and timestep 76000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5107438504061871 on episode 267 and timestep 80000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.511919118231957 on episode 280 and timestep 84000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4970103059374634 on episode 294 and timestep 88000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.5350929928694816 on episode 307 and timestep 92000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.5499999999999999 on episode 307 and timestep 92100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.431552339922658 on episode 320 and timestep 96000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4902271403066124 on episode 334 and timestep 100000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4076772393411209 on episode 347 and timestep 104000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.49999999999999994 on episode 354 and timestep 106200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.4656626635821513 on episode 360 and timestep 108000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.37703744952603274 on episode 374 and timestep 112000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3326513629827125 on episode 387 and timestep 116000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.37954372694434596 on episode 400 and timestep 120000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.39124295167613743 on episode 414 and timestep 124000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.44999999999999996 on episode 414 and timestep 124200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3414929917163284 on episode 427 and timestep 128000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3765534237758373 on episode 440 and timestep 132000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3864445135001905 on episode 454 and timestep 136000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.31745408327577723 on episode 467 and timestep 140000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3317890581210388 on episode 480 and timestep 144000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3513525380703807 on episode 494 and timestep 148000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.388704078069745 on episode 507 and timestep 152000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.39999999999999997 on episode 512 and timestep 153600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.31196305839742533 on episode 520 and timestep 156000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.31237375038002735 on episode 534 and timestep 160000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.31439124231106397 on episode 547 and timestep 164000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3010550847993055 on episode 560 and timestep 168000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2662360331223643 on episode 574 and timestep 172000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.24666351719386803 on episode 587 and timestep 176000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.30342723789240555 on episode 600 and timestep 180000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.35 on episode 603 and timestep 180900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2599547951165179 on episode 614 and timestep 184000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.19686930809329925 on episode 627 and timestep 188000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2118393329700944 on episode 640 and timestep 192000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.21424126919050177 on episode 654 and timestep 196000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.3 on episode 660 and timestep 198000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.22496214543220994 on episode 667 and timestep 200000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.18087189533821277 on episode 680 and timestep 204000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.22921007111213293 on episode 694 and timestep 208000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.25 on episode 706 and timestep 211800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.21652804490802344 on episode 707 and timestep 212000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.1970403344487145 on episode 720 and timestep 216000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.19608376258822682 on episode 734 and timestep 220000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.18420502094891752 on episode 747 and timestep 224000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.2 on episode 757 and timestep 227100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.17961907109428604 on episode 760 and timestep 228000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.1795995095731273 on episode 774 and timestep 232000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.16030648075955833 on episode 787 and timestep 236000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.14936960001314037 on episode 800 and timestep 240000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.15000000000000002 on episode 801 and timestep 240300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3005952910213775 on episode 814 and timestep 244000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2486820237475554 on episode 827 and timestep 248000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.23956325555971242 on episode 840 and timestep 252000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.24292002940733248 on episode 854 and timestep 256000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2133300249191623 on episode 867 and timestep 260000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.14686671350040562 on episode 880 and timestep 264000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2089101224562223 on episode 894 and timestep 268000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.17981014749438837 on episode 907 and timestep 272000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.10000000000000002 on episode 909 and timestep 272700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.23195439373547722 on episode 920 and timestep 276000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2572739524953401 on episode 934 and timestep 280000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.2441360156008651 on episode 947 and timestep 284000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decaying std to 0.1 on episode 954 and timestep 286200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.229629640105336 on episode 960 and timestep 288000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3415543417282912 on episode 974 and timestep 292000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model with average reward -0.3029267623892048 on episode 987 and timestep 296000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (2.26 GB CUDA)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending early on episode 994 and timestep 298200\n",
      "\n",
      "Reset Environment: 0.13452185901837765\n",
      "Environment Setup: 9.941536761042698\n",
      "Calculate Actions: 871.5679537820158\n",
      "Step Environment: 237.8564275458748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record Rewards: 57.445545384943216\n",
      "Record Stats: 0.4735558680047234\n",
      "Early Stopping: 0.12236400199390118\n",
      "Update Policy: 4842.703110512002\n",
      "Total: 6020.245015714896\n"
     ]
    }
   ],
   "source": [
    "# Tracking parameters\n",
    "# Use `watch -d -n 0.5 nvidia-smi` to watch CUDA memory usage\n",
    "# Use `top` to watch system memory usage\n",
    "use_wandb = True\n",
    "\n",
    "# Initialize classes\n",
    "env = inept.environments.trajectory(*modalities, **env_kwargs, device=DEVICE)\n",
    "policy = inept.models.PPO(**policy_kwargs)\n",
    "early_stopping = inept.utilities.EarlyStopping(**es_kwargs)\n",
    "\n",
    "# Initialize wandb\n",
    "if use_wandb: wandb.init(\n",
    "    project='INEPT',\n",
    "    config={\n",
    "        **{'note/'+k:v for k, v in note_kwargs.items()},\n",
    "        **{'data/'+k:v for k, v in data_kwargs.items()},\n",
    "        **{'env/'+k:v for k, v in env_kwargs.items()},\n",
    "        **{'policy/'+k:v for k, v in policy_kwargs.items()},\n",
    "        **{'train/'+k:v for k, v in train_kwargs.items()},\n",
    "        **{'es/'+k:v for k, v in es_kwargs.items()},\n",
    "    },\n",
    ")\n",
    "\n",
    "# Initialize logging vars\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "timer = inept.utilities.time_logger(discard_first_sample=True)\n",
    "timestep = 0; episode = 1\n",
    "\n",
    "# CLI\n",
    "print('Beginning training')\n",
    "print(f'Subsampling {policy_kwargs[\"update_max_batch\"]} states with minibatches of size {policy_kwargs[\"update_minibatch\"]} from {int(train_kwargs[\"update_timesteps\"] * data_kwargs[\"num_nodes\"])} total.')\n",
    "\n",
    "# Simulation loop\n",
    "while timestep < train_kwargs['max_timesteps']:\n",
    "    # Reset environment\n",
    "    env.reset()\n",
    "    # env.reward_scales['reward_origin'] = episode / 1e3\n",
    "    timer.log('Reset Environment')\n",
    "\n",
    "    # Start episode\n",
    "    ep_timestep = 0; ep_reward = 0; ep_itemized_reward = defaultdict(lambda: 0)\n",
    "    while ep_timestep < train_kwargs['max_ep_timesteps']:\n",
    "        with torch.no_grad():\n",
    "            # Get current state\n",
    "            state = env.get_state(include_modalities=True)\n",
    "            timer.log('Environment Setup')\n",
    "\n",
    "            # Get actions from policy\n",
    "            actions = policy.act_macro(state, keys=list(range(num_nodes))).detach()\n",
    "            timer.log('Calculate Actions')\n",
    "\n",
    "            # Step environment and get reward\n",
    "            rewards, finished, itemized_rewards = env.step(actions, return_rewards=True)\n",
    "            timer.log('Step Environment')\n",
    "\n",
    "            # Record rewards for policy\n",
    "            policy.memory.record(\n",
    "                rewards=rewards.cpu().tolist(),\n",
    "                is_terminals=finished,\n",
    "            )\n",
    "\n",
    "            # Record rewards for logging\n",
    "            ep_reward = ep_reward + rewards.cpu().mean()\n",
    "            for k, v in itemized_rewards.items():\n",
    "                ep_itemized_reward[k] += v.cpu().mean()\n",
    "            timer.log('Record Rewards')\n",
    "\n",
    "        # Iterate\n",
    "        timestep += 1\n",
    "        ep_timestep += 1\n",
    "\n",
    "        # Update model\n",
    "        if timestep % train_kwargs['update_timesteps'] == 0:\n",
    "            print(f'Updating model with average reward {np.mean(policy.memory.storage[\"rewards\"])} on episode {episode} and timestep {timestep}', end='')\n",
    "            policy.update()\n",
    "            print(f' ({torch.cuda.max_memory_allocated() / 1024**3:.2f} GB CUDA)')\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            timer.log('Update Policy')\n",
    "\n",
    "        # Escape if finished\n",
    "        if finished: break\n",
    "\n",
    "    # Upload stats\n",
    "    ep_reward = (ep_reward / ep_timestep).item()\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            **{\n",
    "            'episode': episode,\n",
    "            'update': int(timestep / train_kwargs['update_timesteps']),\n",
    "            'end_timestep': timestep,\n",
    "            'average_reward': ep_reward,\n",
    "            'action_std': policy.action_std,\n",
    "            },\n",
    "            **{'rewards/'+k: (v / ep_timestep).item() for k, v in ep_itemized_reward.items()},\n",
    "        })\n",
    "    timer.log('Record Stats')\n",
    "\n",
    "    # Decay model std\n",
    "    if early_stopping(ep_reward):\n",
    "        # End if already at minimum\n",
    "        if policy.action_std <= policy.action_std_min:\n",
    "            print(f'Ending early on episode {episode} and timestep {timestep}')\n",
    "            break\n",
    "\n",
    "        # Decay and reset early stop\n",
    "        policy.decay_action_std()\n",
    "        early_stopping.reset()\n",
    "\n",
    "        # CLI\n",
    "        print(f'Decaying std to {policy.action_std} on episode {episode} and timestep {timestep}')\n",
    "    timer.log('Early Stopping')\n",
    "\n",
    "    # Iterate\n",
    "    episode += 1\n",
    "\n",
    "# CLI Timer\n",
    "print()\n",
    "timer.aggregate('sum')\n",
    "\n",
    "# Save model\n",
    "wgt_file = os.path.join(MODEL_FOLDER, 'policy.wgt')\n",
    "torch.save(policy.state_dict(), wgt_file)  # Save just weights\n",
    "if use_wandb: wandb.save(wgt_file)\n",
    "mdl_file = os.path.join(MODEL_FOLDER, 'policy.mdl')\n",
    "torch.save(policy, mdl_file)  # Save whole model\n",
    "if use_wandb: wandb.save(mdl_file)\n",
    "\n",
    "# Finish wandb\n",
    "if use_wandb: wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
