{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cython is not active\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['RAY_DEDUP_LOGS'] = '0'\n",
    "import functools as ft\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "import tqdm.auto\n",
    "import tqdm.notebook\n",
    "tqdm.notebook.tqdm = tqdm.auto.tqdm  # Enable text output in notebooks\n",
    "\n",
    "import celltrip\n",
    "\n",
    "# Detect Cython\n",
    "CYTHON_ACTIVE = os.path.splitext(celltrip.utility.general.__file__)[1] in ('.c', '.so')\n",
    "print(f'Cython is{\" not\" if not CYTHON_ACTIVE else \"\"} active')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High priority\n",
    "  - Implement stages\n",
    "  - Allow memory to pre-process keys and persistent storage\n",
    "  - Add checkpoints\n",
    "  - Add model loading\n",
    "  - Add train/val to dataloader\n",
    "  - Add state manager to env and then parallelize in analysis, maybe make `analyze` function\n",
    "  - Arguments\n",
    "  - Partition detection in `train_policy`\n",
    "  - Seed policy initialization, add reproducibility tag to wait for all rollouts before updating\n",
    "  - Add metric returns for updates\n",
    "  - Add hook for wandb, etc.\n",
    "  - Local data loading per worker\n",
    "  - Subtract working memory on host node\n",
    "- Medium Priority\n",
    "  - Add parallelism on max_batch and update. With update, encase whole epoch as ray function so splitting occurs within ray function, using ray.remote inline API to allow for non-ray usage. Then, adjustable policy weight sync (i.e. 1 epoch, 10 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Arguments\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser(description='Train CellTRIP model', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "# # TODO: Figure out how to format arguments for appending h5ad files\n",
    "# parser.add_argument('datasets', type=str, required=False, help='.h5ad files to use for data')\n",
    "# parser.add_argument('--concatenate', type=str, required=False, help=\n",
    "#     '.h5ad files to concatenate as a single modality, may be used multiple times')\n",
    "\n",
    "# group = parser.add_argument_group('General')\n",
    "# group.add_argument('--seed', default=42, type=int, help='**Seed for random calls during training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/repos/inept/celltrip/utility/processing.py:97: RuntimeWarning: Modality 1 too small for PCA (2 features), skipping\n",
      "  warnings.warn(\n",
      "2025-03-19 02:03:42,392\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://192.168.0.119:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1000\n",
      "\u001b[36m(wrapper pid=3299449)\u001b[0m {'Event Type': 'Rollout', 'Policy Iteration': 0, 'Episode timesteps': 1000, 'Episode reward': -0.48428988456726074, 'Episode itemized reward': {'distance': 0.0, 'origin': 0.0, 'bound': -0.48428988456726074, 'velocity': 0.0, 'action': 0.0}, 'Memories': 200000, 'Partition': None, 'Time': 25.713873845990747, 'memory': 85981003, 'VRAM': 121088000}\n",
      "1000\n",
      "Calibrated rollout memory (0.57 GiB) and VRAM (0.60 GiB)\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "2000\n",
      "\u001b[36m(wrapper pid=3299446)\u001b[0m {'Event Type': 'Rollout', 'Policy Iteration': 0, 'Episode timesteps': 1000, 'Episode reward': -0.4082002639770508, 'Episode itemized reward': {'distance': 0.0, 'origin': 0.0, 'bound': -0.4082002639770508, 'velocity': 0.0, 'action': 0.0}, 'Memories': 200000, 'Partition': None, 'Time': 25.759319161064923, 'memory': 85962920, 'VRAM': 121088000}\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "3000\n",
      "\u001b[36m(wrapper pid=3299451)\u001b[0m {'Event Type': 'Rollout', 'Policy Iteration': 0, 'Episode timesteps': 1000, 'Episode reward': -0.39163991808891296, 'Episode itemized reward': {'distance': 0.0, 'origin': 0.0, 'bound': -0.39163991808891296, 'velocity': 0.0, 'action': 0.0}, 'Memories': 200000, 'Partition': None, 'Time': 24.136948983883485}\n",
      "4000\n",
      "\u001b[36m(wrapper pid=3299453)\u001b[0m {'Event Type': 'Rollout', 'Policy Iteration': 0, 'Episode timesteps': 1000, 'Episode reward': -0.4318999648094177, 'Episode itemized reward': {'distance': 0.0, 'origin': 0.0, 'bound': -0.4318999648094177, 'velocity': 0.0, 'action': 0.0}, 'Memories': 200000, 'Partition': None, 'Time': 24.20915970299393}\n",
      "4000\n",
      "4000\n",
      "4000\n",
      "4000\n",
      "4000\n",
      "4000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "fnames = ['../data/MERFISH/expression.h5ad', '../data/MERFISH/spatial.h5ad']\n",
    "partition_cols = None\n",
    "adatas = celltrip.utility.processing.read_adatas(*fnames, on_disk=False)\n",
    "celltrip.utility.processing.test_adatas(*adatas, partition_cols=partition_cols)\n",
    "\n",
    "# Construct dataloader\n",
    "dataloader = celltrip.utility.processing.PreprocessFromAnnData(\n",
    "    *adatas, partition_cols=partition_cols, num_nodes=200, pca_dim=128, seed=42)\n",
    "modalities, adata_obs, adata_vars = dataloader.sample()\n",
    "\n",
    "# Initialize Ray\n",
    "ray.shutdown()\n",
    "ray.init(\n",
    "    resources={'VRAM': torch.cuda.get_device_properties(0).total_memory},\n",
    "    dashboard_host='0.0.0.0')\n",
    "\n",
    "# Initialize distributed manager\n",
    "policy_init, memory_init = celltrip.train.get_train_initializers(\n",
    "    3, [m.shape[1] for m in modalities])\n",
    "distributed_manager = celltrip.train.DistributedManager(\n",
    "    # modalities=modalities, env_init=env_init,\n",
    "    policy_init=policy_init,\n",
    "    memory_init=memory_init)\n",
    "\n",
    "# Perform training\n",
    "celltrip.train.train_policy(distributed_manager, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cancel\n",
    "# # dm.cancel()\n",
    "# # dm.clean()\n",
    "# # dm.rollout(dummy=True)\n",
    "# # dm.wait()\n",
    "\n",
    "# # Clear locks\n",
    "# dm.policy_manager.release_locks.remote()\n",
    "\n",
    "# # Get policy\n",
    "# device = DEVICE\n",
    "# policy = policy_init().to(device)\n",
    "# celltrip.train.set_policy_state(policy, ray.get(dm.policy_manager.get_policy_state.remote()))\n",
    "\n",
    "# # Get memory\n",
    "# memory = memory_init(policy)\n",
    "# memory.append_memory(\n",
    "#     *ray.get(dm.policy_manager.get_memory_storage.remote()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy.update(memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get state of job from ObjectRef\n",
    "# import ray.util.state\n",
    "# object_id = dm.futures['simulation'][0].hex()\n",
    "# object_state = ray.util.state.get_objects(object_id)[0]\n",
    "# object_state.task_status\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "celltrip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
