{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T02:59:24.226169Z",
     "iopub.status.busy": "2024-10-04T02:59:24.225945Z",
     "iopub.status.idle": "2024-10-04T02:59:24.251175Z",
     "shell.execute_reply": "2024-10-04T02:59:24.250680Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env WANDB_NOTEBOOK_NAME train.ipynb\n",
    "%env WANDB_SILENT true\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import data\n",
    "import celltrip\n",
    "\n",
    "# Set params\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data/')\n",
    "MODEL_FOLDER = os.path.join(BASE_FOLDER, 'models/')\n",
    "if not os.path.isdir(MODEL_FOLDER): os.makedirs(MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minor\n",
    "  - Record which partition is used in WandB\n",
    "  - Add folder and wandb arguments\n",
    "\n",
    "- Medium\n",
    "  - Add train and validation to imputation\n",
    "  - Refactor utilities to many different modules\n",
    "  - Implement early stopping on trajectory env (and change memory buffer to match)\n",
    "\n",
    "- Major\n",
    "  - Add multithreading to forward and distributed to backward (ray)\n",
    "  - Fix reconstruction speed of memories, will result in 10x training speedup (likely the cause for low GPU utilization)\n",
    "\n",
    "- Later\n",
    "  - Convert to functions for use in library\n",
    "\n",
    "- Justification\n",
    "  - Check that rewards are normalized after (?) advantage\n",
    "  - Rerun imputation applications with scaled=False (?) and total_statistics=True\n",
    "  - Make sure that `scaled=True` on `euclidean_dist` for env partitions is justified\n",
    "\n",
    "- LINKS\n",
    "  - [Original paper (pg 24)](https://arxiv.org/pdf/1909.07528.pdf)\n",
    "  - [Original blog](https://openai.com/research/emergent-tool-use)\n",
    "  - [Gym](https://gymnasium.farama.org/)\n",
    "  - [Slides](https://glouppe.github.io/info8004-advanced-machine-learning/pdf/pleroy-hide-and-seek.pdf)\n",
    "  - [PPO implementation](https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py#L38)\n",
    "  - [Residual SA](https://github.com/openai/multi-agent-emergence-environments/blob/bafaf1e11e6398624116761f91ae7c93b136f395/ma_policy/layers.py#L89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Train CellTRIP model', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "# Cast arguments\n",
    "str_or_int = lambda x: int(x) if x.isdecimal() else x\n",
    "int_or_none = lambda x: int(x) if x.lower() != 'none' else None\n",
    "\n",
    "# Important parameters\n",
    "group = parser.add_argument_group('General')\n",
    "group.add_argument('--seed', default=42, type=int, help='**Seed for random calls during training')\n",
    "group.add_argument('--gpu', default='0', type=str, help='**GPU to use for computation')\n",
    "\n",
    "# Data parameters\n",
    "group = parser.add_argument_group('Data')\n",
    "group.add_argument('--dataset', type=str, required=True, help='Dataset to use')\n",
    "group.add_argument('--no_standardize', action='store_true', help='Don\\'t standardize data')\n",
    "group.add_argument('--top_variant', type=int_or_none, nargs='*', help='Top variant features to filter for each modality')\n",
    "group.add_argument('--pca_dim', default=[512, 512], type=int_or_none, nargs='*', help='PCA features to generate for each modality')\n",
    "group.add_argument('--num_nodes', type=int, nargs='*', help='Nodes to sample from data for each episode')\n",
    "\n",
    "# Environment parameters\n",
    "group = parser.add_argument_group('Environment')\n",
    "# TODO: Add more from class\n",
    "group.add_argument('--dim', default=16, type=int, help='CellTRIP output latent space dimension')\n",
    "group.add_argument('--reward_distance_target', type=int, nargs='*', help='Target modalities for imputation, leave empty for imputation')\n",
    "\n",
    "# Environment reward weights\n",
    "group.add_argument('--env_stages', default=[0b00001, 0b10001, 0b10111, 0b01111], type=int, nargs='*', help=(\n",
    "    'Environment stages for training, as input, integers are expected with each binary '\n",
    "    'digit acting as a flag for `penalty_bound`, `penalty_velocity`, `penalty_action`, `reward_distance`, `reward_origin`, respectively'))\n",
    "\n",
    "# Policy parameters\n",
    "group = parser.add_argument_group('Policy')\n",
    "group.add_argument('--max_nodes', type=int, help='**Max number of nodes to include in a single computation (i.e. 100 = 1 self node, 99 neighbor nodes)')\n",
    "group.add_argument('--sample_strategy', choices=('random', 'proximity', 'random-proximity'), default='random-proximity', type=str, help='Neighbor sampling strategy to use if `max_nodes` is fewer than in state')\n",
    "group.add_argument('--reproducible_strategy', default='hash', type=str_or_int, help='Method to enforce reproducible sampling between forward and backward, may be `hash` or int')\n",
    "# Backpropagation\n",
    "group.add_argument('--update_maxbatch', type=int, help='**Total number of memories to sample from during backprop')\n",
    "group.add_argument('--update_batch', default=int(1e4), type=int, help='**Number of memories to sample from during each backprop epoch')\n",
    "group.add_argument('--update_minibatch', default=int(1e4), type=int, help='**Max memories to backprop at a time')\n",
    "group.add_argument('--update_load_level', default='minibatch', choices=('maxbatch', 'batch', 'minibatch'), help='**What stage to reconstruct memories from compressed form')\n",
    "group.add_argument('--update_cast_level', default='minibatch', choices=('maxbatch', 'batch', 'minibatch'), type=str, help='**What stage to cast to GPU memory')\n",
    "# Internal arguments\n",
    "group.add_argument('--feature_embed_dim', default=32, type=int, help='Dimension of modal embedding')\n",
    "group.add_argument('--embed_dim', default=64, type=int, help='Internal dimension of state representation')\n",
    "# Training Arguments\n",
    "group.add_argument('--action_std_init', default=.6, type=float, help='Initial policy randomness, in std')\n",
    "group.add_argument('--action_std_decay', default=.05, type=float, help='Policy randomness decrease per stage iteration')\n",
    "group.add_argument('--action_std_min', default=.15, type=float, help='Final policy randomness')\n",
    "group.add_argument('--memory_prune', default=100, type=int, help='How many memories to prune from the end of the data')\n",
    "\n",
    "# Training parameters\n",
    "group = parser.add_argument_group('Training')\n",
    "group.add_argument('--max_ep_timesteps', default=int(1e3), type=int, help='Number of timesteps per episode')\n",
    "group.add_argument('--max_timesteps', default=int(5e6), type=int, help='Absolute max timesteps')\n",
    "group.add_argument('--update_timesteps', default=int(5e3), type=int, help='Number of timesteps per policy update')\n",
    "group.add_argument('--max_batch', default=None, type=int, help='**Max number of nodes to calculate actions for at a time')\n",
    "group.add_argument('--no_episode_random_samples', action='store_true', help='Don\\'t refresh episode each epoch')\n",
    "group.add_argument('--episode_partitioning_feature', type=int, help='Type feature to partition by for episode random samples')\n",
    "group.add_argument('--use_wandb', action='store_true', help='**Record performance to wandb')\n",
    "\n",
    "# Early stopping parameters\n",
    "group = parser.add_argument_group('Early Stopping')\n",
    "# TODO: Add disable option\n",
    "group.add_argument('--buffer', default=6, type=int, help='Leniency for early stopping criterion, in updates')\n",
    "group.add_argument('--window_size', default=3, type=int, help='Window size for early stopping criterion evaluation, in updates')\n",
    "\n",
    "# Notebook defaults and script handling\n",
    "if not celltrip.utilities.is_notebook():\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = parser.parse_args('--dataset ExSeq --max_nodes 100 --pca_dim None'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse groups\n",
    "# https://stackoverflow.com/a/46929320\n",
    "# TODO: Remove `None` entries\n",
    "arg_groups = {}\n",
    "for group in parser._action_groups:\n",
    "    group_dict = {a.dest: getattr(args, a.dest, None) for a in group._group_actions}\n",
    "    arg_groups[group.title] = argparse.Namespace(**group_dict)\n",
    "# Convert to dict\n",
    "arg_groups = {k1: vars(v1) for k1, v1 in arg_groups.items()}\n",
    "# Invert NOs\n",
    "arg_groups = {k1: {k2[3:] if k2.startswith('no_') else k2: not v2 if k2.startswith('no_') else v2 for k2, v2 in v1.items()} for k1, v1 in arg_groups.items()}\n",
    "# Check list args for None values\n",
    "arg_groups = {k1: {k2: v2[0] if isinstance(v2, list) and len(v2) == 1 and v2[0] is None else v2 for k2, v2 in v1.items()} for k1, v1 in arg_groups.items()}\n",
    "# Scale early stopping parameters\n",
    "for k in ('buffer', 'window_size'):\n",
    "    arg_groups['Early Stopping'][k] *= int(arg_groups['Training']['update_timesteps'] / arg_groups['Training']['max_ep_timesteps'])\n",
    "# Default dimension parameters\n",
    "arg_groups['Policy']['positional_dim'] = 2*arg_groups['Environment']['dim']\n",
    "arg_groups['Policy']['output_dim'] = arg_groups['Environment']['dim']\n",
    "\n",
    "# Unencode env stages\n",
    "env_stages_encoded = arg_groups['Environment'].pop('env_stages')\n",
    "stage_order = ('penalty_bound', 'penalty_velocity', 'penalty_action', 'reward_distance', 'reward_origin')\n",
    "arg_groups['Stages'] = []\n",
    "for num in env_stages_encoded:\n",
    "    arg_groups['Stages'].append({})\n",
    "    for stage in stage_order:\n",
    "        arg_groups['Stages'][-1][stage] = num & 1\n",
    "        num = num >> 1\n",
    "\n",
    "# Set env vars\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=arg_groups['General']['gpu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "torch.manual_seed(arg_groups['General']['seed'])\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed(arg_groups['General']['seed'])\n",
    "np.random.seed(arg_groups['General']['seed'])\n",
    "\n",
    "# Load data\n",
    "modalities, types, features = data.load_data(arg_groups['Data']['dataset'], DATA_FOLDER)\n",
    "\n",
    "# Filter data (TemporalBrain)\n",
    "# mask = [(t.startswith('Adol') or t.startswith('Inf')) for t in types[0][:, 1]]\n",
    "# modalities, types = [m[mask] for m in modalities], [t[mask] for t in types]\n",
    "\n",
    "# Preprocess data\n",
    "ppc = celltrip.utilities.Preprocessing(**arg_groups['Data'], device=DEVICE)\n",
    "processed_modalities, features = ppc.fit_transform(modalities, features)\n",
    "modalities = processed_modalities\n",
    "\n",
    "# Fixed samples\n",
    "if not arg_groups['Training']['episode_random_samples']:\n",
    "    processed_modalities, keys = ppc.subsample(processed_modalities, return_idx=True)\n",
    "    processed_modalities = ppc.cast(processed_modalities)\n",
    "    modalities = processed_modalities\n",
    "\n",
    "# CLI\n",
    "else:\n",
    "    if arg_groups['Training']['episode_partitioning_feature'] is not None:\n",
    "        names, counts = np.unique(types[0][:, arg_groups['Training']['episode_partitioning_feature']], return_counts=True)\n",
    "        print('Episode groups: ' + ', '.join([f'{n} ({c})' for n, c in zip(names, counts)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:02:37.910857Z",
     "iopub.status.busy": "2024-10-04T03:02:37.910641Z",
     "iopub.status.idle": "2024-10-06T09:35:38.277673Z",
     "shell.execute_reply": "2024-10-06T09:35:38.277220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tracking parameters\n",
    "# Use `watch -d -n 0.5 nvidia-smi` to watch CUDA memory usage\n",
    "# Use `top` to watch system memory usage\n",
    "# Run script and put following above function to profile\n",
    "#    from memory_profiler import profile\n",
    "#    @profile\n",
    "# Use cProfiler to profile timing:\n",
    "#    python -m cProfile -s time -o profile.prof train.py\n",
    "#    snakeviz profile.prof\n",
    "\n",
    "# Initialize classes\n",
    "env = celltrip.environments.trajectory(*modalities, **arg_groups['Environment'], **arg_groups['Stages'][0], device=DEVICE)  # Set to first stage\n",
    "arg_groups['Policy']['modal_dims'] = [m.shape[1] for m in env.get_return_modalities()]\n",
    "policy = celltrip.models.PPO(**arg_groups['Policy'], device=DEVICE).train()\n",
    "early_stopping = celltrip.utilities.EarlyStopping(**arg_groups['Early Stopping'])\n",
    "\n",
    "# Initialize wandb\n",
    "if arg_groups['Training']['use_wandb']: wandb.init(\n",
    "    project='CellTRIP',\n",
    "    config={\n",
    "        **{'note/'+k:v for k, v in arg_groups[\"General\"].items()},\n",
    "        **{'data/'+k:v for k, v in arg_groups[\"Data\"].items()},\n",
    "        **{'env/'+k:v for k, v in arg_groups[\"Environment\"].items()},\n",
    "        **{'stages/'+k:v for k, v in arg_groups[\"Stages\"].items()},\n",
    "        **{'policy/'+k:v for k, v in arg_groups[\"Policy\"].items()},\n",
    "        **{'train/'+k:v for k, v in arg_groups[\"Training\"].items()},\n",
    "        **{'es/'+k:v for k, v in arg_groups[\"Early Stopping\"].items()},\n",
    "    },\n",
    ")\n",
    "\n",
    "# Initialize logging vars\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "timer = celltrip.utilities.time_logger(discard_first_sample=True)\n",
    "timestep = 0; episode = 1; stage = 0\n",
    "\n",
    "# CLI\n",
    "print('Beginning training')\n",
    "\n",
    "# Simulation loop\n",
    "while timestep < arg_groups['Training']['max_timesteps']:\n",
    "    # Sample new data\n",
    "    if arg_groups['Training']['episode_random_samples']:\n",
    "        modalities, keys = ppc.subsample(\n",
    "            processed_modalities,\n",
    "            # NOTE: Partitioning currently only supports aligned modalities\n",
    "            partition=types[0][:, arg_groups['Training']['episode_partitioning_feature']] if arg_groups['Training']['episode_partitioning_feature'] is not None else None,\n",
    "            return_idx=True)\n",
    "        modalities = ppc.cast(modalities)\n",
    "        env.set_modalities(modalities)\n",
    "\n",
    "    # Reset environment\n",
    "    env.reset()\n",
    "    timer.log('Reset Environment')\n",
    "\n",
    "    # Start episode\n",
    "    ep_timestep = 0; ep_reward = 0; ep_itemized_reward = defaultdict(lambda: 0)\n",
    "    while ep_timestep < arg_groups['Training']['max_ep_timesteps']:\n",
    "        with torch.no_grad():\n",
    "            # Get current state\n",
    "            state = env.get_state(include_modalities=True)\n",
    "            timer.log('Environment Setup')\n",
    "\n",
    "            # Get actions from policy\n",
    "            actions = policy.act_macro(\n",
    "                state,\n",
    "                keys=keys,\n",
    "                max_batch=arg_groups['Training']['max_batch'],\n",
    "            ).detach()\n",
    "            timer.log('Calculate Actions')\n",
    "\n",
    "            # Step environment and get reward\n",
    "            rewards, finished, itemized_rewards = env.step(actions, return_itemized_rewards=True)\n",
    "            finished = finished or (ep_timestep == arg_groups['Training']['max_ep_timesteps']-1)  # Maybe move logic inside env?\n",
    "            timer.log('Step Environment')\n",
    "\n",
    "            # Record rewards for policy\n",
    "            policy.memory.record(\n",
    "                rewards=rewards.cpu().tolist(),\n",
    "                is_terminals=finished,\n",
    "            )\n",
    "\n",
    "            # Record rewards for logging\n",
    "            ep_reward = ep_reward + rewards.cpu().mean()\n",
    "            for k, v in itemized_rewards.items():\n",
    "                ep_itemized_reward[k] += v.cpu().mean()\n",
    "            timer.log('Record Rewards')\n",
    "\n",
    "        # Iterate\n",
    "        timestep += 1\n",
    "        ep_timestep += 1\n",
    "\n",
    "        # Update model\n",
    "        if timestep % arg_groups['Training']['update_timesteps'] == 0:\n",
    "            # assert False\n",
    "            print(f'Updating model with average reward {np.mean(sum(policy.memory.storage[\"rewards\"], []))} on episode {episode} and timestep {timestep}', end='')\n",
    "            policy.update()\n",
    "            print(f' ({torch.cuda.max_memory_allocated() / 1024**3:.2f} GB CUDA)')\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            timer.log('Update Policy')\n",
    "\n",
    "        # Escape if finished\n",
    "        if finished: break\n",
    "\n",
    "    # Upload stats\n",
    "    ep_reward = (ep_reward / ep_timestep).item()\n",
    "    update = int(timestep / arg_groups['Training']['update_timesteps'])\n",
    "    if arg_groups['Training']['use_wandb']:\n",
    "        wandb.log({\n",
    "            **{\n",
    "            # Measurements\n",
    "            'end_timestep': timestep,\n",
    "            'episode': episode,\n",
    "            'update': update,\n",
    "            'stage': stage,\n",
    "            # Parameters\n",
    "            'action_std': policy.action_std,\n",
    "            # Outputs\n",
    "            'average_reward': ep_reward,\n",
    "            },\n",
    "            **{'rewards/'+k: (v / ep_timestep).item() for k, v in ep_itemized_reward.items()},\n",
    "        })\n",
    "    timer.log('Record Stats')\n",
    "\n",
    "    # Decay model std\n",
    "    if early_stopping(ep_reward) or timestep >= arg_groups['Training']['max_timesteps']:\n",
    "        # Save model\n",
    "        wgt_file = os.path.join(MODEL_FOLDER, f'policy_{stage:02}.wgt')\n",
    "        torch.save(policy.state_dict(), wgt_file)  # Save just weights\n",
    "        if arg_groups['Training']['use_wandb']: wandb.save(wgt_file)\n",
    "        # mdl_file = os.path.join(MODEL_FOLDER, f'policy_{stage:02}.mdl')\n",
    "        # torch.save(policy, mdl_file)  # Save whole model\n",
    "        # if train_kwargs['use_wandb']: wandb.save(mdl_file)\n",
    "\n",
    "        # End if maximum timesteps reached\n",
    "        if timestep >= arg_groups['Training']['max_timesteps']:\n",
    "            print('Maximal timesteps reached')\n",
    "\n",
    "        # End if at minimum `action_std`\n",
    "        if policy.action_std - policy.action_std_min <= 1e-3:\n",
    "            print(f'Ending early on episode {episode} and timestep {timestep}')\n",
    "            break\n",
    "\n",
    "        # Activate next stage or decay\n",
    "        stage += 1\n",
    "        print(f'Advancing training to stage {stage}')\n",
    "        if stage < len(arg_groups['Stages']):\n",
    "            # Activate next stage\n",
    "            env.set_rewards(arg_groups['Stages'][stage])\n",
    "        else:\n",
    "            # Decay policy randomness\n",
    "            policy.decay_action_std()\n",
    "            # CLI\n",
    "            print(f'Decaying std to {policy.action_std} on episode {episode} and timestep {timestep}')\n",
    "        # stage += 1  # Stage var is one ahead of index\n",
    "\n",
    "        # Reset early stopping\n",
    "        early_stopping.reset()\n",
    "    timer.log('Early Stopping')\n",
    "\n",
    "    # Iterate\n",
    "    episode += 1\n",
    "\n",
    "# CLI Timer\n",
    "print()\n",
    "timer.aggregate('sum')\n",
    "\n",
    "# Finish wandb\n",
    "if arg_groups['Training']['use_wandb']: wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
