{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T02:59:24.226169Z",
     "iopub.status.busy": "2024-10-04T02:59:24.225945Z",
     "iopub.status.idle": "2024-10-04T02:59:24.251175Z",
     "shell.execute_reply": "2024-10-04T02:59:24.250680Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env WANDB_NOTEBOOK_NAME train.ipynb\n",
    "%env WANDB_SILENT true\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import data\n",
    "import inept\n",
    "\n",
    "# Set params\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data/')\n",
    "MODEL_FOLDER = os.path.join(BASE_FOLDER, 'models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VERIFY\n",
    "  - Check that rewards are normalized after (?) advantage\n",
    "\n",
    "- HIGH PRIORITY\n",
    "  - Add parallel envs of different sizes\n",
    "  - Fix reconstruction speed of memories, will result in 10x training speedup (likely the cause for low GPU utilization)\n",
    "\n",
    "- LOW PRIORITY\n",
    "  - Add parallel envs of different sizes\n",
    "  - Make backward (MAX_NODES, MAX_BATCH) batching work\n",
    "  - Add multithreading to forward and distributed to backward\n",
    "\n",
    "- LINKS\n",
    "  - [Original paper (pg 24)](https://arxiv.org/pdf/1909.07528.pdf)\n",
    "  - [Original blog](https://openai.com/research/emergent-tool-use)\n",
    "  - [Gym](https://gymnasium.farama.org/)\n",
    "  - [Slides](https://glouppe.github.io/info8004-advanced-machine-learning/pdf/pleroy-hide-and-seek.pdf)\n",
    "  - [PPO implementation](https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py#L38)\n",
    "  - [Residual SA](https://github.com/openai/multi-agent-emergence-environments/blob/bafaf1e11e6398624116761f91ae7c93b136f395/ma_policy/layers.py#L89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:02:37.884210Z",
     "iopub.status.busy": "2024-10-04T03:02:37.883922Z",
     "iopub.status.idle": "2024-10-04T03:02:37.908516Z",
     "shell.execute_reply": "2024-10-04T03:02:37.908096Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notebook kwargs\n",
    "note_kwargs = {'seed': 42}\n",
    "\n",
    "# Data parameters\n",
    "data_kwargs = {\n",
    "    'dataset': 'MouseVisual',\n",
    "    'standardize': True,\n",
    "    # 'pca_dim': [min(512, *M.shape) for M in modalities],\n",
    "    'num_nodes': 10,\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "env_kwargs = {\n",
    "    'dim': 3,  # 2 = (x, y, vx, vy), 3 = (x, y, z, vx, vy, vz)\n",
    "    'reward_distance_target': 1,  # None tries to emulate all modalities, ints or lists of ints only consider those modalities as targets\n",
    "}\n",
    "\n",
    "# Environment reward weights\n",
    "stages_kwargs = {\n",
    "    'env': (\n",
    "        # Boundary, +origin, +vel+act, -origin+dist\n",
    "        {'penalty_bound': 1},\n",
    "        {'reward_origin': 1},\n",
    "        {'penalty_velocity': 1, 'penalty_action': 1},\n",
    "        {'reward_origin': 0, 'reward_distance': 1},\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Policy parameters\n",
    "policy_kwargs = {\n",
    "    # Main arguments\n",
    "    'num_features_per_node': 2*env_kwargs['dim'],\n",
    "    # 'modal_sizes': None,  # Determined in the running script\n",
    "    'output_dim': env_kwargs['dim'],\n",
    "    # Backpropagation\n",
    "    'update_maxbatch': None,  # Total memory to sample from during backprop\n",
    "    'update_batch': int(1e4),  # Memory to sample from during each backprop epoch\n",
    "    'update_minibatch': int(1e4),  # Max memories to backprop at a time\n",
    "    'update_load_level': 'minibatch',  # What stage to reconstruct memories from compressed form\n",
    "    'update_cast_level': 'minibatch',  # What stage to cast to GPU memory\n",
    "    # Internal arguments\n",
    "    'embed_dim': 64,\n",
    "    'feature_embed_dim': 32,\n",
    "    # Training arguments\n",
    "    'action_std_init': .6,\n",
    "    'action_std_min': .1,\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_kwargs = {\n",
    "    'max_ep_timesteps': 1e3,\n",
    "    'max_timesteps': 5e6,\n",
    "    'update_timesteps': 5e3,\n",
    "    'max_batch': None,  # Max number of nodes to calculate actions for at a time\n",
    "    'max_nodes': None,  # Max number of nodes to use as neighbors in action calculation\n",
    "    'episode_random_samples': True,  # Random nodes each epoch\n",
    "    'use_wandb': True,  # Record performance to wandb\n",
    "}\n",
    "\n",
    "# Early stopping parameters\n",
    "es_kwargs = {\n",
    "    # Global parameters\n",
    "    'buffer': 6 * int(train_kwargs['update_timesteps'] / train_kwargs['max_ep_timesteps']),  # 6 training cycles\n",
    "    # `average` method parameters\n",
    "    'window_size': 3 * int(train_kwargs['update_timesteps'] / train_kwargs['max_ep_timesteps']),  # 3 training cycles\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(note_kwargs['seed'])\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed(note_kwargs['seed'])\n",
    "np.random.seed(note_kwargs['seed'])\n",
    "\n",
    "# Load data\n",
    "modalities, types, features = data.load_data(data_kwargs['dataset'], DATA_FOLDER)\n",
    "ppc = inept.utilities.Preprocessing(**data_kwargs, device=DEVICE)\n",
    "processed_modalities = ppc.fit_transform(modalities)\n",
    "\n",
    "# Fixed samples\n",
    "if not train_kwargs['episode_random_samples']:\n",
    "    modalities, keys = ppc.subsample(processed_modalities, return_idx=True)\n",
    "    modalities = ppc.cast(modalities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:02:37.910857Z",
     "iopub.status.busy": "2024-10-04T03:02:37.910641Z",
     "iopub.status.idle": "2024-10-06T09:35:38.277673Z",
     "shell.execute_reply": "2024-10-06T09:35:38.277220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tracking parameters\n",
    "# Use `watch -d -n 0.5 nvidia-smi` to watch CUDA memory usage\n",
    "# Use `top` to watch system memory usage\n",
    "# Run script and put following above function to profile\n",
    "#    from memory_profiler import profile\n",
    "#    @profile\n",
    "# Use cProfiler to profile timing:\n",
    "#    python -m cProfile -s time -o profile.prof train.py\n",
    "#    snakeviz profile.prof\n",
    "\n",
    "# Initialize classes\n",
    "env = inept.environments.trajectory(*modalities, **env_kwargs, **stages_kwargs['env'][0], device=DEVICE)  # Set to first stage\n",
    "policy_kwargs['modal_sizes'] = [m.shape[1] for m in env.get_return_modalities()]\n",
    "policy = inept.models.PPO(**policy_kwargs, device=DEVICE).train()\n",
    "early_stopping = inept.utilities.EarlyStopping(**es_kwargs)\n",
    "\n",
    "# Initialize wandb\n",
    "if train_kwargs['use_wandb']: wandb.init(\n",
    "    project='INEPT',\n",
    "    config={\n",
    "        **{'note/'+k:v for k, v in note_kwargs.items()},\n",
    "        **{'data/'+k:v for k, v in data_kwargs.items()},\n",
    "        **{'env/'+k:v for k, v in env_kwargs.items()},\n",
    "        **{'stages/'+k:v for k, v in stages_kwargs.items()},\n",
    "        **{'policy/'+k:v for k, v in policy_kwargs.items()},\n",
    "        **{'train/'+k:v for k, v in train_kwargs.items()},\n",
    "        **{'es/'+k:v for k, v in es_kwargs.items()},\n",
    "    },\n",
    ")\n",
    "\n",
    "# Initialize logging vars\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "timer = inept.utilities.time_logger(discard_first_sample=True)\n",
    "timestep = 0; episode = 1; stage = 0\n",
    "\n",
    "# CLI\n",
    "print('Beginning training')\n",
    "num_train_nodes = data_kwargs['num_nodes'] if train_kwargs['max_nodes'] is None else min(data_kwargs['num_nodes'], train_kwargs['max_nodes'])\n",
    "num_train_batch = data_kwargs['num_nodes'] if train_kwargs['max_batch'] is None else min(data_kwargs['num_nodes'], train_kwargs['max_nodes'])\n",
    "print(\n",
    "    f'Training using {num_train_nodes} nodes out of a'\n",
    "    f' total {data_kwargs[\"num_nodes\"]} with forward batches of'\n",
    "    f' size {num_train_batch}.'\n",
    ")\n",
    "update_maxbatch_print = (\n",
    "    policy_kwargs[\"update_maxbatch\"]\n",
    "    if policy_kwargs[\"update_maxbatch\"] is not None else \n",
    "    'all'\n",
    ")\n",
    "print(\n",
    "    f'Training on {update_maxbatch_print} states'\n",
    "    f' with batches of size {policy_kwargs[\"update_batch\"]}'\n",
    "    f' and minibatches of size {policy_kwargs[\"update_minibatch\"]}'\n",
    "    f' from {int(train_kwargs[\"update_timesteps\"] * data_kwargs[\"num_nodes\"])} total.')\n",
    "\n",
    "# Simulation loop\n",
    "while timestep < train_kwargs['max_timesteps']:\n",
    "    # Sample new data\n",
    "    if train_kwargs['episode_random_samples']:\n",
    "        modalities, keys = ppc.subsample(processed_modalities, return_idx=True)\n",
    "        modalities = ppc.cast(modalities)\n",
    "        env.set_modalities(modalities)\n",
    "\n",
    "    # Reset environment\n",
    "    env.reset()\n",
    "    timer.log('Reset Environment')\n",
    "\n",
    "    # Start episode\n",
    "    ep_timestep = 0; ep_reward = 0; ep_itemized_reward = defaultdict(lambda: 0)\n",
    "    while ep_timestep < train_kwargs['max_ep_timesteps']:\n",
    "        with torch.no_grad():\n",
    "            # Get current state\n",
    "            state = env.get_state(include_modalities=True)\n",
    "            timer.log('Environment Setup')\n",
    "\n",
    "            # Get actions from policy\n",
    "            actions = policy.act_macro(\n",
    "                state,\n",
    "                keys=keys,\n",
    "                max_batch=train_kwargs['max_batch'],\n",
    "                max_nodes=train_kwargs['max_nodes'],\n",
    "            ).detach()\n",
    "            timer.log('Calculate Actions')\n",
    "\n",
    "            # Step environment and get reward\n",
    "            rewards, finished, itemized_rewards = env.step(actions, return_itemized_rewards=True)\n",
    "            finished = finished or (ep_timestep == train_kwargs['max_ep_timesteps']-1)  # Maybe move logic inside env?\n",
    "            timer.log('Step Environment')\n",
    "\n",
    "            # Record rewards for policy\n",
    "            policy.memory.record(\n",
    "                rewards=rewards.cpu().tolist(),\n",
    "                is_terminals=finished,\n",
    "            )\n",
    "\n",
    "            # Record rewards for logging\n",
    "            ep_reward = ep_reward + rewards.cpu().mean()\n",
    "            for k, v in itemized_rewards.items():\n",
    "                ep_itemized_reward[k] += v.cpu().mean()\n",
    "            timer.log('Record Rewards')\n",
    "\n",
    "        # Iterate\n",
    "        timestep += 1\n",
    "        ep_timestep += 1\n",
    "\n",
    "        # Update model\n",
    "        if timestep % train_kwargs['update_timesteps'] == 0:\n",
    "            # assert False\n",
    "            print(f'Updating model with average reward {np.mean(policy.memory.storage[\"rewards\"])} on episode {episode} and timestep {timestep}', end='')\n",
    "            policy.update()\n",
    "            print(f' ({torch.cuda.max_memory_allocated() / 1024**3:.2f} GB CUDA)')\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            timer.log('Update Policy')\n",
    "\n",
    "        # Escape if finished\n",
    "        if finished: break\n",
    "\n",
    "    # Upload stats\n",
    "    ep_reward = (ep_reward / ep_timestep).item()\n",
    "    update = int(timestep / train_kwargs['update_timesteps'])\n",
    "    if train_kwargs['use_wandb']:\n",
    "        wandb.log({\n",
    "            **{\n",
    "            # Measurements\n",
    "            'end_timestep': timestep,\n",
    "            'episode': episode,\n",
    "            'update': update,\n",
    "            'stage': stage,\n",
    "            # Parameters\n",
    "            'action_std': policy.action_std,\n",
    "            # Outputs\n",
    "            'average_reward': ep_reward,\n",
    "            },\n",
    "            **{'rewards/'+k: (v / ep_timestep).item() for k, v in ep_itemized_reward.items()},\n",
    "        })\n",
    "    timer.log('Record Stats')\n",
    "\n",
    "    # Decay model std\n",
    "    if early_stopping(ep_reward) or timestep >= train_kwargs['max_timesteps']:\n",
    "        # Save model\n",
    "        wgt_file = os.path.join(MODEL_FOLDER, f'policy_{stage:02}.wgt')\n",
    "        torch.save(policy.state_dict(), wgt_file)  # Save just weights\n",
    "        if train_kwargs['use_wandb']: wandb.save(wgt_file)\n",
    "        mdl_file = os.path.join(MODEL_FOLDER, f'policy_{stage:02}.mdl')\n",
    "        torch.save(policy, mdl_file)  # Save whole model\n",
    "        if train_kwargs['use_wandb']: wandb.save(mdl_file)\n",
    "\n",
    "        # End if maximum timesteps reached\n",
    "        if timestep >= train_kwargs['max_timesteps']:\n",
    "            print('Maximal timesteps reached')\n",
    "\n",
    "        # End if at minimum `action_std`\n",
    "        if policy.action_std <= policy.action_std_min:\n",
    "            print(f'Ending early on episode {episode} and timestep {timestep}')\n",
    "            break\n",
    "\n",
    "        # Activate next stage or decay\n",
    "        stage += 1\n",
    "        # CLI\n",
    "        print(f'Advancing training to stage {stage}')\n",
    "        if stage < len(stages_kwargs['env']):\n",
    "            # Activate next stage\n",
    "            env.set_rewards(stages_kwargs['env'][stage])\n",
    "        else:\n",
    "            # Decay policy randomness\n",
    "            policy.decay_action_std()\n",
    "            # CLI\n",
    "            print(f'Decaying std to {policy.action_std} on episode {episode} and timestep {timestep}')\n",
    "\n",
    "        # Reset early stopping\n",
    "        early_stopping.reset()\n",
    "    timer.log('Early Stopping')\n",
    "\n",
    "    # Iterate\n",
    "    episode += 1\n",
    "\n",
    "# CLI Timer\n",
    "print()\n",
    "timer.aggregate('sum')\n",
    "\n",
    "# Finish wandb\n",
    "if train_kwargs['use_wandb']: wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
