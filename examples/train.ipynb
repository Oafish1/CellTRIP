{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T02:59:24.226169Z",
     "iopub.status.busy": "2024-10-04T02:59:24.225945Z",
     "iopub.status.idle": "2024-10-04T02:59:24.251175Z",
     "shell.execute_reply": "2024-10-04T02:59:24.250680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: WANDB_NOTEBOOK_NAME=train.ipynb\n",
      "env: WANDB_SILENT=true\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env WANDB_NOTEBOOK_NAME train.ipynb\n",
    "%env WANDB_SILENT true\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import data\n",
    "import inept\n",
    "\n",
    "# Set params\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data/')\n",
    "MODEL_FOLDER = os.path.join(BASE_FOLDER, 'models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VERIFY\n",
    "  - Check that rewards are normalized after (?) advantage\n",
    "\n",
    "- HIGH PRIORITY\n",
    "  - Record which partition is used in WandB\n",
    "  - Fix reconstruction speed of memories, will result in 10x training speedup (likely the cause for low GPU utilization)\n",
    "\n",
    "- LOW PRIORITY\n",
    "  - Move argument comments inside model, env, etc.\n",
    "  - Add compatibility with no env reward arguments\n",
    "  - Add multithreading to forward and distributed to backward\n",
    "\n",
    "- LINKS\n",
    "  - [Original paper (pg 24)](https://arxiv.org/pdf/1909.07528.pdf)\n",
    "  - [Original blog](https://openai.com/research/emergent-tool-use)\n",
    "  - [Gym](https://gymnasium.farama.org/)\n",
    "  - [Slides](https://glouppe.github.io/info8004-advanced-machine-learning/pdf/pleroy-hide-and-seek.pdf)\n",
    "  - [PPO implementation](https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py#L38)\n",
    "  - [Residual SA](https://github.com/openai/multi-agent-emergence-environments/blob/bafaf1e11e6398624116761f91ae7c93b136f395/ma_policy/layers.py#L89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:02:37.884210Z",
     "iopub.status.busy": "2024-10-04T03:02:37.883922Z",
     "iopub.status.idle": "2024-10-04T03:02:37.908516Z",
     "shell.execute_reply": "2024-10-04T03:02:37.908096Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notebook kwargs\n",
    "note_kwargs = {'seed': 42}\n",
    "\n",
    "# Data parameters\n",
    "data_kwargs = {\n",
    "    'dataset': 'ExSeq',\n",
    "    'standardize': True,\n",
    "    # 'top_variant': [4e4, 4e4],\n",
    "    # 'pca_dim': [512, 512],\n",
    "    'num_nodes': None,\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "env_kwargs = {\n",
    "    'dim': 3,  # 2 = (x, y, vx, vy), 3 = (x, y, z, vx, vy, vz), etc.\n",
    "    'reward_distance_target': 1,  # None tries to emulate all modalities, ints or lists of ints only consider those modalities as targets\n",
    "}\n",
    "\n",
    "# Environment reward weights\n",
    "stages_kwargs = {\n",
    "    'env': (\n",
    "        # Boundary, +origin, +vel+act, -origin+dist\n",
    "        {'penalty_bound': 1},\n",
    "        {'reward_origin': 1},\n",
    "        {'penalty_velocity': 1, 'penalty_action': 1},\n",
    "        {'reward_origin': 0, 'reward_distance': 1},\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Policy parameters\n",
    "policy_kwargs = {\n",
    "    # Main arguments\n",
    "    'positional_dim': 2*env_kwargs['dim'],\n",
    "    # 'modal_dims': None,  # Determined in the running script\n",
    "    'output_dim': env_kwargs['dim'],\n",
    "    # Forward memory management\n",
    "    'max_nodes': 100,  # Max number of nodes to include in a single computation (i.e. 100 = 1 self node, 99 neighbor nodes)\n",
    "    'sample_strategy': 'random-proximity',  # Neighbor sampling strategy to use if `max_nodes` is fewer than in state\n",
    "    'reproducible_strategy': 'hash',  # Method to enforce reproducible sampling between forward and backward\n",
    "    # Backpropagation\n",
    "    'update_maxbatch': None,  # Total memory to sample from during backprop\n",
    "    'update_batch': int(1e4),  # Memory to sample from during each backprop epoch\n",
    "    'update_minibatch': int(1e4),  # Max memories to backprop at a time\n",
    "    'update_load_level': 'minibatch',  # What stage to reconstruct memories from compressed form\n",
    "    'update_cast_level': 'minibatch',  # What stage to cast to GPU memory\n",
    "    # Internal arguments\n",
    "    'embed_dim': 64,\n",
    "    'feature_embed_dim': 32,\n",
    "    # Training arguments\n",
    "    'action_std_init': .6,\n",
    "    'action_std_min': .1,\n",
    "    'memory_prune': 100,  # How many memories to prune from the end of the data\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "train_kwargs = {\n",
    "    'max_ep_timesteps': 1e3,\n",
    "    'max_timesteps': 5e6,\n",
    "    'update_timesteps': 5e3,\n",
    "    'max_batch': None,  # Max number of nodes to calculate actions for at a time\n",
    "    'episode_random_samples': True,  # Refresh episode each epoch\n",
    "    'episode_partitioning_feature': 1,  # Type feature to partition by for episode random samples\n",
    "    'use_wandb': False,  # Record performance to wandb\n",
    "}\n",
    "\n",
    "# Early stopping parameters\n",
    "es_kwargs = {\n",
    "    # Global parameters\n",
    "    'buffer': 6 * int(train_kwargs['update_timesteps'] / train_kwargs['max_ep_timesteps']),  # 6 training cycles\n",
    "    # `average` method parameters\n",
    "    'window_size': 3 * int(train_kwargs['update_timesteps'] / train_kwargs['max_ep_timesteps']),  # 3 training cycles\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "torch.manual_seed(note_kwargs['seed'])\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed(note_kwargs['seed'])\n",
    "np.random.seed(note_kwargs['seed'])\n",
    "\n",
    "# Load data\n",
    "modalities, types, features = data.load_data(data_kwargs['dataset'], DATA_FOLDER)\n",
    "\n",
    "# Filter data (TemporalBrain)\n",
    "# mask = [(t.startswith('Adol') or t.startswith('Inf')) for t in types[0][:, 1]]\n",
    "# modalities, types = [m[mask] for m in modalities], [t[mask] for t in types]\n",
    "\n",
    "# Preprocess data\n",
    "ppc = inept.utilities.Preprocessing(**data_kwargs, device=DEVICE)\n",
    "processed_modalities = ppc.fit_transform(modalities)\n",
    "modalities = processed_modalities\n",
    "\n",
    "# Fixed samples\n",
    "if not train_kwargs['episode_random_samples']:\n",
    "    processed_modalities, keys = ppc.subsample(processed_modalities, return_idx=True)\n",
    "    processed_modalities = ppc.cast(processed_modalities)\n",
    "    modalities = processed_modalities\n",
    "\n",
    "# CLI\n",
    "else:\n",
    "    if train_kwargs['episode_partitioning_feature'] is not None:\n",
    "        names, counts = np.unique(types[0][:, train_kwargs['episode_partitioning_feature']], return_counts=True)\n",
    "        print('Episode groups: ' + ', '.join([f'{n} ({c})' for n, c in zip(names, counts)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T03:02:37.910857Z",
     "iopub.status.busy": "2024-10-04T03:02:37.910641Z",
     "iopub.status.idle": "2024-10-06T09:35:38.277673Z",
     "shell.execute_reply": "2024-10-06T09:35:38.277220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tracking parameters\n",
    "# Use `watch -d -n 0.5 nvidia-smi` to watch CUDA memory usage\n",
    "# Use `top` to watch system memory usage\n",
    "# Run script and put following above function to profile\n",
    "#    from memory_profiler import profile\n",
    "#    @profile\n",
    "# Use cProfiler to profile timing:\n",
    "#    python -m cProfile -s time -o profile.prof train.py\n",
    "#    snakeviz profile.prof\n",
    "\n",
    "# Initialize classes\n",
    "env = inept.environments.trajectory(*modalities, **env_kwargs, **stages_kwargs['env'][0], device=DEVICE)  # Set to first stage\n",
    "policy_kwargs['modal_dims'] = [m.shape[1] for m in env.get_return_modalities()]\n",
    "policy = inept.models.PPO(**policy_kwargs, device=DEVICE).train()\n",
    "early_stopping = inept.utilities.EarlyStopping(**es_kwargs)\n",
    "\n",
    "# Initialize wandb\n",
    "if train_kwargs['use_wandb']: wandb.init(\n",
    "    project='INEPT',\n",
    "    config={\n",
    "        **{'note/'+k:v for k, v in note_kwargs.items()},\n",
    "        **{'data/'+k:v for k, v in data_kwargs.items()},\n",
    "        **{'env/'+k:v for k, v in env_kwargs.items()},\n",
    "        **{'stages/'+k:v for k, v in stages_kwargs.items()},\n",
    "        **{'policy/'+k:v for k, v in policy_kwargs.items()},\n",
    "        **{'train/'+k:v for k, v in train_kwargs.items()},\n",
    "        **{'es/'+k:v for k, v in es_kwargs.items()},\n",
    "    },\n",
    ")\n",
    "\n",
    "# Initialize logging vars\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "timer = inept.utilities.time_logger(discard_first_sample=True)\n",
    "timestep = 0; episode = 1; stage = 0\n",
    "\n",
    "# CLI\n",
    "print('Beginning training')\n",
    "\n",
    "# Simulation loop\n",
    "while timestep < train_kwargs['max_timesteps']:\n",
    "    # Sample new data\n",
    "    if train_kwargs['episode_random_samples']:\n",
    "        modalities, keys = ppc.subsample(\n",
    "            processed_modalities,\n",
    "            # NOTE: Partitioning currently only supports aligned modalities\n",
    "            partition=types[0][:, train_kwargs['episode_partitioning_feature']] if train_kwargs['episode_partitioning_feature'] is not None else None,\n",
    "            return_idx=True)\n",
    "        modalities = ppc.cast(modalities)\n",
    "        env.set_modalities(modalities)\n",
    "\n",
    "    # Reset environment\n",
    "    env.reset()\n",
    "    timer.log('Reset Environment')\n",
    "\n",
    "    # Start episode\n",
    "    ep_timestep = 0; ep_reward = 0; ep_itemized_reward = defaultdict(lambda: 0)\n",
    "    while ep_timestep < train_kwargs['max_ep_timesteps']:\n",
    "        with torch.no_grad():\n",
    "            # Get current state\n",
    "            state = env.get_state(include_modalities=True)\n",
    "            timer.log('Environment Setup')\n",
    "\n",
    "            # Get actions from policy\n",
    "            actions = policy.act_macro(\n",
    "                state,\n",
    "                keys=keys,\n",
    "                max_batch=train_kwargs['max_batch'],\n",
    "            ).detach()\n",
    "            timer.log('Calculate Actions')\n",
    "\n",
    "            # Step environment and get reward\n",
    "            rewards, finished, itemized_rewards = env.step(actions, return_itemized_rewards=True)\n",
    "            finished = finished or (ep_timestep == train_kwargs['max_ep_timesteps']-1)  # Maybe move logic inside env?\n",
    "            timer.log('Step Environment')\n",
    "\n",
    "            # Record rewards for policy\n",
    "            policy.memory.record(\n",
    "                rewards=rewards.cpu().tolist(),\n",
    "                is_terminals=finished,\n",
    "            )\n",
    "\n",
    "            # Record rewards for logging\n",
    "            ep_reward = ep_reward + rewards.cpu().mean()\n",
    "            for k, v in itemized_rewards.items():\n",
    "                ep_itemized_reward[k] += v.cpu().mean()\n",
    "            timer.log('Record Rewards')\n",
    "\n",
    "        # Iterate\n",
    "        timestep += 1\n",
    "        ep_timestep += 1\n",
    "\n",
    "        # Update model\n",
    "        if timestep % train_kwargs['update_timesteps'] == 0:\n",
    "            # assert False\n",
    "            print(f'Updating model with average reward {np.mean(sum(policy.memory.storage[\"rewards\"], []))} on episode {episode} and timestep {timestep}', end='')\n",
    "            policy.update()\n",
    "            print(f' ({torch.cuda.max_memory_allocated() / 1024**3:.2f} GB CUDA)')\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            timer.log('Update Policy')\n",
    "\n",
    "        # Escape if finished\n",
    "        if finished: break\n",
    "\n",
    "    # Upload stats\n",
    "    ep_reward = (ep_reward / ep_timestep).item()\n",
    "    update = int(timestep / train_kwargs['update_timesteps'])\n",
    "    if train_kwargs['use_wandb']:\n",
    "        wandb.log({\n",
    "            **{\n",
    "            # Measurements\n",
    "            'end_timestep': timestep,\n",
    "            'episode': episode,\n",
    "            'update': update,\n",
    "            'stage': stage,\n",
    "            # Parameters\n",
    "            'action_std': policy.action_std,\n",
    "            # Outputs\n",
    "            'average_reward': ep_reward,\n",
    "            },\n",
    "            **{'rewards/'+k: (v / ep_timestep).item() for k, v in ep_itemized_reward.items()},\n",
    "        })\n",
    "    timer.log('Record Stats')\n",
    "\n",
    "    # Decay model std\n",
    "    if early_stopping(ep_reward) or timestep >= train_kwargs['max_timesteps']:\n",
    "        # Save model\n",
    "        wgt_file = os.path.join(MODEL_FOLDER, f'policy_{stage:02}.wgt')\n",
    "        torch.save(policy.state_dict(), wgt_file)  # Save just weights\n",
    "        if train_kwargs['use_wandb']: wandb.save(wgt_file)\n",
    "        mdl_file = os.path.join(MODEL_FOLDER, f'policy_{stage:02}.mdl')\n",
    "        torch.save(policy, mdl_file)  # Save whole model\n",
    "        if train_kwargs['use_wandb']: wandb.save(mdl_file)\n",
    "\n",
    "        # End if maximum timesteps reached\n",
    "        if timestep >= train_kwargs['max_timesteps']:\n",
    "            print('Maximal timesteps reached')\n",
    "\n",
    "        # End if at minimum `action_std`\n",
    "        if policy.action_std <= policy.action_std_min:\n",
    "            print(f'Ending early on episode {episode} and timestep {timestep}')\n",
    "            break\n",
    "\n",
    "        # Activate next stage or decay\n",
    "        stage += 1\n",
    "        # CLI\n",
    "        print(f'Advancing training to stage {stage}')\n",
    "        if stage < len(stages_kwargs['env']):\n",
    "            # Activate next stage\n",
    "            env.set_rewards(stages_kwargs['env'][stage])\n",
    "        else:\n",
    "            # Decay policy randomness\n",
    "            policy.decay_action_std()\n",
    "            # CLI\n",
    "            print(f'Decaying std to {policy.action_std} on episode {episode} and timestep {timestep}')\n",
    "\n",
    "        # Reset early stopping\n",
    "        early_stopping.reset()\n",
    "    timer.log('Early Stopping')\n",
    "\n",
    "    # Iterate\n",
    "    episode += 1\n",
    "\n",
    "# CLI Timer\n",
    "print()\n",
    "timer.aggregate('sum')\n",
    "\n",
    "# Finish wandb\n",
    "if train_kwargs['use_wandb']: wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
