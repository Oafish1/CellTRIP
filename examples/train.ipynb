{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cython is active\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Set env vars\n",
    "os.environ['RAY_DEDUP_LOGS'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "# Enable text output in notebooks\n",
    "import tqdm.auto\n",
    "import tqdm.notebook\n",
    "tqdm.notebook.tqdm = tqdm.auto.tqdm\n",
    "\n",
    "import celltrip\n",
    "import data\n",
    "\n",
    "# Detect Cython\n",
    "CYTHON_ACTIVE = os.path.splitext(celltrip.utility.general.__file__)[1] in ('.c', '.so')\n",
    "print(f'Cython is{\" not\" if not CYTHON_ACTIVE else \"\"} active')\n",
    "\n",
    "# Set params\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data/')\n",
    "MODEL_FOLDER = os.path.join(BASE_FOLDER, 'models/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High priority\n",
    "  - Optimize cancels to only cancel non-running\n",
    "  - Implement stages\n",
    "  - Add partitioning\n",
    "  - Make ray init command for bash and add to README\n",
    "- Medium Priority\n",
    "  - Add seeding\n",
    "  - Add state manager to env and then parallelize in analysis, maybe make `analyze` function\n",
    "  - Add parallelism on max_batch and update. With update, encase whole epoch as ray function so splitting occurs within ray function, using ray.remote inline API to allow for non-ray usage. Then, adjustable policy weight sync (i.e. 1 epoch, 10 epochs)\n",
    "- Low Priority\n",
    "  - Allow memory to pre-process keys and persistent storage\n",
    "  - Add hook for wandb, ex.\n",
    "  - Move preprocessing to manager\n",
    "  - Figure out why sometimes just throws CUDA not available errors\n",
    "  - Better split_state reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities, types, features = data.load_data('MMD-MA', DATA_FOLDER)\n",
    "ppc = celltrip.utility.processing.Preprocessing(pca_dim=128, device=DEVICE)\n",
    "processed_modalities, features = ppc.fit_transform(modalities, features)\n",
    "# modalities = ppc.cast(processed_modalities)\n",
    "modalities = [m.astype(np.float32) for m in processed_modalities]\n",
    "# modalities = [np.concatenate([m for _ in range(10000)], axis=0) for m in modalities]\n",
    "# modalities = [m[:100] for m in modalities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 20:33:08,988\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://192.168.0.119:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(wrapper pid=2361463)\u001b[0m Timestep 100 - Reward -0.040\n",
      "\u001b[36m(wrapper pid=2361463)\u001b[0m Timestep 200 - Reward -0.233\n",
      "\u001b[36m(wrapper pid=2361463)\u001b[0m Timestep 300 - Reward -0.173\n",
      "\u001b[36m(wrapper pid=2361463)\u001b[0m Timestep 400 - Reward -0.220\n",
      "\u001b[36m(wrapper pid=2361463)\u001b[0m Timestep 500 - Reward -0.180\n",
      "\u001b[36m(wrapper pid=2361463)\u001b[0m Timestep 600 - Reward -0.200\n",
      "\u001b[36m(wrapper pid=2361463)\u001b[0m Timestep 700 - Reward -0.260\n",
      "\u001b[36m(wrapper pid=2361463)\u001b[0m Timestep 800 - Reward -0.193\n",
      "\u001b[36m(wrapper pid=2361463)\u001b[0m Timestep 900 - Reward -0.180\n",
      "\u001b[36m(wrapper pid=2361463)\u001b[0m Timestep 1000 - Reward -0.253\n",
      "\u001b[36m(wrapper pid=2361463)\u001b[0m Simulation finished in 1000 steps with mean reward -0.200\n",
      "\u001b[36m(wrapper pid=2361465)\u001b[0m Timestep 100 - Reward -0.047\n",
      "\u001b[36m(wrapper pid=2361464)\u001b[0m Timestep 100 - Reward -0.053\n",
      "\u001b[36m(wrapper pid=2361470)\u001b[0m Timestep 100 - Reward -0.053\n",
      "\u001b[36m(wrapper pid=2361466)\u001b[0m Timestep 100 - Reward -0.033\n",
      "\u001b[36m(wrapper pid=2361465)\u001b[0m Timestep 200 - Reward -0.307\n",
      "\u001b[36m(wrapper pid=2361470)\u001b[0m Timestep 200 - Reward -0.240\n",
      "\u001b[36m(wrapper pid=2361464)\u001b[0m Timestep 200 - Reward -0.220\n",
      "\u001b[36m(wrapper pid=2361466)\u001b[0m Timestep 200 - Reward -0.173\n",
      "\u001b[36m(wrapper pid=2361470)\u001b[0m Timestep 300 - Reward -0.200\n",
      "\u001b[36m(wrapper pid=2361465)\u001b[0m Timestep 300 - Reward -0.187\n",
      "\u001b[36m(wrapper pid=2361464)\u001b[0m Timestep 300 - Reward -0.293\n",
      "\u001b[36m(wrapper pid=2361466)\u001b[0m Timestep 300 - Reward -0.200\n",
      "\u001b[36m(wrapper pid=2361470)\u001b[0m Timestep 400 - Reward -0.240\n",
      "\u001b[36m(wrapper pid=2361465)\u001b[0m Timestep 400 - Reward -0.220\n",
      "\u001b[36m(wrapper pid=2361464)\u001b[0m Timestep 400 - Reward -0.267\n",
      "\u001b[36m(wrapper pid=2361466)\u001b[0m Timestep 400 - Reward -0.233\n",
      "\u001b[36m(wrapper pid=2361470)\u001b[0m Timestep 500 - Reward -0.280\n",
      "\u001b[36m(wrapper pid=2361465)\u001b[0m Timestep 500 - Reward -0.280\n",
      "\u001b[36m(wrapper pid=2361464)\u001b[0m Timestep 500 - Reward -0.273\n",
      "\u001b[36m(wrapper pid=2361466)\u001b[0m Timestep 500 - Reward -0.253\n",
      "\u001b[36m(wrapper pid=2361470)\u001b[0m Timestep 600 - Reward -0.227\n",
      "\u001b[36m(wrapper pid=2361465)\u001b[0m Timestep 600 - Reward -0.287\n",
      "\u001b[36m(wrapper pid=2361464)\u001b[0m Timestep 600 - Reward -0.253\n",
      "\u001b[36m(wrapper pid=2361466)\u001b[0m Timestep 600 - Reward -0.200\n",
      "\u001b[36m(wrapper pid=2361470)\u001b[0m Timestep 700 - Reward -0.280\n",
      "\u001b[36m(wrapper pid=2361465)\u001b[0m Timestep 700 - Reward -0.267\n",
      "\u001b[36m(wrapper pid=2361464)\u001b[0m Timestep 700 - Reward -0.207\n",
      "\u001b[36m(wrapper pid=2361466)\u001b[0m Timestep 700 - Reward -0.267\n"
     ]
    }
   ],
   "source": [
    "# Behavioral functions\n",
    "dim = 3\n",
    "policy_init = lambda modalities: celltrip.policy.PPO(\n",
    "    positional_dim=2*dim,\n",
    "    modal_dims=[m.shape[1] for m in modalities],\n",
    "    output_dim=dim,\n",
    "    # BACKWARDS\n",
    "    # epochs=5,\n",
    "    # memory_prune=0,\n",
    "    update_load_level='batch',\n",
    "    update_cast_level='minibatch',\n",
    "    update_batch=1e4,\n",
    "    update_minibatch=3e3,\n",
    "    # SAMPLING\n",
    "    # max_batch=100,\n",
    "    max_nodes=100,\n",
    "    # DEVICE\n",
    "    device='cpu')\n",
    "# policy = policy_init(modalities)\n",
    "# policy_init = lambda _: policy\n",
    "env_init = lambda policy, modalities: celltrip.environment.EnvironmentBase(\n",
    "    *modalities,\n",
    "    dim=dim,\n",
    "    # max_timesteps=1e2,\n",
    "    penalty_bound=1,\n",
    "    device=policy.device)\n",
    "memory_init = lambda policy: celltrip.memory.AdvancedMemoryBuffer(\n",
    "    sum(policy.modal_dims),\n",
    "    split_args=policy.split_args)\n",
    "\n",
    "# Initialize ray and distributed\n",
    "ray.shutdown()\n",
    "ray.init(\n",
    "    resources={'VRAM': torch.cuda.get_device_properties(0).total_memory},\n",
    "    dashboard_host='0.0.0.0')\n",
    "dm = celltrip.train.DistributedManager(\n",
    "    modalities,\n",
    "    policy_init=policy_init,\n",
    "    env_init=env_init,\n",
    "    memory_init=memory_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 20:33:09.830911\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queueing 20 rollouts\n",
      "Calibrated rollout memory (0.39 GiB) and VRAM (0.47 GiB)\n",
      "Queueing 20 rollouts\n"
     ]
    }
   ],
   "source": [
    "# Train loop iter\n",
    "max_rollout_futures = 20\n",
    "num_updates = 0; calibrated = False\n",
    "while True:\n",
    "    # Retrieve active futures\n",
    "    futures = dm.get_futures()\n",
    "    num_futures = len(dm.get_all_futures())\n",
    "\n",
    "    # CLI\n",
    "    # print('; '.join([f'{k} ({len(v)})' for k, v in futures.items()]))\n",
    "    # print(ray.available_resources())\n",
    "\n",
    "    ## Check for futures to add\n",
    "    # Check memory and apply update if needed \n",
    "    if len(futures['update']) == 0 and dm.get_memory_len() >= int(1e6):\n",
    "        # assert False\n",
    "        print(f'Queueing policy update {num_updates+1}')\n",
    "        dm.cancel()  # Cancel all non-running (TODO)\n",
    "        dm.update()\n",
    "\n",
    "    # Add rollouts if no update future and below max queued futures\n",
    "    elif len(futures['update']) == 0 and num_futures < max_rollout_futures:\n",
    "        num_new_rollouts = max_rollout_futures - num_futures\n",
    "        print(f'Queueing {num_new_rollouts} rollouts')\n",
    "        dm.rollout(num_new_rollouts, dummy=False)\n",
    "\n",
    "    ## Check for completed futures\n",
    "    # Completed rollouts\n",
    "    if len(ray.wait(futures['rollout'], timeout=0)[0]) > 0:\n",
    "        # Calibrate if needed\n",
    "        all_variants_run = True  # TODO: Set to true if all partitions have been run\n",
    "        if dm.resources['rollout']['core']['memory'] == 0 and all_variants_run:\n",
    "            dm.calibrate()\n",
    "            print(\n",
    "                f'Calibrated rollout'\n",
    "                f' memory ({dm.resources[\"rollout\"][\"core\"][\"memory\"] / 2**30:.2f} GiB)'\n",
    "                f' and VRAM ({dm.resources[\"rollout\"][\"custom\"][\"VRAM\"] / 2**30:.2f} GiB)')\n",
    "            dm.cancel(); time.sleep(1)  # Cancel all non-running (TODO)\n",
    "            dm.policy_manager.release_locks.remote()\n",
    "        # Clean if calibrated\n",
    "        if dm.resources['rollout']['core']['memory'] != 0: dm.clean('rollout')\n",
    "\n",
    "    # Completed updates\n",
    "    if len(ray.wait(futures['update'], timeout=0)[0]) > 0:\n",
    "        num_updates += 1\n",
    "        # Calibrate if needed\n",
    "        if dm.resources['update']['core']['memory'] == 0:\n",
    "            dm.calibrate()\n",
    "            print(\n",
    "                f'Calibrated update'\n",
    "                f' memory ({dm.resources[\"update\"][\"core\"][\"memory\"] / 2**30:.3f} GiB)'\n",
    "                f' and VRAM ({dm.resources[\"update\"][\"custom\"][\"VRAM\"] / 2**30:.3f} GiB)')\n",
    "        dm.clean('update')\n",
    "\n",
    "    # Wait for a new completion\n",
    "    num_futures = len(dm.get_all_futures())\n",
    "    if num_futures > 0:\n",
    "        num_completed_futures = len(dm.wait(num_returns=num_futures, timeout=0))\n",
    "        if num_completed_futures != num_futures: dm.wait(num_returns=num_completed_futures+1)\n",
    "\n",
    "    # Escape\n",
    "    if num_updates >= 50: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cancel\n",
    "# # dm.cancel()\n",
    "# # dm.clean()\n",
    "# # dm.rollout(dummy=True)\n",
    "# # dm.wait()\n",
    "\n",
    "# # Clear locks\n",
    "# dm.policy_manager.release_locks.remote()\n",
    "\n",
    "# # Get policy\n",
    "# device = DEVICE\n",
    "# policy = policy_init(modalities).to(device)\n",
    "# celltrip.training.set_policy_state(policy, ray.get(dm.policy_manager.get_policy_state.remote()))\n",
    "\n",
    "# # Get memory\n",
    "# memory = memory_init(policy)\n",
    "# memory.append_memory(\n",
    "#     *ray.get(dm.policy_manager.get_memory_storage.remote()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get state of job from ObjectRef\n",
    "# import ray.util.state\n",
    "# object_id = dm.futures['simulation'][0].hex()\n",
    "# object_state = ray.util.state.get_objects(object_id)[0]\n",
    "# object_state.task_status\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
