{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools as ft\n",
    "import time\n",
    "\n",
    "# Set env vars\n",
    "os.environ['RAY_DEDUP_LOGS'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "# Enable text output in notebooks\n",
    "import tqdm.auto\n",
    "import tqdm.notebook\n",
    "tqdm.notebook.tqdm = tqdm.auto.tqdm\n",
    "\n",
    "import celltrip\n",
    "import data\n",
    "\n",
    "# Detect Cython\n",
    "CYTHON_ACTIVE = os.path.splitext(celltrip.utility.general.__file__)[1] in ('.c', '.so')\n",
    "print(f'Cython is{\" not\" if not CYTHON_ACTIVE else \"\"} active')\n",
    "\n",
    "# Set params\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data/')\n",
    "MODEL_FOLDER = os.path.join(BASE_FOLDER, 'models/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High priority\n",
    "  - Add metric returns for updates\n",
    "  - Optimize cancels to only cancel non-running\n",
    "  - Implement stages\n",
    "  - Add partitioning\n",
    "  - Make ray init command for bash and add to README\n",
    "- Medium Priority\n",
    "  - Add seeding\n",
    "  - Add state manager to env and then parallelize in analysis, maybe make `analyze` function\n",
    "  - Add parallelism on max_batch and update. With update, encase whole epoch as ray function so splitting occurs within ray function, using ray.remote inline API to allow for non-ray usage. Then, adjustable policy weight sync (i.e. 1 epoch, 10 epochs)\n",
    "- Low Priority\n",
    "  - Allow memory to pre-process keys and persistent storage\n",
    "  - Add hook for wandb, ex.\n",
    "  - Move preprocessing to manager\n",
    "  - Figure out why sometimes just throws CUDA not available errors\n",
    "  - Better split_state reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "fnames = ['../data/MERFISH/expression.h5ad', '../data/MERFISH/spatial.h5ad']\n",
    "partition_cols = 'layer'\n",
    "adatas = celltrip.utility.processing.read_adatas(*fnames, on_disk=False)\n",
    "celltrip.utility.processing.test_adatas(*adatas, partition_cols=partition_cols)\n",
    "\n",
    "# Dataloader\n",
    "dataloader = celltrip.utility.processing.PreprocessFromAnnData(\n",
    "    *adatas, partition_cols=partition_cols, pca_dim=128)\n",
    "modalities, adata_obs, adata_vars = dataloader.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modalities, types, features = data.load_data('MMD-MA', DATA_FOLDER)\n",
    "# ppc = celltrip.utility.processing.Preprocessing(pca_dim=128, device=DEVICE)\n",
    "# processed_modalities, features = ppc.fit_transform(modalities, features)\n",
    "# # modalities = ppc.cast(processed_modalities)\n",
    "# modalities = [m.astype(np.float32) for m in processed_modalities]\n",
    "# # modalities = [np.concatenate([m for _ in range(10000)], axis=0) for m in modalities]\n",
    "# # modalities = [m[:100] for m in modalities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behavioral functions\n",
    "dim = 3\n",
    "modal_dims = [m.shape[1] for m in modalities]\n",
    "policy_init = lambda: celltrip.policy.PPO(\n",
    "    positional_dim=2*dim,\n",
    "    modal_dims=modal_dims,\n",
    "    output_dim=dim,\n",
    "    # BACKWARDS\n",
    "    # epochs=5,\n",
    "    # memory_prune=0,\n",
    "    update_load_level='batch',\n",
    "    update_cast_level='minibatch',\n",
    "    update_batch=1e4,\n",
    "    update_minibatch=3e3,\n",
    "    # SAMPLING\n",
    "    # max_batch=100,\n",
    "    max_nodes=100,\n",
    "    # DEVICE\n",
    "    device='cpu')\n",
    "# policy = policy_init(modalities)\n",
    "# policy_init = lambda _: policy\n",
    "env_init = lambda policy, modalities: celltrip.environment.EnvironmentBase(\n",
    "    *modalities,\n",
    "    dim=dim,\n",
    "    # max_timesteps=1e2,\n",
    "    penalty_bound=1,\n",
    "    device=policy.device)\n",
    "memory_init = lambda policy: celltrip.memory.AdvancedMemoryBuffer(\n",
    "    sum(policy.modal_dims),\n",
    "    split_args=policy.split_args)\n",
    "\n",
    "# Initialize ray and distributed\n",
    "ray.shutdown()\n",
    "ray.init(\n",
    "    resources={'VRAM': torch.cuda.get_device_properties(0).total_memory},\n",
    "    dashboard_host='0.0.0.0')\n",
    "dm = celltrip.train.DistributedManager(\n",
    "    modalities,\n",
    "    policy_init=policy_init,\n",
    "    env_init=env_init,\n",
    "    memory_init=memory_init,\n",
    "    max_jobs_per_gpu=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop iter\n",
    "max_rollout_futures = 1; num_updates = 0; calibrated = False\n",
    "while True:\n",
    "    # Retrieve active futures\n",
    "    futures = dm.get_futures()\n",
    "    num_futures = len(dm.get_all_futures())\n",
    "\n",
    "    # Compute feasibility of futures\n",
    "    available_resources = ray.available_resources()\n",
    "    rollout_feasible = np.array([\n",
    "        v <= available_resources[k]\n",
    "        if k in available_resources else False\n",
    "        for k, v in dm.get_requirements('rollout').items()]).all()\n",
    "    # update_feasible = np.array([\n",
    "    #     v <= available_resources[k]\n",
    "    #     for k, v in dm.get_requirements('update').items()]).all()\n",
    "\n",
    "    # CLI\n",
    "    # print('; '.join([f'{k} ({len(v)})' for k, v in futures.items()]))\n",
    "    # print(ray.available_resources())\n",
    "\n",
    "    ## Check for futures to add\n",
    "    # Check memory and apply update if needed \n",
    "    if len(futures['update']) == 0 and dm.get_memory_len() >= int(1e6):\n",
    "        # assert False\n",
    "        # print(f'Queueing policy update {num_updates+1}')\n",
    "        # dm.cancel()  # Cancel all non-running (TODO)\n",
    "        dm.update(meta={'Policy Iteration': num_updates+1})\n",
    "\n",
    "    # Add rollouts if no update future and below max queued futures\n",
    "    elif rollout_feasible and len(futures['update']) == 0 and num_futures < max_rollout_futures:\n",
    "        # print(f'Queueing {num_new_rollouts} rollouts')\n",
    "        # dm.rollout(num_new_rollouts, dummy=False)\n",
    "        modalities, adata_obs, adata_vars, partition = dataloader.sample(return_partition=True)\n",
    "        dm.rollout(\n",
    "            modalities=modalities,\n",
    "            keys=adata_obs[0].index.to_numpy(),\n",
    "            meta={\n",
    "                'Policy Iteration': num_updates,\n",
    "                'Partition': partition},\n",
    "            dummy=False)\n",
    "\n",
    "    ## Check for completed futures\n",
    "    # Completed rollouts\n",
    "    if len(ray.wait(futures['rollout'], timeout=0)[0]) > 0:\n",
    "        # Calibrate if needed\n",
    "        all_variants_run = True  # TODO: Set to true if all partitions have been run\n",
    "        if dm.resources['rollout']['core']['memory'] == 0 and all_variants_run:\n",
    "            dm.calibrate()\n",
    "            max_rollout_futures = 20\n",
    "            # print(\n",
    "            #     f'Calibrated rollout'\n",
    "            #     f' memory ({dm.resources[\"rollout\"][\"core\"][\"memory\"] / 2**30:.2f} GiB)'\n",
    "            #     f' and VRAM ({dm.resources[\"rollout\"][\"custom\"][\"VRAM\"] / 2**30:.2f} GiB)')\n",
    "            # dm.cancel(); time.sleep(1)  # Cancel all non-running (TODO)\n",
    "            # dm.policy_manager.release_locks.remote()\n",
    "        # Clean if calibrated\n",
    "        if dm.resources['rollout']['core']['memory'] != 0: dm.clean('rollout')\n",
    "\n",
    "    # Completed updates\n",
    "    if len(ray.wait(futures['update'], timeout=0)[0]) > 0:\n",
    "        num_updates += 1\n",
    "        # Calibrate if needed\n",
    "        if dm.resources['update']['core']['memory'] == 0:\n",
    "            dm.calibrate()\n",
    "            # print(\n",
    "            #     f'Calibrated update'\n",
    "            #     f' memory ({dm.resources[\"update\"][\"core\"][\"memory\"] / 2**30:.3f} GiB)'\n",
    "            #     f' and VRAM ({dm.resources[\"update\"][\"custom\"][\"VRAM\"] / 2**30:.3f} GiB)')\n",
    "        dm.clean('update')\n",
    "\n",
    "    # Wait to scan again\n",
    "    # num_futures = len(dm.get_all_futures())\n",
    "    # if num_futures > 0:\n",
    "    #     num_completed_futures = len(dm.wait(num_returns=num_futures, timeout=0))\n",
    "    #     if num_completed_futures != num_futures: dm.wait(num_returns=num_completed_futures+1)\n",
    "    # else: time.sleep(1)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Escape\n",
    "    if num_updates >= 50: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cancel\n",
    "# # dm.cancel()\n",
    "# # dm.clean()\n",
    "# # dm.rollout(dummy=True)\n",
    "# # dm.wait()\n",
    "\n",
    "# # Clear locks\n",
    "# dm.policy_manager.release_locks.remote()\n",
    "\n",
    "# # Get policy\n",
    "# device = DEVICE\n",
    "# policy = policy_init().to(device)\n",
    "# celltrip.train.set_policy_state(policy, ray.get(dm.policy_manager.get_policy_state.remote()))\n",
    "\n",
    "# # Get memory\n",
    "# memory = memory_init(policy)\n",
    "# memory.append_memory(\n",
    "#     *ray.get(dm.policy_manager.get_memory_storage.remote()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy.update(memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get state of job from ObjectRef\n",
    "# import ray.util.state\n",
    "# object_id = dm.futures['simulation'][0].hex()\n",
    "# object_state = ray.util.state.get_objects(object_id)[0]\n",
    "# object_state.task_status\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "celltrip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
