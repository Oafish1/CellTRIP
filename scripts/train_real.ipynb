{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import ray.util.collective as col\n",
    "import ray.util.scheduling_strategies\n",
    "import torch\n",
    "\n",
    "import celltrip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worker Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_init(parent_dir=False):\n",
    "    # Create dataloader\n",
    "    fnames = ['./data/MERFISH/expression.h5ad', './data/MERFISH/spatial.h5ad']\n",
    "    if parent_dir: fnames = ['.' + f for f in fnames]\n",
    "    partition_cols = None  # 'layer'\n",
    "    adatas = celltrip.utility.processing.read_adatas(*fnames, on_disk=False)\n",
    "    celltrip.utility.processing.test_adatas(*adatas, partition_cols=partition_cols)\n",
    "    dataloader = celltrip.utility.processing.PreprocessFromAnnData(\n",
    "        *adatas, partition_cols=partition_cols,  num_nodes=200,\n",
    "        pca_dim=128, seed=42)\n",
    "    # modalities, adata_obs, adata_vars = dataloader.sample()\n",
    "    # Return env\n",
    "    return celltrip.environment.EnvironmentBase(dataloader, dim=3, penalty_bound=1, reward_origin=0)\n",
    "\n",
    "# Default 25Gb Forward, 14Gb Update, at max capacity\n",
    "policy_init = lambda env: celltrip.policy.PPO(\n",
    "    2*env.dim, env.dataloader.modal_dims, env.dim,\n",
    "    forward_batch_size=int(5e5)) # update_iterations=2, minibatch_size=3e3,\n",
    "\n",
    "memory_init = lambda policy: celltrip.memory.AdvancedMemoryBuffer(\n",
    "    sum(policy.modal_dims), split_args=policy.split_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1e-4, num_gpus=1e-4)\n",
    "class Worker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy_init,\n",
    "        env_init,\n",
    "        memory_init=lambda: None,\n",
    "        world_size=1,\n",
    "        rank=0,\n",
    "        learner=True,\n",
    "        parent=None,  # Policy parent worker ref\n",
    "    ):\n",
    "        # Detect device\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # Parameters\n",
    "        self.env = env_init().to(device)\n",
    "        self.policy = policy_init(self.env).to(device)\n",
    "        self.memory = memory_init(self.policy)\n",
    "        self.rank = rank\n",
    "        self.learner = learner\n",
    "        self.parent = parent\n",
    "\n",
    "        # World initialization\n",
    "        if learner: col.init_collective_group(world_size, rank, 'nccl', 'learners')\n",
    "        if parent is None: col.init_collective_group(world_size, rank, 'nccl', 'heads')\n",
    "\n",
    "        # Policy parameters\n",
    "        self.policy_iteration = 0\n",
    "        self.sync_policy(iterate_if_exclusive_runner=False)  # Works because non-heads will wait for head init\n",
    "\n",
    "        # Memory parameters\n",
    "        self.memory_buffer = []\n",
    "\n",
    "    @celltrip.decorator.metrics(append_to_dict=True)\n",
    "    # @celltrip.decorator.profile(time_annotation=True)\n",
    "    def rollout(self, **kwargs):\n",
    "        # Perform rollout\n",
    "        result = celltrip.train.simulate_until_completion(\n",
    "            self.policy, self.env, self.memory, **kwargs)\n",
    "        self.memory.propagate_rewards()\n",
    "        env_nodes = self.env.num_nodes\n",
    "        self.env.reset()\n",
    "\n",
    "        # Clean memory\n",
    "        self.memory.cleanup()\n",
    "\n",
    "        # Record\n",
    "        timestep, reward, itemized_reward = result\n",
    "        ret = {\n",
    "            'Event Type': 'Rollout',\n",
    "            'Policy Iteration': self.policy_iteration,\n",
    "            'Rank': self.rank,\n",
    "            'Timesteps': timestep,\n",
    "            'Memories': timestep*env_nodes,\n",
    "            'Reward': reward,\n",
    "            'Itemized Reward': itemized_reward}\n",
    "        return ret\n",
    "\n",
    "    def rollout_until_new(self, num_new, condition='steps', **kwargs):\n",
    "        # Parameters\n",
    "        if condition == 'memories': measure = self.memory.get_new_len\n",
    "        elif condition == 'steps': measure = self.memory.get_new_steps\n",
    "        else: raise ValueError(f'Condition `{condition}` not found.')\n",
    "\n",
    "        # Compute rollouts\n",
    "        ret = []\n",
    "        while measure() < num_new:\n",
    "            ret.append(self.rollout(**kwargs))\n",
    "        return ret\n",
    "    \n",
    "    @celltrip.decorator.metrics(append_to_dict=True)\n",
    "    # @celltrip.decorator.profile(time_annotation=True)\n",
    "    def update(self, **kwargs):\n",
    "        # Perform update\n",
    "        self.memory.normalize_rewards()\n",
    "        self.policy.update(self.memory, **kwargs, verbose=True)\n",
    "\n",
    "        # Annotate\n",
    "        self.policy_iteration += 1\n",
    "        num_new_memories = self.memory.get_new_len()\n",
    "        num_replay_memories = self.memory.get_replay_len()\n",
    "\n",
    "        # Clean\n",
    "        self.memory.mark_sampled()\n",
    "\n",
    "        # Record\n",
    "        ret = {\n",
    "            'Event Type': 'Update',\n",
    "            'Policy Iteration': self.policy_iteration,\n",
    "            'Rank': self.rank,\n",
    "            'New Memories': num_new_memories,\n",
    "            'Replay Memories': num_replay_memories,\n",
    "            'Total Memories': len(self.memory)}\n",
    "        return ret\n",
    "\n",
    "    @celltrip.decorator.metrics(append_to_dict=True, dict_index=1)\n",
    "    def send_memory(self, **kwargs):\n",
    "        # Put in object store\n",
    "        mem = self.memory.get_storage(**kwargs)\n",
    "        ref = ray.put(mem)\n",
    "\n",
    "        # Record\n",
    "        ret = {\n",
    "            'Event Type': 'Send Memory',\n",
    "            'Rank': self.rank,\n",
    "            'Memories': sum([s.shape[0] for s in mem[0]['states']])}\n",
    "        return ref, ret\n",
    "    \n",
    "    @celltrip.decorator.metrics(append_to_dict=True)\n",
    "    # @celltrip.decorator.profile(time_annotation=True)\n",
    "    def recv_memories(self, new_memories):\n",
    "        # Append memories\n",
    "        num_memories = 0\n",
    "        for new_memory in new_memories:\n",
    "            new_memory = ray.get(new_memory)\n",
    "            self.memory.append_memory(*new_memory)\n",
    "            num_memories += sum([s.shape[0] for s in new_memory[0]['states']])\n",
    "\n",
    "        # Clean memory\n",
    "        self.memory.cleanup()\n",
    "\n",
    "        # Record\n",
    "        ret = {\n",
    "            'Event Type': 'Receive Memories',\n",
    "            'Rank': self.rank,\n",
    "            'Memories': num_memories}\n",
    "        return ret\n",
    "    \n",
    "    def get_policy_state(self):\n",
    "        return celltrip.train.get_policy_state(self.policy)\n",
    "    \n",
    "    @celltrip.decorator.metrics(append_to_dict=True)\n",
    "    def sync_policy(self, iterate_if_exclusive_runner=True):\n",
    "        # Copy policy\n",
    "        if self.parent is not None:\n",
    "            policy_state = ray.get(self.parent.get_policy_state.remote())\n",
    "            celltrip.train.set_policy_state(self.policy, policy_state)\n",
    "        else:\n",
    "            self.policy.synchronize('heads')\n",
    "\n",
    "        # Iterate policy\n",
    "        if not self.learner and iterate_if_exclusive_runner:\n",
    "            self.policy_iteration += 1\n",
    "            self.memory.mark_sampled()\n",
    "\n",
    "        # Record\n",
    "        ret = {\n",
    "            'Event Type': 'Synchronize Policy',\n",
    "            'Policy Iteration': self.policy_iteration,\n",
    "            'Rank': self.rank,\n",
    "            'Inherited': self.parent is not None}\n",
    "        return ret\n",
    "\n",
    "    def destroy(self):\n",
    "        col.destroy_collective_group()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(\n",
    "    address='ray://127.0.0.1:10001',\n",
    "    runtime_env={\n",
    "        'env_vars': {\n",
    "            'RAY_DEDUP_LOGS': '0',\n",
    "            # Irregular node fixes\n",
    "            # NOTE: Important, NCCL will timeout if network device is non-standard\n",
    "            # 'CUDA_LAUNCH_BLOCKING': '1',  # Slow, only for compatibility with X windows\n",
    "            # 'NCCL_SOCKET_IFNAME': 'tailscale',  # lo,en,wls,docker,tailscale\n",
    "            # 'NCCL_IB_DISABLE': '1',\n",
    "            # 'NCCL_CUMEM_ENABLE': '0',\n",
    "            # 'NCCL_DEBUG': 'INFO',\n",
    "        }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train(\n",
    "    num_gpus,\n",
    "    num_learners,\n",
    "    num_runners,\n",
    "    learners_can_be_runners=True,\n",
    "    sync_across_nodes=True,\n",
    "    updates=50,\n",
    "    steps=int(5e3),\n",
    "    rollout_kwargs={},\n",
    "    update_kwargs={}):\n",
    "    # Make placement group for GPUs\n",
    "    pg_gpu = ray.util.placement_group(num_gpus*[{'CPU': 1, 'GPU': 1}])\n",
    "    ray.get(pg_gpu.ready(), timeout=10)\n",
    "\n",
    "    # Assign workers\n",
    "    num_learner_runners = min(num_learners, num_runners) if learners_can_be_runners else 0\n",
    "    num_exclusive_learners = num_learners - num_learner_runners if learners_can_be_runners else num_learners\n",
    "    num_exclusive_runners = num_runners - num_learner_runners\n",
    "    num_workers = num_exclusive_learners + num_learner_runners + num_exclusive_runners\n",
    "    num_head_workers = min(num_gpus, num_workers)\n",
    "    assert num_learners <= num_gpus, '`num_learners` cannot be greater than `num_gpus`.'\n",
    "\n",
    "    # Create workers\n",
    "    workers = []\n",
    "    for i in range(num_workers):\n",
    "        bundle_idx = i % num_head_workers\n",
    "        child_num = i // num_head_workers\n",
    "        rank = float(bundle_idx + child_num * 10**-(np.floor(np.log10((num_workers-1)//num_head_workers))+1))\n",
    "        parent = workers[bundle_idx] if i >= num_head_workers else None\n",
    "        w = (\n",
    "            Worker\n",
    "                .options(\n",
    "                    scheduling_strategy=ray.util.scheduling_strategies.PlacementGroupSchedulingStrategy(\n",
    "                        placement_group=pg_gpu, placement_group_bundle_index=bundle_idx))\n",
    "                .remote(\n",
    "                    policy_init, env_init, memory_init,\n",
    "                    world_size=num_head_workers, rank=rank,\n",
    "                    learner=i<num_learners, parent=parent))\n",
    "        workers.append(w)\n",
    "    learners = workers[:-num_exclusive_runners]\n",
    "    runners = workers[num_exclusive_learners:]\n",
    "    heads = workers[:num_head_workers]\n",
    "    non_heads = workers[num_head_workers:]\n",
    "\n",
    "    # Run policy updates\n",
    "    # TODO: Add/try async updates\n",
    "    # TODO: Maybe 80 epochs\n",
    "    records = []\n",
    "    for policy_iteration in range(updates):\n",
    "        # Rollouts\n",
    "        num_records = len(records)\n",
    "        if policy_iteration==0:\n",
    "            new_records = ray.get([w.rollout.remote(**rollout_kwargs, return_metrics=True) for w in runners])\n",
    "            records += new_records\n",
    "            for record in records[-(len(records)-num_records):]: print(record)\n",
    "        num_records = len(records)\n",
    "        new_records = ray.get([w.rollout_until_new.remote(num_new=steps/num_workers, **rollout_kwargs) for w in runners])\n",
    "        records += sum(new_records, [])\n",
    "        for record in records[-(len(records)-num_records):]: print(record)\n",
    "\n",
    "        # Collect memories\n",
    "        num_records = len(records)\n",
    "        ret = ray.get([w.send_memory.remote(new=True) for w in runners])\n",
    "        new_memories, new_records = [[r[i] for r in ret] for i in range(2)]\n",
    "        records += new_records\n",
    "        for record in records[-(len(records)-num_records):]: print(record)\n",
    "\n",
    "        # Broadcast memories\n",
    "        num_records = len(records)\n",
    "        new_records = []\n",
    "        for i, w1 in enumerate(learners):\n",
    "            if sync_across_nodes:\n",
    "                new_memories_w1 = [ref for w2, ref in zip(runners, new_memories) if w1!=w2]\n",
    "            else:\n",
    "                new_memories_w1 = new_memories[num_head_workers+i::num_head_workers]\n",
    "            future = w1.recv_memories.remote(new_memories=new_memories_w1)\n",
    "            new_records.append(future)\n",
    "        new_records = ray.get(new_records)\n",
    "        records += new_records\n",
    "        for record in records[-(len(records)-num_records):]: print(record)\n",
    "\n",
    "        # Updates\n",
    "        num_records = len(records)\n",
    "        new_records = ray.get([w.update.remote(\n",
    "            **update_kwargs, return_metrics=policy_iteration==0) for w in learners])\n",
    "        records += new_records\n",
    "        for record in records[-(len(records)-num_records):]: print(record)\n",
    "\n",
    "        # Synchronize policies\n",
    "        num_records = len(records)\n",
    "        new_records = ray.get([w.sync_policy.remote() for w in heads])\n",
    "        new_records += ray.get([w.sync_policy.remote() for w in non_heads])\n",
    "        records += new_records\n",
    "        for record in records[-(len(records)-num_records):]: print(record)\n",
    "\n",
    "    # Destroy\n",
    "    # workers[0].destroy.remote()\n",
    "    # [ray.kill(w) for w in workers]\n",
    "\n",
    "    # Return\n",
    "    return workers\n",
    "\n",
    "start = time.perf_counter()\n",
    "# workers = ray.get(train.remote(2, 2, 4, rollout_kwargs={'dummy': True}, update_kwargs={'update_iterations': 5}))\n",
    "# workers = ray.get(train.remote(2, 2, 4, sync_across_nodes=False))  # sync_across_nodes=False\n",
    "workers = ray.get(train.remote(2, 1, 4))  # TODO: Adjust lr_gamma\n",
    "print(time.perf_counter() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = env_init(parent_dir=True).to('cuda')\n",
    "# policy = policy_init(env).to('cuda')\n",
    "# memory = memory_init(policy)\n",
    "# celltrip.train.simulate_until_completion(policy, env, memory)\n",
    "# memory.propagate_rewards()\n",
    "# memory.normalize_rewards()\n",
    "# # for _ in range(5):\n",
    "# #     memory.append_memory(memory)\n",
    "# len(memory)\n",
    "# # memory.fast_sample(10_000, shuffle=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
