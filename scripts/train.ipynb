{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cython is active\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shlex\n",
    "\n",
    "import ray\n",
    "\n",
    "import celltrip\n",
    "\n",
    "# Detect Cython\n",
    "CYTHON_ACTIVE = os.path.splitext(celltrip.utility.general.__file__)[1] in ('.c', '.so')\n",
    "print(f'Cython is{\" not\" if not CYTHON_ACTIVE else \"\"} active')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train.py s3://nkalafut-celltrip/dyngen/logcounts.h5ad s3://nkalafut-celltrip/dyngen/counts_protein.h5ad --backed --dim 32 --penalty_velocity 0 --penalty_action 0 --train_split .8 --num_gpus 2 --num_learners 2 --num_runners 2 --update_timesteps 1_000_000 --max_timesteps 800_000_000 --dont_sync_across_nodes --logfile s3://nkalafut-celltrip/logs/Dyngen-260121-OnlyPinning.log --flush_iterations 1 --checkpoint_iterations 50 --checkpoint_dir s3://nkalafut-celltrip/checkpoints --checkpoint_name Dyngen-260121-OnlyPinning\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "# NOTE: It is not recommended to use s3 with credentials unless the creds are permanent, the bucket is public, or this is run on AWS\n",
    "parser = argparse.ArgumentParser(description='Train CellTRIP model', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "# Reading\n",
    "group = parser.add_argument_group('Input')\n",
    "group.add_argument('input_files', type=str, nargs='*', help='h5ad files to be used for input')\n",
    "group.add_argument('--merge_files', type=str, action='append', nargs='+', help='h5ad files to merge as input')\n",
    "group.add_argument('--partition_cols', type=str, nargs='+', help='Columns for data partitioning, found in `adata.obs` DataFrame')\n",
    "group.add_argument('--backed', action='store_true', help='Read data directly from disk or s3, saving memory at the cost of time')\n",
    "group.add_argument('--input_modalities', type=int, nargs='+', help='Input modalities to give to CellTRIP')\n",
    "group.add_argument('--sample_counts', type=int, nargs='+', help='Normalized sample counts per modality. 0 indicates no normalization')\n",
    "group.add_argument('--log_modalities', type=int, nargs='+', help='Modalities to which a log transform should be applied')\n",
    "group.add_argument('--pca_dim', type=int, nargs='+', default=[512], help='PCA preprocessing dimension, optionally per-modality. 0 indicates no PCA')\n",
    "group.add_argument('--target_modalities', type=int, nargs='+', help='Target modalities to emulate, dictates environment reward')\n",
    "group.add_argument('--spatial', type=int, nargs='+', help='Which modalities are spatial, dictates pinning strategy')\n",
    "# Algorithm\n",
    "group = parser.add_argument_group('Algorithm')\n",
    "group.add_argument('--dim', type=int, default=32, help='Dimensions in the output latent space')\n",
    "group.add_argument('--discrete', action='store_true', help='Use the discrete model rather than continuous')\n",
    "group.add_argument('--train_mask', type=str, help='File or `obs` column containing boolean training mask')\n",
    "group.add_argument('--train_split', type=float, default=1., help='Fraction of input data to use as training. Overwritten by `train_mask`')\n",
    "group.add_argument('--train_partitions', action='store_true', help='Split training/validation data across partitions rather than samples')\n",
    "# Weights\n",
    "# group.add_argument('--reward_distance', type=float, default=0., help='Distance reward weight')\n",
    "group.add_argument('--reward_pinning', type=float, default=1., help='Pinning reward weight')\n",
    "# group.add_argument('--reward_origin', type=float, default=0., help='Origin reward weight')\n",
    "# group.add_argument('--penalty_bound', type=float, default=0., help='Bound penalty weight')\n",
    "group.add_argument('--penalty_velocity', type=float, default=1., help='Velocity penalty weight')\n",
    "group.add_argument('--penalty_action', type=float, default=1., help='Action penalty weight')\n",
    "# Computation\n",
    "group = parser.add_argument_group('Computation')\n",
    "group.add_argument('--num_gpus', type=int, default=1, help='Number of GPUs to use during computation')\n",
    "group.add_argument('--num_learners', type=int, default=1, help='Number of learners used in backward computation, cannot exceed GPUs')\n",
    "group.add_argument('--num_runners', type=int, default=1, help='Number of workers for environment simulation')\n",
    "# Training\n",
    "group = parser.add_argument_group('Training')\n",
    "group.add_argument('--update_timesteps', type=int, default=int(1e6), help='Number of timesteps recorded before each update')\n",
    "group.add_argument('--max_timesteps', type=int, default=int(8e8), help='Maximum number of timesteps to compute before exiting')\n",
    "group.add_argument('--dont_sync_across_nodes', action='store_true', help='Avoid memory sync across nodes, saving overhead time at the cost of stability')\n",
    "# File saves\n",
    "group = parser.add_argument_group('Logging')\n",
    "group.add_argument('--logfile', type=str, default='cli', help='Location for log file, can be `cli`, `<local_file>`, or `<s3 location>`')\n",
    "group.add_argument('--flush_iterations', default=1, type=int, help='Number of iterations to wait before flushing logs')\n",
    "group.add_argument('--checkpoint', type=str, help='Checkpoint to use for initializing model')\n",
    "group.add_argument('--checkpoint_iterations', type=int, default=50, help='Number of updates to wait before recording checkpoints')\n",
    "group.add_argument('--checkpoint_dir', type=str, default='./checkpoints', help='Directory for checkpoints')\n",
    "group.add_argument('--checkpoint_name', type=str, help='Run name, for checkpointing')\n",
    "\n",
    "# Notebook defaults and script handling\n",
    "if not celltrip.utility.notebook.is_notebook():\n",
    "    # ray job submit -- python train.py...\n",
    "    config = parser.parse_args()\n",
    "else:\n",
    "    # experiment_name = 'Flysta-251026'\n",
    "    # experiment_name = 'MERFISH30k-153-250914'\n",
    "    experiment_name = 'Dyngen-260121-OnlyPinning'\n",
    "    # experiment_name = 'Cortex-251024-2'\n",
    "    # experiment_name = 'CancerVel-250913'\n",
    "    # experiment_name = 'PerturbMM-gex-250928'\n",
    "    # experiment_name = 'DrugSeries-251117-pip-nosampnorm'\n",
    "    # experiment_name = 'ExpVal-251121-nosampnorm'\n",
    "    # experiment_name = 'vcc-251019'\n",
    "    bucket_name = 'nkalafut-celltrip'\n",
    "    command = (\n",
    "        # scGLUE\n",
    "        # f's3://{bucket_name}/scGLUE/Chen-2019-RNA.h5ad s3://{bucket_name}/scGLUE/Chen-2019-ATAC.h5ad '\n",
    "        # f's3://{bucket_name}/scGLUE/Chen-2019-RNA.h5ad s3://{bucket_name}/scGLUE/Chen-2019-ATAC.h5ad --input_modalities 0 --target_modalities 0 '\n",
    "        # f'../data/scglue/Chen-2019-RNA.h5ad ../data/scglue/Chen-2019-ATAC.h5ad --input_modalities 0 --target_modalities 0 '\n",
    "        # Tahoe-100M\n",
    "        # f'--merge_files ' + ' '.join([f's3://{bucket_name}/Tahoe/plate{i}_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab.h5ad' for i in range(1, 15)]) + ' '\n",
    "        # f'--partition_cols sample '\n",
    "\n",
    "        # scMultiSim\n",
    "        # f's3://{bucket_name}/scMultiSim/expression.h5ad s3://{bucket_name}/scMultiSim/peaks.h5ad '\n",
    "        # Dyngen\n",
    "        f's3://{bucket_name}/dyngen/logcounts.h5ad s3://{bucket_name}/dyngen/counts_protein.h5ad '\n",
    "\n",
    "        # MERFISH\n",
    "        # f's3://{bucket_name}/MERFISH/expression.h5ad s3://{bucket_name}/MERFISH/spatial.h5ad --target_modalities 1 --spatial 1 '\n",
    "        # MERFISH Bench\n",
    "        # f's3://{bucket_name}/MERFISH_Bench/expression.h5ad s3://{bucket_name}/MERFISH_Bench/spatial.h5ad '\n",
    "        # f'--target_modalities 1 --spatial 1 '\n",
    "        # MERFISH30k\n",
    "        # f's3://{bucket_name}/MERFISH30k/expression.h5ad s3://{bucket_name}/MERFISH30k/spatial.h5ad '\n",
    "        # f'--target_modalities 1 --spatial 1 '\n",
    "        # f'--partition_cols slice_id '\n",
    "        # Cortex\n",
    "        # f's3://{bucket_name}/Cortex/brain_st_cortex_expression.h5ad s3://{bucket_name}/Cortex/brain_st_cortex_spatial.h5ad '\n",
    "        # f'--sample_counts 10_000 0 '\n",
    "        # f'--log_modalities 0 '\n",
    "        # f'--target_modalities 1 '\n",
    "        # # f'--spatial 1 '\n",
    "\n",
    "        # Flysta3D\n",
    "        # f' '.join([f'--merge_files ' + ' ' .join([f's3://{bucket_name}/Flysta3D/{p}_{m}.h5ad' for p in ('E14-16h_a', 'E16-18h_a', 'L1_a', 'L2_a', 'L3_b')]) for m in ('expression', 'spatial')]) + ' '\n",
    "        # f'--target_modalities 1 --spatial 1 '\n",
    "        # f'--partition_cols development '\n",
    "        # Particular stage Flysta\n",
    "        # f' '.join([f'--merge_files ' + ' ' .join([f's3://{bucket_name}/Flysta3D/{p}_{m}.h5ad' for p in ('L2_a',)]) for m in ('expression', 'spatial')]) + ' '\n",
    "        # f'--target_modalities 1 --spatial 1 '\n",
    "        # f'--partition_cols development '\n",
    "\n",
    "        # TemporalBrain\n",
    "        # f's3://{bucket_name}/TemporalBrain/expression.h5ad s3://{bucket_name}/TemporalBrain/peaks.h5ad '\n",
    "        # f'--partition_cols \"Donor ID\" '\n",
    "\n",
    "        # Virtual Cell Challenge\n",
    "        # f's3://{bucket_name}/VirtualCell/expression.h5ad --sample_counts 10_000 --log_modalities 0 '\n",
    "        # # f'--partition_cols target_gene '\n",
    "        # CancerVel\n",
    "        # NOTE: Make sure to check here that NAN sgAssign partitions are chosen\n",
    "        # f's3://{bucket_name}/CancerVel/expression.h5ad '\n",
    "        # f'--partition_cols days '  # sgAssignNew\n",
    "        # DrugSeries\n",
    "        # f's3://{bucket_name}/DrugSeries/expression.h5ad '\n",
    "        # # f'--sample_counts 10_000 '\n",
    "        # f'--log_modalities 0 '\n",
    "        # # f'--partition_cols treatment '\n",
    "\n",
    "        # ExpVal\n",
    "        # f's3://{bucket_name}/ExpVal/expression.h5ad '\n",
    "        # # f'--sample_counts 10_000 '\n",
    "        # f'--log_modalities 0 '\n",
    "        # f'--partition_cols sample '\n",
    "\n",
    "        # PerturbMM\n",
    "        # f's3://{bucket_name}/PerturbMM/expression.h5ad s3://{bucket_name}/PerturbMM/spatial.h5ad '\n",
    "        # f'--target_modalities 1 '\n",
    "        # f'--spatial 1 '\n",
    "        # f'--partition_cols slice_id '\n",
    "        # PerturbMM GEX\n",
    "        # f's3://{bucket_name}/PerturbMM/expression.h5ad '\n",
    "        # f'--partition_cols slice_id '\n",
    "\n",
    "        f'--backed '\n",
    "        # f'--dim 2 '\n",
    "        # f'--dim 8 '\n",
    "        f'--dim 32 '\n",
    "        # f'--dim 64 '\n",
    "        # f'--pca_dim 512 '\n",
    "        # f'--pca_dim 1024 0 '\n",
    "        # f'--pca_dim 2048 0 '\n",
    "        # f'--discrete '\n",
    "\n",
    "        # Weight modifications\n",
    "        # f'--reward_pinning 0 '\n",
    "        f'--penalty_velocity 0 '\n",
    "        f'--penalty_action 0 '\n",
    "\n",
    "        # Column split\n",
    "        # f'--train_mask is_slice153 '  # MERFISH30k\n",
    "        # f'--train_mask known_not_d6 '  # CancerVel\n",
    "        # f'--train_mask slice_bc1_train '  # PerturbMM\n",
    "        # f'--train_mask train '  # DrugSeries\n",
    "        # f'--train_mask Train '  # ExpVal\n",
    "        # f'--train_mask training '  # VCC\n",
    "        # Sample split\n",
    "        f'--train_split .8 '\n",
    "        # Partition split\n",
    "        # f'--train_split .8 '\n",
    "        # f'--train_partitions '\n",
    "        # Single partition\n",
    "        # f'--train_split .0001 '\n",
    "        # f'--train_partitions '\n",
    "        # All data\n",
    "        # f'--train_split 1. '\n",
    "\n",
    "        f'--num_gpus 2 --num_learners 2 --num_runners 2 '\n",
    "        f'--update_timesteps 1_000_000 '\n",
    "        f'--max_timesteps 800_000_000 '\n",
    "        # f'--update_timesteps 100_000 '\n",
    "        # f'--max_timesteps 100_000_000 '\n",
    "        f'--dont_sync_across_nodes '\n",
    "        f'--logfile s3://{bucket_name}/logs/{experiment_name}.log '\n",
    "        f'--flush_iterations 1 '\n",
    "        # f'--checkpoint s3://nkalafut-celltrip/checkpoints/DrugSeries-251110-0800.weights '\n",
    "        f'--checkpoint_iterations 50 '\n",
    "        f'--checkpoint_dir s3://{bucket_name}/checkpoints '\n",
    "        f'--checkpoint_name {experiment_name}')\n",
    "    config = parser.parse_args(shlex.split(command))\n",
    "    print(f'python train.py {command}')\n",
    "    \n",
    "# Defaults\n",
    "if config.checkpoint_name is None:\n",
    "    config.checkpoint_name = f'RUN_{random.randint(0, 2**32):0>10}'\n",
    "    print(f'Run Name: {config.checkpoint_name}')\n",
    "# print(config)  # CLI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Ray\n",
    "ray.shutdown()\n",
    "a = ray.init(\n",
    "    # address='ray://100.85.187.118:10001',\n",
    "    # address='ray://localhost:10001',\n",
    "    address='auto',\n",
    "    # runtime_env={\n",
    "    #     'py_modules': [celltrip],\n",
    "    #     'pip': '../requirements.txt',\n",
    "    #     'env_vars': {\n",
    "    #         'RAY_DEDUP_LOGS': '0'}},\n",
    "    # '{\"py_modules\": [\"celltrip\"], \"pip\": \"../requirements.txt\", \"env_vars\": {\"RAY_DEDUP_LOGS\": \"0\"}}'\n",
    "    # _system_config={'enable_worker_prestart': True}  # Doesn't really work for scripts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1e-4)\n",
    "def train(config):\n",
    "    import celltrip\n",
    "\n",
    "    # Initialization\n",
    "    dataloader_kwargs = {\n",
    "        'num_nodes': [2**9, 2**11],\n",
    "        'pca_dim': config.pca_dim if len(config.pca_dim) > 1 else config.pca_dim[0],\n",
    "        'sample_count': config.sample_counts,\n",
    "        'pre_log': config.log_modalities,\n",
    "        # 'num_nodes': None,\n",
    "        'mask': config.train_split if config.train_mask is None else config.train_mask,\n",
    "        'mask_partitions': config.train_partitions}  # {'num_nodes': 20, 'pca_dim': 128}\n",
    "    environment_kwargs = {\n",
    "        'input_modalities': config.input_modalities,\n",
    "        'target_modalities': config.target_modalities,\n",
    "        'dim': config.dim,\n",
    "        'discrete': config.discrete,\n",
    "        'reward_pinning': config.reward_pinning,\n",
    "        'penalty_velocity': config.penalty_velocity,\n",
    "        'penalty_action': config.penalty_action}  # , 'spherical': config.discrete\n",
    "    policy_kwargs = {\n",
    "        'forward_batch_size': int(1e3),\n",
    "        'vision_size': int(1e3),\n",
    "        'pinning_spatial': config.spatial}\n",
    "    memory_kwargs = {'device': 'cuda:0'}  # Skips casting, cutting time significantly for relatively small batch sizes\n",
    "    initializers = celltrip.train.get_initializers(\n",
    "        input_files=config.input_files, merge_files=config.merge_files,\n",
    "        backed=config.backed, partition_cols=config.partition_cols,\n",
    "        dataloader_kwargs=dataloader_kwargs,\n",
    "        environment_kwargs=environment_kwargs,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        memory_kwargs=memory_kwargs)\n",
    "\n",
    "    # Stages\n",
    "    stage_functions = [\n",
    "        # lambda w: w.env.set_delta(.1),\n",
    "        # lambda w: w.env.set_delta(.05),\n",
    "        # lambda w: w.env.set_delta(.01),\n",
    "        # lambda w: w.env.set_delta(.005),\n",
    "    ]\n",
    "\n",
    "    # Run function\n",
    "    celltrip.train.train_celltrip(\n",
    "        initializers=initializers,\n",
    "        num_gpus=config.num_gpus, num_learners=config.num_learners,\n",
    "        num_runners=config.num_runners, max_timesteps=config.max_timesteps,\n",
    "        update_timesteps=config.update_timesteps, sync_across_nodes=not config.dont_sync_across_nodes,\n",
    "        flush_iterations=config.flush_iterations,\n",
    "        checkpoint_iterations=config.checkpoint_iterations, checkpoint_dir=config.checkpoint_dir,\n",
    "        checkpoint=config.checkpoint, checkpoint_name=config.checkpoint_name,\n",
    "        stage_functions=stage_functions, logfile=config.logfile)\n",
    "\n",
    "ray.get(train.remote(config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import numpy as np\n",
    "# # import torch\n",
    "# # torch.random.manual_seed(42)\n",
    "# # np.random.seed(42)\n",
    "\n",
    "# # Initialize locally\n",
    "# dataloader_kwargs = {\n",
    "#     'num_nodes': [2**9, 2**11],\n",
    "#     'pca_dim': config.pca_dim if len(config.pca_dim) > 1 else config.pca_dim[0],\n",
    "#     'sample_count': config.sample_counts,\n",
    "#     'pre_log': config.log_modalities,\n",
    "#     # 'num_nodes': None,\n",
    "#     'mask': config.train_split if config.train_mask is None else config.train_mask,\n",
    "#     'mask_partitions': config.train_partitions}  # {'num_nodes': 20, 'pca_dim': 128}\n",
    "# environment_kwargs = {\n",
    "#     'input_modalities': config.input_modalities,\n",
    "#     'target_modalities': config.target_modalities,\n",
    "#     'dim': config.dim,\n",
    "#     'discrete': config.discrete}  # , 'spherical': config.discrete\n",
    "# policy_kwargs = {\n",
    "#     'forward_batch_size': int(1e3),\n",
    "#     'vision_size': int(1e3),\n",
    "#     'pinning_spatial': config.spatial}\n",
    "# # config.update_timesteps = 100_000\n",
    "# # config.max_timesteps = 20_000_000\n",
    "# memory_kwargs = {'device': 'cuda:0'}  # Skips casting, cutting time significantly for relatively small batch sizes\n",
    "# env_init, policy_init, memory_init = celltrip.train.get_initializers(\n",
    "#     input_files=config.input_files, merge_files=config.merge_files,\n",
    "#     backed=config.backed, partition_cols=config.partition_cols,\n",
    "#     dataloader_kwargs=dataloader_kwargs,\n",
    "#     environment_kwargs=environment_kwargs,\n",
    "#     policy_kwargs=policy_kwargs,\n",
    "#     memory_kwargs=memory_kwargs)\n",
    "\n",
    "# # Environment\n",
    "# # os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "# try: env\n",
    "# except: env = env_init().to('cuda')\n",
    "\n",
    "# # Policy\n",
    "# policy = policy_init(env).to('cuda')\n",
    "\n",
    "# # Memory\n",
    "# memory = memory_init(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Forward\n",
    "# import line_profiler\n",
    "# memory.mark_sampled()\n",
    "# memory.cleanup()\n",
    "# prof = line_profiler.LineProfiler(\n",
    "#     celltrip.train.simulate_until_completion,\n",
    "#     celltrip.policy.PPO.forward,\n",
    "#     celltrip.policy.EntitySelfAttentionLite.forward,\n",
    "#     celltrip.policy.ResidualAttention.forward,\n",
    "#     celltrip.environment.EnvironmentBase.step)\n",
    "# ret = prof.runcall(celltrip.train.simulate_until_completion, env, policy, memory, max_memories=config.update_timesteps, reset_on_finish=True)\n",
    "# print('ROLLOUT: ' + f'total: {ret[2]:.3f}, ' + ', '.join([f'{k}: {v:.3f}' for k, v in ret[3].items()]))\n",
    "# # memory.feed_new(policy.reward_standardization)\n",
    "# memory.compute_advantages()  # moving_standardization=policy.reward_standardization\n",
    "# prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Memory pull\n",
    "# import line_profiler\n",
    "# prof = line_profiler.LineProfiler(\n",
    "#     celltrip.memory.AdvancedMemoryBuffer.__getitem__)\n",
    "# ret = prof.runcall(memory.__getitem__, np.random.choice(len(memory), 10_000, replace=False))\n",
    "# memory.compute_advantages()\n",
    "# prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Updates\n",
    "# import line_profiler\n",
    "# prof = line_profiler.LineProfiler(\n",
    "#     # memory.fast_sample, policy.actor_critic.forward,\n",
    "#     celltrip.policy.ResidualAttentionBlock.forward,\n",
    "#     policy.calculate_losses, policy.update,\n",
    "#     celltrip.memory.AdvancedMemoryBuffer.__getitem__)\n",
    "# ret = prof.runcall(policy.update, memory, verbose=True)\n",
    "# print('UPDATE: ' + ', '.join([f'{k}: {v:.3f}' for ret_dict in ret[1:] for k, v in ret_dict.items()]))\n",
    "# prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(int(config.max_timesteps / config.update_timesteps)):\n",
    "#     # Forward\n",
    "#     memory.mark_sampled()\n",
    "#     memory.cleanup()\n",
    "#     ret = celltrip.train.simulate_until_completion(\n",
    "#         env, policy, memory,\n",
    "#         max_memories=config.update_timesteps,\n",
    "#         # max_timesteps=100,\n",
    "#         reset_on_finish=True)\n",
    "#     print('ROLLOUT: ' + f'iterations: {ret[0]: 5.0f}, ' + f'total: {ret[2]: 5.3f}, ' + ', '.join([f'{k}: {v: 5.3f}' for k, v in ret[3].items()]))\n",
    "#     memory.compute_advantages()\n",
    "\n",
    "#     # Update\n",
    "#     # NOTE: Training often only improves when PopArt and actual distribution match\n",
    "#     ret = policy.update(memory, verbose=False)\n",
    "#     print('UPDATE: ' + ', '.join([f'{k}: {v: 5.3f}' for ret_dict in ret[1:] for k, v in ret_dict.items()]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "celltrip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
