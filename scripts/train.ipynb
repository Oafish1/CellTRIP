{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train/test\n",
    "- Save preprocessing\n",
    "- Early stopping for `train_celltrip` based on action_std and/or KL\n",
    "- Maybe [this](https://arxiv.org/abs/2102.09430) but probably not\n",
    "- [EFS on clusters maybe](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/aws.html#start-ray-with-the-ray-cluster-launcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cython is not active\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import ray\n",
    "\n",
    "import celltrip\n",
    "\n",
    "# Detect Cython\n",
    "CYTHON_ACTIVE = os.path.splitext(celltrip.utility.general.__file__)[1] in ('.c', '.so')\n",
    "print(f'Cython is{\" not\" if not CYTHON_ACTIVE else \"\"} active')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train.py s3://nkalafut-celltrip/MERFISH/expression.h5ad s3://nkalafut-celltrip/MERFISH/spatial.h5ad --target_modalities 1 --backed --dim 16 --train_split .8 --num_gpus 3 --num_learners 3 --num_runners 3 --update_timesteps 3_225_000 --max_timesteps 3_225_000_000 --dont_sync_across_nodes --logfile s3://nkalafut-celltrip/logs/doubled-allloss.log --flush_iterations 1 --checkpoint_dir s3://nkalafut-celltrip/checkpoints --checkpoint_name doubled-allloss\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "# NOTE: It is not recommended to use s3 with credentials unless the creds are permanent, the bucket is public, or this is run on AWS\n",
    "parser = argparse.ArgumentParser(description='Train CellTRIP model', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "# Reading\n",
    "group = parser.add_argument_group('Input')\n",
    "group.add_argument('input_files', type=str, nargs='*', help='h5ad files to be used for input')\n",
    "group.add_argument('--merge_files', type=str, action='append', nargs='+', help='h5ad files to merge as input')\n",
    "group.add_argument('--partition_cols', type=str, action='append', nargs='+', help='Columns for data partitioning, found in `adata.obs` DataFrame')\n",
    "group.add_argument('--backed', action='store_true', help='Read data directly from disk or s3, saving memory at the cost of time')\n",
    "group.add_argument('--input_modalities', type=int, nargs='+', help='Input modalities to give to CellTRIP')\n",
    "group.add_argument('--target_modalities', type=int, nargs='+', help='Target modalities to emulate, dictates environment reward')\n",
    "# Algorithm\n",
    "group = parser.add_argument_group('Algorithm')\n",
    "group.add_argument('--dim', type=int, default=16, help='Dimensions in the output latent space')\n",
    "group.add_argument('--train_split', type=float, default=1., help='Fraction of input data to use as training')\n",
    "# Computation\n",
    "group = parser.add_argument_group('Computation')\n",
    "group.add_argument('--num_gpus', type=int, default=1, help='Number of GPUs to use during computation')\n",
    "group.add_argument('--num_learners', type=int, default=1, help='Number of learners used in backward computation, cannot exceed GPUs')\n",
    "group.add_argument('--num_runners', type=int, default=1, help='Number of workers for environment simulation')\n",
    "# Training\n",
    "group = parser.add_argument_group('Training')\n",
    "group.add_argument('--update_timesteps', type=int, default=int(1e6), help='Number of timesteps recorded before each update')\n",
    "group.add_argument('--max_timesteps', type=int, default=int(2e9), help='Maximum number of timesteps to compute before exiting')\n",
    "group.add_argument('--dont_sync_across_nodes', action='store_true', help='Avoid memory sync across nodes, saving overhead time at the cost of stability')\n",
    "# File saves\n",
    "group = parser.add_argument_group('Logging')\n",
    "group.add_argument('--logfile', type=str, default='cli', help='Location for log file, can be `cli`, `<local_file>`, or `<s3 location>`')\n",
    "group.add_argument('--flush_iterations', default=25, type=int, help='Number of iterations to wait before flushing logs')\n",
    "group.add_argument('--checkpoint', type=str, help='Checkpoint to use for initializing model')\n",
    "group.add_argument('--checkpoint_iterations', type=int, default=100, help='Number of updates to wait before recording checkpoints')\n",
    "group.add_argument('--checkpoint_dir', type=str, default='./checkpoints', help='Directory for checkpoints')\n",
    "group.add_argument('--checkpoint_name', type=str, help='Run name, for checkpointing')\n",
    "\n",
    "# Notebook defaults and script handling\n",
    "if not celltrip.utility.notebook.is_notebook():\n",
    "    # ray job submit -- python train.py...\n",
    "    config = parser.parse_args()\n",
    "else:\n",
    "    experiment_name = 'doubled-allloss'\n",
    "    command = (\n",
    "        f's3://nkalafut-celltrip/MERFISH/expression.h5ad s3://nkalafut-celltrip/MERFISH/spatial.h5ad --target_modalities 1 '\n",
    "        # f's3://nkalafut-celltrip/scGLUE/Chen-2019-RNA.h5ad s3://nkalafut-celltrip/scGLUE/Chen-2019-ATAC.h5ad '\n",
    "        f'--backed '\n",
    "        # f'--dim 3 '12288\n",
    "        f'--dim 16 '\n",
    "        f'--train_split .8 '\n",
    "        f'--num_gpus 3 --num_learners 3 --num_runners 3 '\n",
    "        f'--update_timesteps 3_225_000 '\n",
    "        f'--max_timesteps 3_225_000_000 '\n",
    "        f'--dont_sync_across_nodes '\n",
    "        f'--logfile s3://nkalafut-celltrip/logs/{experiment_name}.log '\n",
    "        f'--flush_iterations 1 '\n",
    "        # f'--checkpoint s3://nkalafut-celltrip/checkpoints/doubled-allloss-0100.weights '\n",
    "        f'--checkpoint_dir s3://nkalafut-celltrip/checkpoints '\n",
    "        f'--checkpoint_name {experiment_name}')\n",
    "    config = parser.parse_args(command.split(' '))\n",
    "    print(f'python train.py {command}')\n",
    "    \n",
    "# Defaults\n",
    "if config.checkpoint_name is None:\n",
    "    config.checkpoint_name = f'RUN_{random.randint(0, 2**32):0>10}'\n",
    "    print(f'Run Name: {config.checkpoint_name}')\n",
    "# print(config)  # CLI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start Ray\n",
    "# ray.shutdown()\n",
    "# a = ray.init(\n",
    "#     # address='ray://100.85.187.118:10001',\n",
    "#     address='ray://localhost:10001',\n",
    "#     runtime_env={\n",
    "#         'py_modules': [celltrip],\n",
    "#         'pip': '../requirements.txt',\n",
    "#         'env_vars': {\n",
    "#             # **access_keys,\n",
    "#             'RAY_DEDUP_LOGS': '0'}},\n",
    "#         # 'NCCL_SOCKET_IFNAME': 'tailscale',  # lo,en,wls,docker,tailscale\n",
    "#     _system_config={'enable_worker_prestart': True})  # Doesn't really work for scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ray.remote(num_cpus=1e-4)\n",
    "# def train(config):\n",
    "#     import celltrip\n",
    "\n",
    "#     # Initialization\n",
    "#     dataloader_kwargs = {'mask': config.train_split}  # {'num_nodes': 20, 'pca_dim': 128}\n",
    "#     environment_kwargs = {\n",
    "#         'input_modalities': config.input_modalities,\n",
    "#         'target_modalities': config.target_modalities, 'dim': config.dim}\n",
    "#     initializers = celltrip.train.get_initializers(\n",
    "#         input_files=config.input_files, merge_files=config.merge_files,\n",
    "#         backed=config.backed, dataloader_kwargs=dataloader_kwargs,\n",
    "#         environment_kwargs=environment_kwargs)\n",
    "\n",
    "#     # Stages\n",
    "#     stage_functions = []\n",
    "\n",
    "#     # Run function\n",
    "#     celltrip.train.train_celltrip(\n",
    "#         initializers=initializers,\n",
    "#         num_gpus=config.num_gpus, num_learners=config.num_learners,\n",
    "#         num_runners=config.num_runners, max_timesteps=config.max_timesteps,\n",
    "#         update_timesteps=config.update_timesteps, sync_across_nodes=not config.dont_sync_across_nodes,\n",
    "#         flush_iterations=config.flush_iterations,\n",
    "#         checkpoint_iterations=config.checkpoint_iterations, checkpoint_dir=config.checkpoint_dir,\n",
    "#         checkpoint=config.checkpoint, checkpoint_name=config.checkpoint_name,\n",
    "#         stage_functions=stage_functions, logfile=config.logfile)\n",
    "\n",
    "# ray.get(train.remote(config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/repos/inept/celltrip/utility/processing.py:109: RuntimeWarning: Modality 0 too small for PCA (253 features), skipping\n",
      "  warnings.warn(\n",
      "/home/thema/repos/inept/celltrip/utility/processing.py:109: RuntimeWarning: Modality 1 too small for PCA (2 features), skipping\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize locally\n",
    "os.environ['AWS_PROFILE'] = 'waisman-admin'\n",
    "\n",
    "dataloader_kwargs = {'num_nodes': 200, 'mask': config.train_split}  # {'num_nodes': 20, 'pca_dim': 128}\n",
    "environment_kwargs = {\n",
    "    'input_modalities': config.input_modalities,\n",
    "    'target_modalities': config.target_modalities, 'dim': config.dim}\n",
    "env_init, policy_init, memory_init = celltrip.train.get_initializers(\n",
    "    input_files=config.input_files, merge_files=config.merge_files,\n",
    "    backed=config.backed, dataloader_kwargs=dataloader_kwargs,\n",
    "    # policy_kwargs={'minibatch_size': 10_000},\n",
    "    environment_kwargs=environment_kwargs)\n",
    "del env\n",
    "\n",
    "# Environment\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "try: env\n",
    "except: env = env_init().to('cuda')\n",
    "\n",
    "# Policy\n",
    "policy = policy_init(env).to('cuda')\n",
    "policy.minibatch_size = 10_000\n",
    "# policy.update_iterations = 5\n",
    "\n",
    "# Memory\n",
    "memory = memory_init(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: -180.160, distance: -1.904, origin: 0.000, bound: -169.205, velocity: -0.586, action: -8.465\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "import line_profiler\n",
    "memory.mark_sampled()\n",
    "memory.cleanup()\n",
    "prof = line_profiler.LineProfiler(\n",
    "    celltrip.train.simulate_until_completion,\n",
    "    celltrip.policy.PPO.forward,\n",
    "    celltrip.policy.EntitySelfAttentionLite.forward,\n",
    "    celltrip.policy.ResidualAttention.forward,\n",
    "    celltrip.environment.EnvironmentBase.step)\n",
    "ret = prof.runcall(celltrip.train.simulate_until_completion, env, policy, memory, max_memories=policy.epoch_size, reset_on_finish=True)\n",
    "print(f'total: {ret[2]:.3f}, ' + ', '.join([f'{k}: {v:.3f}' for k, v in ret[3].items()]))\n",
    "# memory.feed_new(policy.reward_standardization)\n",
    "memory.compute_advantages()  # moving_standardization=policy.reward_standardization\n",
    "# prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/repos/inept/celltrip/policy.py:1212: UserWarning: No group \"learners\" found.\n",
      "  warnings.warn(f'No group \"{group}\" found.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0.187, PPO: 0.013, critic: 0.803, entropy: -22.746, KL: 0.024\n",
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 1.32316 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: forward at line 392\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   392                                               def forward(self, x, kv=None, mask=None):\n",
      "   393                                                   # Parameters\n",
      "   394       280          0.0      0.0      0.0          if kv is None: x = kv\n",
      "   395                                           \n",
      "   396                                                   # Residual Attention\n",
      "   397       280          0.1      0.0      6.5          x1 = self.norms[0](x)\n",
      "   398                                                   # kv1 = self.norms[1](kv)\n",
      "   399       280          0.0      0.0      0.0          kv1 = kv\n",
      "   400       280          0.9      0.0     66.2          x2, _ = self.attention(x1, kv1, kv1, attn_mask=mask)\n",
      "   401       280          0.0      0.0      1.6          x = x + x2\n",
      "   402       280          0.1      0.0      6.5          x1 = self.norms[2](x)\n",
      "   403       280          0.2      0.0     17.6          x2 = self.mlp(x1)\n",
      "   404       280          0.0      0.0      1.6          x = x + x2\n",
      "   405       280          0.0      0.0      0.0          return x\n",
      "\n",
      "Total time: 8.09746 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: update at line 991\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   991                                               def update(\n",
      "   992                                                   self,\n",
      "   993                                                   memory,\n",
      "   994                                                   update_iterations=None,\n",
      "   995                                                   standardize_returns=True,\n",
      "   996                                                   verbose=False,\n",
      "   997                                                   # Collective args\n",
      "   998                                                   sync_iterations=None,\n",
      "   999                                                   **kwargs,\n",
      "  1000                                               ):\n",
      "  1001                                                   # NOTE: The number of epochs is spread across `world_size` workers\n",
      "  1002                                                   # NOTE: Assumes col.init_collective_group has already been called if world_size > 1\n",
      "  1003                                                   # Parameters\n",
      "  1004         1          0.0      0.0      0.0          if update_iterations is None: update_iterations = self.update_iterations\n",
      "  1005         1          0.0      0.0      0.0          if sync_iterations is None: sync_iterations = self.sync_iterations\n",
      "  1006                                           \n",
      "  1007                                                   # Collective operations\n",
      "  1008         1          0.0      0.0      0.0          use_collective = col.is_group_initialized('default')\n",
      "  1009                                           \n",
      "  1010                                                   # Batch parameters\n",
      "  1011         1          0.0      0.0      0.0          level_dict = {'pool': 0, 'epoch': 1, 'batch': 2, 'minibatch': 3}\n",
      "  1012         1          0.0      0.0      0.0          load_level = level_dict[self.load_level]\n",
      "  1013         1          0.0      0.0      0.0          cast_level = level_dict[self.cast_level]\n",
      "  1014         1          0.0      0.0      0.0          assert cast_level >= load_level, 'Cannot cast without first loading'\n",
      "  1015                                           \n",
      "  1016                                                   # Determine level sizes\n",
      "  1017         1          0.0      0.0      0.0          memory_size = len(memory)\n",
      "  1018                                                   \n",
      "  1019                                                   # Pool size\n",
      "  1020         1          0.0      0.0      0.0          pool_size = self.pool_size\n",
      "  1021         1          0.0      0.0      0.0          if pool_size is not None: pool_size = min(pool_size, memory_size)\n",
      "  1022                                                   \n",
      "  1023                                                   # Epoch size\n",
      "  1024         1          0.0      0.0      0.0          epoch_size = self.epoch_size if not (self.epoch_size is None and pool_size is not None) else pool_size\n",
      "  1025         1          0.0      0.0      0.0          if epoch_size is not None and pool_size is not None: epoch_size = int(min(epoch_size, pool_size))\n",
      "  1026                                                   # epoch_size = np.ceil(epoch_size / denominator).astype(int)\n",
      "  1027                                                   \n",
      "  1028                                                   # Batch size\n",
      "  1029         1          0.0      0.0      0.0          batch_size = self.batch_size if not (self.batch_size is None and epoch_size is not None) else epoch_size\n",
      "  1030         1          0.0      0.0      0.0          if batch_size is not None and epoch_size is not None: batch_size = int(min(batch_size, epoch_size))\n",
      "  1031                                                   # batch_size = np.ceil(batch_size / denominator).astype(int)\n",
      "  1032                                           \n",
      "  1033                                                   # Level adjustment\n",
      "  1034         1          0.0      0.0      0.0          denominator = self.get_world_size('learners') if sync_iterations == 1 else 1  # Adjust sizes if gradients synchronized across GPUs\n",
      "  1035         1          0.0      0.0      0.0          if epoch_size is not None: epoch_size = np.ceil(epoch_size / denominator).astype(int)\n",
      "  1036         1          0.0      0.0      0.0          if batch_size is not None: batch_size = np.ceil(batch_size / denominator).astype(int)\n",
      "  1037                                           \n",
      "  1038                                                   # Minibatch size\n",
      "  1039         1          0.0      0.0      0.0          minibatch_size = self.minibatch_size if not (self.minibatch_size is None and batch_size is not None) else batch_size\n",
      "  1040         1          0.0      0.0      0.0          if minibatch_size is not None and batch_size is not None: minibatch_size = int(min(minibatch_size, batch_size))\n",
      "  1041                                                   # print(f'{pool_size} - {epoch_size} - {batch_size} - {minibatch_size}')\n",
      "  1042                                           \n",
      "  1043                                                   # Cap at max size to reduce redundancy for sequential samples\n",
      "  1044                                                   # NOTE: Pool->epoch is the only non-sequential sample, and is thus not included here\n",
      "  1045                                                   # max_unique_memories = batch_size * update_iterations\n",
      "  1046                                                   # epoch_size = min(epoch_size, max_unique_memories)\n",
      "  1047                                           \n",
      "  1048                                                   # Update moving return mean\n",
      "  1049                                                   # if standardize_returns:\n",
      "  1050                                                   #     # TODO: Clean this up if it works\n",
      "  1051                                                   #     advantages = torch.concat([memory.storage['advantages'][i] for i in range(memory.get_steps()) if memory.storage['staleness'][i]==0]).to(self.policy_iteration.device)\n",
      "  1052                                                   #     state_vals = torch.concat([memory.storage['state_vals'][i] for i in range(memory.get_steps()) if memory.storage['staleness'][i]==0]).to(self.policy_iteration.device)\n",
      "  1053                                                   #     self.return_standardization.update(advantages - state_vals)\n",
      "  1054                                           \n",
      "  1055                                                   # Load pool\n",
      "  1056         1          0.0      0.0      0.0          total_losses = defaultdict(lambda: [])\n",
      "  1057         1          0.0      0.0      0.0          total_statistics = defaultdict(lambda: [])\n",
      "  1058         4          0.0      0.0      0.0          pool_data = _utility.processing.sample_and_cast(\n",
      "  1059         1          0.0      0.0      0.0              memory, None, None, pool_size,\n",
      "  1060         1          0.0      0.0      0.0              current_level=0, load_level=load_level, cast_level=cast_level,\n",
      "  1061         2          0.0      0.0      0.0              device=self.policy_iteration.device, **kwargs)\n",
      "  1062                                           \n",
      "  1063                                                   # Train\n",
      "  1064         1          0.0      0.0      0.0          iterations = 0; synchronized = True; escape = False\n",
      "  1065         4          0.0      0.0      0.0          while True:\n",
      "  1066                                                       # Load epoch\n",
      "  1067        16          0.0      0.0      0.0              epoch_data = _utility.processing.sample_and_cast(\n",
      "  1068         4          0.0      0.0      0.0                  memory, pool_data, pool_size, epoch_size,\n",
      "  1069         4          0.0      0.0      0.0                  current_level=1, load_level=load_level, cast_level=cast_level,\n",
      "  1070         8          0.0      0.0      0.0                  device=self.policy_iteration.device, **kwargs)\n",
      "  1071         4          0.0      0.0      0.0              batches = np.ceil(epoch_size/batch_size).astype(int) if epoch_size is not None else 1\n",
      "  1072        24          0.0      0.0      0.0              for batch_num in range(batches):\n",
      "  1073                                                           # Load batch\n",
      "  1074        20          0.0      0.0      0.0                  batch_losses = defaultdict(lambda: 0)\n",
      "  1075        20          0.0      0.0      0.0                  batch_statistics = defaultdict(lambda: 0)\n",
      "  1076        80          0.0      0.0      0.0                  batch_data = _utility.processing.sample_and_cast(\n",
      "  1077        20          0.0      0.0      0.0                      memory, epoch_data, epoch_size, batch_size,\n",
      "  1078        20          0.0      0.0      0.0                      current_level=2, load_level=load_level, cast_level=cast_level,\n",
      "  1079        20          0.0      0.0      0.0                      device=self.policy_iteration.device, sequential_num=batch_num,\n",
      "  1080        40          0.0      0.0      0.0                      clip_sequential=False, **kwargs)\n",
      "  1081        20          0.0      0.0      0.0                  batch_returns = torch.zeros(0, device=self.policy_iteration.device)\n",
      "  1082        20          0.0      0.0      0.0                  minibatches = np.ceil(batch_size/minibatch_size).astype(int) if batch_size is not None else 1\n",
      "  1083       160          0.0      0.0      0.0                  for minibatch_num in range(minibatches):\n",
      "  1084                                                               # Load minibatch\n",
      "  1085       560          3.0      0.0     36.6                      minibatch_data, minibatch_actual_size = _utility.processing.sample_and_cast(\n",
      "  1086       140          0.0      0.0      0.0                          memory, batch_data, batch_size, minibatch_size,\n",
      "  1087       140          0.0      0.0      0.0                          current_level=3, load_level=load_level, cast_level=cast_level,\n",
      "  1088       140          0.0      0.0      0.0                          device=self.policy_iteration.device, sequential_num=minibatch_num,\n",
      "  1089       280          0.0      0.0      0.0                          clip_sequential=True, **kwargs)\n",
      "  1090                                           \n",
      "  1091                                                               # Get subset data\n",
      "  1092       140          0.0      0.0      0.0                      states = minibatch_data['states']\n",
      "  1093       140          0.0      0.0      0.0                      actions = minibatch_data['actions']\n",
      "  1094       140          0.0      0.0      0.0                      action_logs = minibatch_data['action_logs']\n",
      "  1095       140          0.0      0.0      0.0                      state_vals = minibatch_data['state_vals']\n",
      "  1096       140          0.0      0.0      0.0                      advantages = minibatch_data['advantages']\n",
      "  1097                                                               # rewards = minibatch_data['propagated_rewards']\n",
      "  1098                                           \n",
      "  1099                                                               # Perform backward\n",
      "  1100       280          2.1      0.0     26.0                      losses, statistics = self.calculate_losses(\n",
      "  1101       140          0.0      0.0      0.0                          states, actions, action_logs, state_vals, advantages=advantages, rewards=None)\n",
      "  1102       140          0.0      0.0      0.1                      loss, loss_ppo, loss_critic, loss_entropy, loss_kl = losses\n",
      "  1103       140          0.0      0.0      0.2                      exp_var, = statistics\n",
      "  1104                                           \n",
      "  1105                                                               # Scale and calculate gradient\n",
      "  1106       140          0.0      0.0      0.0                      accumulation_frac = minibatch_actual_size / batch_size\n",
      "  1107       140          0.0      0.0      0.1                      loss = loss * accumulation_frac\n",
      "  1108       140          2.8      0.0     34.6                      loss.backward()  # Longest computation\n",
      "  1109                                           \n",
      "  1110                                                               # Update moving return mean\n",
      "  1111       140          0.0      0.0      0.1                      batch_returns = torch.cat((batch_returns, advantages+state_vals), dim=0)\n",
      "  1112                                           \n",
      "  1113                                                               # Scale and record\n",
      "  1114       140          0.0      0.0      0.0                      batch_losses['Total'] += loss.detach()\n",
      "  1115       140          0.0      0.0      0.1                      batch_losses['PPO'] += loss_ppo.detach().mean() * accumulation_frac\n",
      "  1116       140          0.0      0.0      0.1                      batch_losses['critic'] += loss_critic.detach().mean() * accumulation_frac\n",
      "  1117       140          0.0      0.0      0.1                      batch_losses['entropy'] += loss_entropy.detach().mean() * accumulation_frac\n",
      "  1118       140          0.0      0.0      0.1                      batch_losses['KL'] += loss_kl.detach().mean() * accumulation_frac\n",
      "  1119       140          0.0      0.0      0.1                      batch_statistics['Moving Return Mean'] += self.return_standardization.mean.mean().item() * accumulation_frac\n",
      "  1120       140          0.0      0.0      0.1                      batch_statistics['Moving Return STD'] += self.return_standardization.std.mean().item() * accumulation_frac\n",
      "  1121       140          0.0      0.0      0.1                      batch_statistics['Moving Reward Mean'] += self.reward_standardization.mean.mean().item() * accumulation_frac\n",
      "  1122       140          0.0      0.0      0.1                      batch_statistics['Moving Reward STD'] += self.reward_standardization.std.mean().item() * accumulation_frac\n",
      "  1123       140          0.0      0.0      0.1                      batch_statistics['Return Mean'] += (advantages+state_vals).detach().mean().item() * accumulation_frac\n",
      "  1124       140          0.0      0.0      0.1                      batch_statistics['Return STD'] += (advantages+state_vals).detach().std().item() * accumulation_frac\n",
      "  1125       140          0.0      0.0      0.1                      batch_statistics['Advantage Mean'] += advantages.detach().mean().item() * accumulation_frac\n",
      "  1126       140          0.0      0.0      0.1                      batch_statistics['Advantage STD'] += advantages.detach().std().item() * accumulation_frac\n",
      "  1127       140          0.0      0.0      0.0                      batch_statistics['Log STD'] += self.get_log_std() * accumulation_frac\n",
      "  1128       140          0.0      0.0      0.0                      batch_statistics['Explained Variance'] += exp_var.detach().item() * accumulation_frac\n",
      "  1129                                                           \n",
      "  1130                                                           # Record\n",
      "  1131       120          0.0      0.0      0.0                  for k, v in batch_losses.items(): total_losses[k].append(v)\n",
      "  1132       220          0.0      0.0      0.0                  for k, v in batch_statistics.items(): total_statistics[k].append(v)\n",
      "  1133                                           \n",
      "  1134                                                           # Synchronize GPU policies and step\n",
      "  1135                                                           # NOTE: Synchronize gradients every batch if =1, else synchronize whole model\n",
      "  1136                                                           # NOTE: =1 keeps optimizers in sync without need for whole-model synchronization\n",
      "  1137        20          0.0      0.0      0.0                  if sync_iterations == 1: self.synchronize('learners', grad=True)  # Sync only grad\n",
      "  1138        20          0.0      0.0      0.0                  if self.kl_early_stop and synchronized: self.copy_policy()\n",
      "  1139        20          0.0      0.0      0.2                  nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.grad_clip)\n",
      "  1140        20          0.0      0.0      0.5                  self.optimizer.step()\n",
      "  1141        20          0.0      0.0      0.0                  self.optimizer.zero_grad()\n",
      "  1142        20          0.0      0.0      0.0                  if sync_iterations != 1:\n",
      "  1143                                                               # Synchronize for offsets\n",
      "  1144                                                               sync_loop = (iterations) % sync_iterations == 0\n",
      "  1145                                                               last_epoch = iterations == update_iterations\n",
      "  1146                                                               if use_collective and (sync_loop or last_epoch):\n",
      "  1147                                                                   self.synchronize('learners')\n",
      "  1148                                                                   synchronized = True\n",
      "  1149                                                               else: synchronized = False\n",
      "  1150                                           \n",
      "  1151                                                           # Update moving return mean\n",
      "  1152        20          0.0      0.0      0.0                  if standardize_returns:\n",
      "  1153        20          0.0      0.0      0.2                      self.return_standardization.update(batch_returns)\n",
      "  1154        20          0.0      0.0      0.0                      self.synchronize('learners', sync_list=self.return_standardization.parameters())\n",
      "  1155                                           \n",
      "  1156                                                           # Update KL beta\n",
      "  1157                                                           # NOTE: Same as Torch KLPENPPOLoss implementation\n",
      "  1158        20          0.0      0.0      0.0                  if self.kl_early_stop or self.kl_beta != 0:\n",
      "  1159                                                               loss_kl_mean = loss_kl.detach().mean()\n",
      "  1160                                                               self.synchronize('learners', sync_list=[loss_kl_mean])\n",
      "  1161                                                               if not self.kl_early_stop:\n",
      "  1162                                                                   exp_limit = 32\n",
      "  1163                                                                   if loss_kl_mean < self.kl_target / 1.5 and self.kl_beta > 2**-exp_limit: self.kl_beta.data *= self.kl_beta_increment[0]\n",
      "  1164                                                                   elif loss_kl_mean > self.kl_target * 1.5 and self.kl_beta < 2**exp_limit: self.kl_beta.data *= self.kl_beta_increment[1]\n",
      "  1165                                           \n",
      "  1166                                                           # Escape and roll back if KLD too high\n",
      "  1167        20          0.0      0.0      0.0                  if self.kl_early_stop:\n",
      "  1168                                                               if loss_kl_mean > 1.5 * self.kl_target:\n",
      "  1169                                                                   if iterations - sync_iterations > 0:\n",
      "  1170                                                                       # Revert to previous synchronized state within kl target\n",
      "  1171                                                                       self.revert_policy()\n",
      "  1172                                                                       # iterations -= sync_iterations\n",
      "  1173                                                                       escape = True; break\n",
      "  1174                                                                   else:\n",
      "  1175                                                                       warnings.warn(\n",
      "  1176                                                                           'Update exceeded KL target too fast! Proceeding with update, but may be unstable. '\n",
      "  1177                                                                           'Try lowering clip or learning rate parameters.')\n",
      "  1178                                                                       escape = True; break\n",
      "  1179                                                                   \n",
      "  1180                                                       # Iterate\n",
      "  1181         4          0.0      0.0      0.0              iterations += 1\n",
      "  1182         4          0.0      0.0      0.0              if iterations >= update_iterations: escape = True\n",
      "  1183                                           \n",
      "  1184                                                       # CLI\n",
      "  1185         4          0.0      0.0      0.0              if verbose and (iterations in (1, 5) or iterations % 10 == 0 or escape):\n",
      "  1186                                                           print(\n",
      "  1187                                                               f'Iteration {iterations:02} - '\n",
      "  1188                                                               + f' + '.join([f'{k} ({np.mean([v.item() for v in vl[-batches:]]):.5f})' for k, vl in total_losses.items()])\n",
      "  1189                                                               + f' :: '\n",
      "  1190                                                               + f', '.join([f'{k} ({np.mean([v for v in vl[-batches:]]):.5f})' for k, vl in total_statistics.items()]))\n",
      "  1191                                           \n",
      "  1192                                                       # Break\n",
      "  1193         4          0.0      0.0      0.0              if escape: break\n",
      "  1194                                           \n",
      "  1195                                                   # Update scheduler\n",
      "  1196         1          0.0      0.0      0.0          self.scheduler.step()\n",
      "  1197                                                   # Update records\n",
      "  1198         1          0.0      0.0      0.0          self.policy_iteration += 1\n",
      "  1199         1          0.0      0.0      0.0          self.copy_policy()\n",
      "  1200                                                   # Return\n",
      "  1201         1          0.0      0.0      0.0          return (\n",
      "  1202         1          0.0      0.0      0.0              iterations,\n",
      "  1203         1          0.0      0.0      0.0              {k: np.mean([v.item() for v in vl]) for k, vl in total_losses.items()},\n",
      "  1204         1          0.0      0.0      0.0              {k: np.mean([v for v in vl]) for k, vl in total_statistics.items()})\n",
      "\n",
      "Total time: 2.101 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: calculate_losses at line 1236\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "  1236                                               def calculate_losses(\n",
      "  1237                                                   self,\n",
      "  1238                                                   states,\n",
      "  1239                                                   actions,\n",
      "  1240                                                   action_logs,\n",
      "  1241                                                   state_vals,\n",
      "  1242                                                   advantages=None,\n",
      "  1243                                                   rewards=None):\n",
      "  1244                                                   # TODO: Maybe implement PFO https://github.com/CLAIRE-Labo/no-representation-no-trust\n",
      "  1245       140          0.0      0.0      0.0          if advantages is not None:\n",
      "  1246                                                       # Get inferred rewards\n",
      "  1247       140          0.0      0.0      0.3              rewards = advantages + state_vals\n",
      "  1248                                                       # print(f'{self.get_policy_iteration()} - {advantages_mean:.3f} - {advantages_std:.3f}')\n",
      "  1249                                                   elif rewards is not None:\n",
      "  1250                                                       # Get advantages\n",
      "  1251                                                       advantages = rewards - state_vals\n",
      "  1252                                                   # Get normalized advantages\n",
      "  1253       140          0.0      0.0      0.7          advantages_mean, advantages_std = advantages.mean(), advantages.std() + 1e-8\n",
      "  1254       140          0.0      0.0      0.3          normalized_advantages = (advantages - advantages_mean) / advantages_std\n",
      "  1255                                                   # Clip advantages for stability\n",
      "  1256                                                   # normalized_advantages =  normalized_advantages.clamp(\n",
      "  1257                                                   #     normalized_advantages.quantile(.05), normalized_advantages.quantile(.95))\n",
      "  1258                                           \n",
      "  1259                                                   # Get normalized returns/rewards (sensitive to minibatch)\n",
      "  1260                                                   # NOTE: Action STD explosion/instability points to a normalization and/or critic fitting issue\n",
      "  1261                                                   #       or, possibly, the returns are too homogeneous - i.e. the problem is solved\n",
      "  1262       140          0.0      0.0      0.4          normalized_rewards = self.return_standardization.apply(rewards)  # , mean=False\n",
      "  1263                                           \n",
      "  1264                                                   # Evaluate actions and states\n",
      "  1265       140          2.0      0.0     93.2          _, action_logs_new, dist_entropy, state_vals_new = self.actor_critic(*states, sample=True, action=actions, entropy=True, critic=True)\n",
      "  1266                                                   # action_logs_new = action_logs_new.clamp(-20, 0)\n",
      "  1267                                                   \n",
      "  1268                                                   # Calculate PPO loss\n",
      "  1269       140          0.0      0.0      0.2          log_ratios = action_logs_new - action_logs\n",
      "  1270                                                   # log_ratios = log_ratios.clamp(-20, 2)\n",
      "  1271       140          0.0      0.0      0.1          ratios = log_ratios.exp()\n",
      "  1272       140          0.0      0.0      0.1          unclipped_ppo = ratios * normalized_advantages\n",
      "  1273       140          0.0      0.0      0.3          clipped_ppo = ratios.clamp(1-self.epsilon_ppo, 1+self.epsilon_ppo) * normalized_advantages\n",
      "  1274       140          0.0      0.0      0.3          loss_ppo = -torch.min(unclipped_ppo, clipped_ppo)\n",
      "  1275                                           \n",
      "  1276                                                   # Calculate KL divergence\n",
      "  1277                                                   # NOTE: A bit odd when it comes to replay\n",
      "  1278                                                   # Discrete\n",
      "  1279                                                   # loss_kl = F.kl_div(action_logs, action_logs_new, reduction='batchmean', log_target=True)\n",
      "  1280                                                   # loss_kl = ((action_logs - action_logs_new)  # * action_logs.exp()).sum(-1)  # Approximation\n",
      "  1281                                                   # Continuous (http://joschu.net/blog/kl-approx.html)\n",
      "  1282       140          0.0      0.0      0.4          loss_kl = (log_ratios.exp() - 1) - log_ratios\n",
      "  1283                                                   # Mask and scale where needed (for replay)\n",
      "  1284                                                   # loss_kl[~new_memories] = 0\n",
      "  1285                                                   # loss_kl = loss_kl * loss_kl.shape[0] / new_memories.sum()\n",
      "  1286                                           \n",
      "  1287                                                   # Calculate critic loss\n",
      "  1288                                                   # unclipped_critic = (state_vals_new - normalized_rewards).square()\n",
      "  1289       140          0.0      0.0      0.0          criteria = F.smooth_l1_loss\n",
      "  1290                                                   # criteria = F.mse_loss\n",
      "  1291       140          0.0      0.0      0.5          unclipped_critic = criteria(state_vals_new, normalized_rewards)\n",
      "  1292       140          0.0      0.0      0.4          clipped_state_vals_new = torch.clamp(state_vals_new, state_vals-self.epsilon_critic, state_vals+self.epsilon_critic)\n",
      "  1293                                                   # clipped_critic = (clipped_state_vals_new - normalized_rewards).square()\n",
      "  1294       140          0.0      0.0      0.2          clipped_critic = criteria(clipped_state_vals_new, normalized_rewards, reduction='none')\n",
      "  1295       140          0.0      0.0      0.2          loss_critic = torch.max(unclipped_critic, clipped_critic)\n",
      "  1296                                                   # if torch.rand(1) < .03:\n",
      "  1297                                                   #     print(\n",
      "  1298                                                   #         f'{state_vals_new.mean().detach().item():.3f}'\n",
      "  1299                                                   #         f' - {rewards.mean().detach().item():.3f}'\n",
      "  1300                                                   #         f' - {normalized_rewards.mean().detach().item():.3f}'\n",
      "  1301                                                   #         f' - {normalized_rewards.std().detach().item():.3f}')\n",
      "  1302                                           \n",
      "  1303                                                   # Calculate explained variance\n",
      "  1304       140          0.0      0.0      1.0          exp_var = (1- (normalized_rewards-state_vals_new).var() / normalized_rewards.var()).clamp(min=-1)\n",
      "  1305                                           \n",
      "  1306                                                   # Calculate entropy bonus\n",
      "  1307                                                   # NOTE: Not included in training grad if action_std is constant\n",
      "  1308                                                   # dist_entropy = -action_logs_new  # Approximation\n",
      "  1309       140          0.0      0.0      0.1          loss_entropy = -dist_entropy\n",
      "  1310                                           \n",
      "  1311                                                   # Construct final loss\n",
      "  1312       140          0.0      0.0      0.0          loss = (\n",
      "  1313       560          0.0      0.0      0.4              loss_ppo\n",
      "  1314       140          0.0      0.0      0.2              + self.critic_weight * loss_critic\n",
      "  1315       140          0.0      0.0      0.1              + self.entropy_weight * loss_entropy\n",
      "  1316       140          0.0      0.0      0.2              + self.kl_beta * loss_kl)\n",
      "  1317       140          0.0      0.0      0.2          loss = loss.mean()\n",
      "  1318                                           \n",
      "  1319       140          0.0      0.0      0.0          return (loss, loss_ppo, loss_critic, loss_entropy, loss_kl), (exp_var,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update\n",
    "import line_profiler\n",
    "prof = line_profiler.LineProfiler(\n",
    "    # memory.fast_sample, policy.actor_critic.forward,\n",
    "    celltrip.policy.ResidualAttentionBlock.forward,\n",
    "    policy.calculate_losses, policy.update)\n",
    "ret = prof.runcall(policy.update, memory, verbose=False)\n",
    "print(', '.join([f'{k}: {v:.3f}' for k, v in ret[1].items()]))\n",
    "prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     # Forward\n",
    "#     import line_profiler\n",
    "#     memory.mark_sampled()\n",
    "#     memory.cleanup()\n",
    "#     prof = line_profiler.LineProfiler(\n",
    "#         celltrip.train.simulate_until_completion,\n",
    "#         celltrip.policy.PPO.forward, celltrip.policy.EntitySelfAttentionLite.forward, celltrip.policy.ResidualAttention.forward,\n",
    "#         celltrip.environment.EnvironmentBase.step)\n",
    "#     ret = prof.runcall(celltrip.train.simulate_until_completion, env, policy, memory, max_memories=policy.epoch_size, reset_on_finish=True)\n",
    "#     print(f'total: {ret[2]:.3f}, ' + ', '.join([f'{k}: {v:.3f}' for k, v in ret[3].items()]))\n",
    "#     # memory.feed_new(policy.reward_standardization)\n",
    "#     memory.compute_advantages()  # moving_standardization=policy.reward_standardization\n",
    "#     # prof.print_stats(output_unit=1)\n",
    "\n",
    "#     # # Pull from memory\n",
    "#     # import line_profiler\n",
    "#     # prof = line_profiler.LineProfiler(\n",
    "#     #     celltrip.memory.AdvancedMemoryBuffer.fast_sample,\n",
    "#     #     celltrip.memory.AdvancedMemoryBuffer._concat_states)\n",
    "#     # ret = prof.runcall(memory.fast_sample, 512, shuffle=False, max_samples_per_state=np.inf)\n",
    "#     # # prof.print_stats(output_unit=1)\n",
    "\n",
    "#     # Update\n",
    "#     import line_profiler\n",
    "#     prof = line_profiler.LineProfiler(policy.update, memory.fast_sample, celltrip.utility.processing.split_state)\n",
    "#     ret = prof.runcall(policy.update, memory, verbose=False)\n",
    "#     # print(', '.join([f'{k}: {v:.3f}' for k, v in ret[1].items()]))\n",
    "#     # prof.print_stats(output_unit=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
