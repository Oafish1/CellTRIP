{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train/test\n",
    "- Save preprocessing\n",
    "- Early stopping for `train_celltrip` based on action_std and/or KL\n",
    "- Maybe [this](https://arxiv.org/abs/2102.09430) but probably not\n",
    "- [EFS on clusters maybe](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/aws.html#start-ray-with-the-ray-cluster-launcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cython is not active\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import ray\n",
    "\n",
    "import celltrip\n",
    "\n",
    "# Detect Cython\n",
    "CYTHON_ACTIVE = os.path.splitext(celltrip.utility.general.__file__)[1] in ('.c', '.so')\n",
    "print(f'Cython is{\" not\" if not CYTHON_ACTIVE else \"\"} active')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train.py s3://nkalafut-celltrip/MERFISH/expression.h5ad s3://nkalafut-celltrip/MERFISH/spatial.h5ad --backed --target_modalities 1 --dim 3 --train_split .8 --num_gpus 3 --num_learners 3 --num_runners 3 --update_timesteps 100_000 --max_timesteps 100_000_000 --dont_sync_across_nodes --logfile s3://nkalafut-celltrip/logs/Partial-L2-VelLinear-ClampLog.log --checkpoint_iterations 100 --checkpoint_dir s3://nkalafut-celltrip/checkpoints --checkpoint_name Partial-L2-VelLinear-ClampLog\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "# NOTE: It is not recommended to use s3 with credentials unless the creds are permanent, the bucket is public, or this is run on AWS\n",
    "parser = argparse.ArgumentParser(description='Train CellTRIP model', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "# Reading\n",
    "group = parser.add_argument_group('Input')\n",
    "group.add_argument('input_files', type=str, nargs='*', help='h5ad files to be used for input')\n",
    "group.add_argument('--merge_files', type=str, action='append', nargs='+', help='h5ad files to merge as input')\n",
    "group.add_argument('--partition_cols', type=str, action='append', nargs='+', help='Columns for data partitioning, found in `adata.obs` DataFrame')\n",
    "group.add_argument('--backed', action='store_true', help='Read data directly from disk or s3, saving memory at the cost of time')\n",
    "group.add_argument('--input_modalities', type=int, nargs='+', help='Input modalities to give to CellTRIP')\n",
    "group.add_argument('--target_modalities', type=int, nargs='+', help='Target modalities to emulate, dictates environment reward')\n",
    "# Algorithm\n",
    "group = parser.add_argument_group('Algorithm')\n",
    "group.add_argument('--dim', type=int, default=16, help='Dimensions in the output latent space')\n",
    "group.add_argument('--train_split', type=float, default=1., help='Fraction of input data to use as training')\n",
    "# Computation\n",
    "group = parser.add_argument_group('Computation')\n",
    "group.add_argument('--num_gpus', type=int, default=1, help='Number of GPUs to use during computation')\n",
    "group.add_argument('--num_learners', type=int, default=1, help='Number of learners used in backward computation, cannot exceed GPUs')\n",
    "group.add_argument('--num_runners', type=int, default=1, help='Number of workers for environment simulation')\n",
    "# Training\n",
    "group = parser.add_argument_group('Training')\n",
    "group.add_argument('--update_timesteps', type=int, default=int(1e6), help='Number of timesteps recorded before each update')\n",
    "group.add_argument('--max_timesteps', type=int, default=int(2e9), help='Maximum number of timesteps to compute before exiting')\n",
    "group.add_argument('--dont_sync_across_nodes', action='store_true', help='Avoid memory sync across nodes, saving overhead time at the cost of stability')\n",
    "# File saves\n",
    "group = parser.add_argument_group('Logging')\n",
    "group.add_argument('--logfile', type=str, default='cli', help='Location for log file, can be `cli`, `<local_file>`, or `<s3 location>`')\n",
    "group.add_argument('--flush_iterations', type=int, help='Number of iterations to wait before flushing logs')\n",
    "group.add_argument('--checkpoint', type=str, help='Checkpoint to use for initializing model')\n",
    "group.add_argument('--checkpoint_iterations', type=int, default=50, help='Number of updates to wait before recording checkpoints')\n",
    "group.add_argument('--checkpoint_dir', type=str, default='./checkpoints', help='Directory for checkpoints')\n",
    "group.add_argument('--checkpoint_name', type=str, help='Run name, for checkpointing')\n",
    "\n",
    "# Notebook defaults and script handling\n",
    "if not celltrip.utility.notebook.is_notebook():\n",
    "    # ray job submit -- python train.py...\n",
    "    config = parser.parse_args()\n",
    "else:\n",
    "    experiment_name = 'Partial-L2-VelLinear-ClampLog'\n",
    "    command = (\n",
    "        f's3://nkalafut-celltrip/MERFISH/expression.h5ad s3://nkalafut-celltrip/MERFISH/spatial.h5ad '\n",
    "        # f'/home/nck/repos/INEPT/data/MERFISH/expression.h5ad /home/nck/repos/INEPT/data/MERFISH/spatial.h5ad '\n",
    "        f'--backed '\n",
    "        f'--target_modalities 1 '\n",
    "        f'--dim 3 '\n",
    "        f'--train_split .8 '\n",
    "        f'--num_gpus 3 --num_learners 3 --num_runners 3 '\n",
    "        f'--update_timesteps 100_000 '\n",
    "        f'--max_timesteps 100_000_000 '\n",
    "        f'--dont_sync_across_nodes '\n",
    "        f'--logfile s3://nkalafut-celltrip/logs/{experiment_name}.log '\n",
    "        # f'--checkpoint s3://nkalafut-celltrip/checkpoints/Partial-L2-VelLinear-0600.weights '\n",
    "        f'--checkpoint_iterations 100 '\n",
    "        f'--checkpoint_dir s3://nkalafut-celltrip/checkpoints '\n",
    "        f'--checkpoint_name {experiment_name}')\n",
    "    config = parser.parse_args(command.split(' '))\n",
    "    print(f'python train.py {command}')\n",
    "    \n",
    "# Defaults\n",
    "if config.checkpoint_name is None:\n",
    "    config.checkpoint_name = f'RUN_{random.randint(0, 2**32):0>10}'\n",
    "    print(f'Run Name: {config.checkpoint_name}')\n",
    "# print(config)  # CLI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start Ray\n",
    "# ray.shutdown()\n",
    "# a = ray.init(\n",
    "#     # address='ray://100.85.187.118:10001',\n",
    "#     address='ray://localhost:10001',\n",
    "#     runtime_env={\n",
    "#         'py_modules': [celltrip],\n",
    "#         'pip': '../requirements.txt',\n",
    "#         'env_vars': {\n",
    "#             # **access_keys,\n",
    "#             'RAY_DEDUP_LOGS': '0'}},\n",
    "#         # 'NCCL_SOCKET_IFNAME': 'tailscale',  # lo,en,wls,docker,tailscale\n",
    "#     _system_config={'enable_worker_prestart': True})  # Doesn't really work for scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ray.remote(num_cpus=1e-4)\n",
    "# def train(config):\n",
    "#     import celltrip\n",
    "\n",
    "#     # Initialization\n",
    "#     dataloader_kwargs = {'mask': config.train_split}  # {'num_nodes': 20, 'pca_dim': 128}\n",
    "#     environment_kwargs = {\n",
    "#         'input_modalities': config.input_modalities,\n",
    "#         'target_modalities': config.target_modalities, 'dim': config.dim}\n",
    "#     initializers = celltrip.train.get_initializers(\n",
    "#         input_files=config.input_files, merge_files=config.merge_files,\n",
    "#         backed=config.backed, dataloader_kwargs=dataloader_kwargs,\n",
    "#         environment_kwargs=environment_kwargs)\n",
    "\n",
    "#     stage_functions = [\n",
    "#         # lambda w: w.env.set_rewards(penalty_velocity=1, penalty_action=1),\n",
    "#         # lambda w: w.env.set_rewards(reward_origin=1),\n",
    "#         # lambda w: w.env.set_rewards(reward_origin=0, reward_distance=1),\n",
    "#         # lambda w: w.env.dataloader.preprocessing.set_num_nodes(500),\n",
    "#         # lambda w: w.env.dataloader.preprocessing.set_num_nodes(1000),\n",
    "#         # lambda w: w.env.dataloader.preprocessing.set_num_nodes(2000),\n",
    "#         # lambda w: w.env.dataloader.preprocessing.set_num_nodes(3000),\n",
    "#     ]\n",
    "\n",
    "#     # Run function\n",
    "#     celltrip.train.train_celltrip(\n",
    "#         initializers=initializers,\n",
    "#         num_gpus=config.num_gpus, num_learners=config.num_learners,\n",
    "#         num_runners=config.num_runners, max_timesteps=config.max_timesteps,\n",
    "#         update_timesteps=config.update_timesteps, sync_across_nodes=not config.dont_sync_across_nodes,\n",
    "#         checkpoint_iterations=config.checkpoint_iterations, checkpoint_dir=config.checkpoint_dir,\n",
    "#         checkpoint=config.checkpoint, checkpoint_name=config.checkpoint_name,\n",
    "#         stage_functions=stage_functions, logfile=config.logfile)\n",
    "\n",
    "# ray.get(train.remote(config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get AWS keys\n",
    "# import boto3\n",
    "# os.environ['AWS_PROFILE'] = 'waisman-admin'\n",
    "# session = boto3.Session()\n",
    "# creds = session.get_credentials()\n",
    "# access_keys = {\n",
    "#     'AWS_ACCESS_KEY_ID': creds.access_key,\n",
    "#     'AWS_SECRET_ACCESS_KEY': creds.secret_key,\n",
    "#     'AWS_DEFAULT_REGION': 'us-east-2'}\n",
    "\n",
    "# # Check s3\n",
    "# import os\n",
    "# import s3fs\n",
    "# os.environ['AWS_PROFILE'] = 'waisman-admin'\n",
    "# s3 = s3fs.S3FileSystem(skip_instance_cache=True)\n",
    "# s3.ls('s3://nkalafut-celltrip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize locally\n",
    "os.environ['AWS_PROFILE'] = 'waisman-admin'\n",
    "\n",
    "dataloader_kwargs = {'mask': config.train_split}  # {'num_nodes': 20, 'pca_dim': 128}\n",
    "environment_kwargs = {\n",
    "    'input_modalities': config.input_modalities,\n",
    "    'target_modalities': config.target_modalities, 'dim': config.dim}\n",
    "env_init, policy_init, memory_init = celltrip.train.get_initializers(\n",
    "    input_files=config.input_files, merge_files=config.merge_files,\n",
    "    backed=config.backed, dataloader_kwargs=dataloader_kwargs,   # policy_kwargs={'minibatch_size': 2_000},\n",
    "    environment_kwargs=environment_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/repos/inept/celltrip/utility/processing.py:109: RuntimeWarning: Modality 0 too small for PCA (253 features), skipping\n",
      "  warnings.warn(\n",
      "/home/thema/repos/inept/celltrip/utility/processing.py:109: RuntimeWarning: Modality 1 too small for PCA (2 features), skipping\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "try: env\n",
    "except: env = env_init().to('cuda')\n",
    "# policy.split_args['max_nodes'] = 2000\n",
    "# policy.forward_batch_size = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = policy_init(env).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = memory_init(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance: -2.123, origin: 0.000, bound: -75.376, velocity: 0.092, action: -2.611\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "import line_profiler\n",
    "memory.mark_sampled()\n",
    "prof = line_profiler.LineProfiler(\n",
    "    celltrip.train.simulate_until_completion,\n",
    "    celltrip.policy.PPO.forward, celltrip.policy.EntitySelfAttentionLite.forward, celltrip.policy.ResidualAttentionBlock.forward,\n",
    "    celltrip.environment.EnvironmentBase.step)\n",
    "ret = prof.runcall(celltrip.train.simulate_until_completion, env, policy, memory, max_timesteps=500, reset_on_finish=True)\n",
    "print(', '.join([f'{k}: {v:.3f}' for k, v in ret[3].items()]))\n",
    "memory.compute_advantages()\n",
    "# prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st = memory.fast_sample(10_000)['states']\n",
    "\n",
    "# def subsample_from_batch_batch(state, idx):\n",
    "#     self_, node_, mask_ = state\n",
    "#     self_\n",
    "\n",
    "# subsample_from_batch_batch(st, np.random.choice(10_000, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pull from memory\n",
    "# import line_profiler\n",
    "# prof = line_profiler.LineProfiler(\n",
    "#     celltrip.memory.AdvancedMemoryBuffer.fast_sample,\n",
    "#     celltrip.memory.AdvancedMemoryBuffer._concat_states)\n",
    "# ret = prof.runcall(memory.fast_sample, 512, shuffle=False, max_samples_per_state=np.inf)\n",
    "# # prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.minibatch_size = 25_000\n",
    "# policy.update_iterations = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/repos/inept/celltrip/policy.py:1119: UserWarning: No group \"learners\" found.\n",
      "  warnings.warn(f'No group \"{group}\" found.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 01 - Total (1.78090) + PPO (-0.02858) + critic (3.70410) + entropy (-4.25694) + KL (-0.00000) :: Advantage Mean (-2.89255), Advantage STD (3.15155), Log STD (0.00000), Explained Variance (0.12372)\n",
      "Iteration 05 - Total (1.17400) + PPO (-0.03046) + critic (2.49411) + entropy (-4.26021) + KL (0.01226) :: Advantage Mean (-2.88816), Advantage STD (3.15203), Log STD (0.00109), Explained Variance (0.30543)\n",
      "Total: 1.331, PPO: -0.029, critic: 2.805, entropy: -4.259, KL: 0.011\n",
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 1.25381 s\n",
      "File: /home/thema/repos/inept/celltrip/memory.py\n",
      "Function: fast_sample at line 180\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   180                                               def fast_sample(\n",
      "   181                                                   self, num_memories, replay_frac=None, max_samples_per_state=None,\n",
      "   182                                                   uniform=None, shuffle=None, efficient=True, round_sample=None):\n",
      "   183                                                   # NOTE: Shuffle should only be used when sequential sampling is taking place\n",
      "   184                                                   # Parameters\n",
      "   185        20          0.0      0.0      0.0          if replay_frac is None: replay_frac = self.replay_frac\n",
      "   186        20          0.0      0.0      0.0          if max_samples_per_state is None: max_samples_per_state = self.max_samples_per_state\n",
      "   187        20          0.0      0.0      0.0          if uniform is None: uniform = self.uniform\n",
      "   188        20          0.0      0.0      0.0          if shuffle is None: shuffle = self.shuffle\n",
      "   189        20          0.0      0.0      0.0          num_replay_memories = int(replay_frac * num_memories)\n",
      "   190        20          0.0      0.0      0.0          num_new_memories = num_memories - num_replay_memories\n",
      "   191                                           \n",
      "   192                                                   # Adjust proportions if needed\n",
      "   193        20          0.0      0.0      0.1          total_new_memories = self.get_new_len(max_per_step=max_samples_per_state)\n",
      "   194        20          0.0      0.0      0.0          total_replay_memories = self.get_replay_len(max_per_step=max_samples_per_state)\n",
      "   195        20          0.0      0.0      0.0          if total_new_memories+total_replay_memories < num_memories:\n",
      "   196                                                       raise RuntimeError(\n",
      "   197                                                           f'Only {total_new_memories+total_replay_memories} possible samples'\n",
      "   198                                                           f' with current parameters, {num_memories} requested')\n",
      "   199        20          0.0      0.0      0.0          adjusted = False\n",
      "   200        20          0.0      0.0      0.0          if total_new_memories < num_new_memories:\n",
      "   201                                                       num_replay_memories += num_new_memories - total_new_memories\n",
      "   202                                                       num_new_memories = total_new_memories\n",
      "   203                                                       adjusted = True\n",
      "   204        20          0.0      0.0      0.0          elif total_replay_memories < num_replay_memories:\n",
      "   205                                                       num_new_memories += num_replay_memories - total_replay_memories\n",
      "   206                                                       num_replay_memories = total_replay_memories\n",
      "   207                                                       adjusted = True\n",
      "   208        20          0.0      0.0      0.0          if adjusted:\n",
      "   209                                                       new_replay_frac = num_replay_memories / (num_replay_memories+num_new_memories)\n",
      "   210                                                       warnings.warn(\n",
      "   211                                                           f'Current `replay_frac` ({replay_frac:.3f}) infeasible,'\n",
      "   212                                                           f' adjusting to {new_replay_frac:.3f}')\n",
      "   213                                           \n",
      "   214                                                   # Initialization\n",
      "   215        20          0.0      0.0      0.0          ret = defaultdict(lambda: [])\n",
      "   216                                                   # memory_indices = []\n",
      "   217                                           \n",
      "   218                                                   # List order\n",
      "   219        20          0.0      0.0      0.0          list_order = np.arange(len(self.storage['keys']))\n",
      "   220        20          0.0      0.0      0.0          if not uniform:\n",
      "   221                                                       # Get random list order\n",
      "   222        20          0.0      0.0      0.0              np.random.shuffle(list_order)\n",
      "   223                                                   else:\n",
      "   224                                                       # Uniform sampling (could also work with duplicates)\n",
      "   225                                                       new_memories_to_record = np.random.choice(self.get_new_len(), num_new_memories, replace=False)\n",
      "   226                                                       replay_memories_to_record = np.random.choice(self.get_replay_len(), num_replay_memories, replace=False)\n",
      "   227                                           \n",
      "   228                                                   # Search for index\n",
      "   229        20          0.0      0.0      0.0          num_replay_memories_recorded, num_new_memories_recorded = np.array(0), np.array(0)\n",
      "   230       300          0.0      0.0      0.0          for list_num in list_order:\n",
      "   231                                                       # Check if should sample\n",
      "   232       300          0.0      0.0      0.0              if self.storage['staleness'][list_num] == 0:\n",
      "   233       300          0.0      0.0      0.0                  working_memories_recorded = num_new_memories_recorded\n",
      "   234       300          0.0      0.0      0.0                  working_memories = num_new_memories\n",
      "   235       300          0.0      0.0      0.0                  if uniform: working_memories_to_record = new_memories_to_record\n",
      "   236                                                       else:\n",
      "   237                                                           working_memories_recorded = num_replay_memories_recorded\n",
      "   238                                                           working_memories = num_replay_memories\n",
      "   239                                                           if uniform: working_memories_to_record = replay_memories_to_record\n",
      "   240       300          0.0      0.0      0.1              if working_memories_recorded >= working_memories: continue\n",
      "   241                                           \n",
      "   242                                                       # Choose random samples\n",
      "   243       300          0.0      0.0      0.0              list_len = len(self.storage['keys'][list_num])\n",
      "   244       300          0.0      0.0      0.0              if not uniform:\n",
      "   245                                                           # Greedily add\n",
      "   246       300          0.0      0.0      0.0                  num_memories_to_add = min(list_len, max_samples_per_state)\n",
      "   247       300          0.0      0.0      0.0                  if working_memories_recorded + num_memories_to_add > working_memories and round_sample != 'up':\n",
      "   248        20          0.0      0.0      0.0                      if round_sample is None:\n",
      "   249        20          0.0      0.0      0.0                          num_memories_to_add = working_memories-working_memories_recorded\n",
      "   250                                                               elif round_sample == 'down': break\n",
      "   251                                                               else: raise RuntimeError(f'`round_sample` method `{round_sample}` not implemented.')\n",
      "   252       300          0.0      0.0      0.0                  if list_len != num_memories_to_add:\n",
      "   253        20          0.0      0.0      0.0                      if round_sample is None:\n",
      "   254        20          0.0      0.0      0.1                          self_idx = np.random.choice(list_len, num_memories_to_add, replace=False)\n",
      "   255       280          0.0      0.0      0.1                  else: self_idx = np.arange(list_len)\n",
      "   256                                                       else:\n",
      "   257                                                           # Uniformly add\n",
      "   258                                                           mask = working_memories_to_record < list_len\n",
      "   259                                                           num_memories_to_add = mask.sum()\n",
      "   260                                                           if num_memories_to_add == 0:\n",
      "   261                                                               working_memories_to_record -= list_len\n",
      "   262                                                               continue\n",
      "   263                                                           self_idx = working_memories_to_record[mask].copy()\n",
      "   264                                                           working_memories_to_record[mask] = len(self)  # Kinda hacky, but works\n",
      "   265                                                           working_memories_to_record -= list_len\n",
      "   266                                           \n",
      "   267                                                       # Get values\n",
      "   268      4200          0.0      0.0      0.1              for k in self.storage:\n",
      "   269                                                           # Skip certain keys\n",
      "   270      3900          0.0      0.0      0.1                  if k not in ('states', 'actions', 'action_logs', 'state_vals', 'advantages', 'propagated_rewards', 'staleness'): continue\n",
      "   271                                           \n",
      "   272                                                           # Special cases\n",
      "   273      2100          0.0      0.0      0.0                  if k == 'states':\n",
      "   274      1200          0.1      0.0      4.8                      val = _utility.processing.split_state(\n",
      "   275       600          0.1      0.0      6.5                          self._append_suffix(\n",
      "   276       300          0.0      0.0      0.0                              self.storage[k][list_num],\n",
      "   277       300          0.0      0.0      0.0                              keys=self.storage['keys'][list_num]),\n",
      "   278       300          0.0      0.0      0.0                          idx=self_idx,\n",
      "   279       300          0.0      0.0      0.0                          **self.split_args)\n",
      "   280                                                               \n",
      "   281                                                           # Single value case\n",
      "   282      1800          0.0      0.0      0.0                  elif k in ('staleness',):\n",
      "   283       300          0.1      0.0      5.1                      val = torch.tensor(len(self_idx)*[self.storage[k][list_num]], device=self.device)\n",
      "   284                                           \n",
      "   285                                                           # Main case\n",
      "   286                                                           else: \n",
      "   287      1500          0.0      0.0      0.0                      if k in ('propagated_rewards', 'normalized_rewards') and self.storage[k][list_num] is None:\n",
      "   288                                                                   raise ValueError('Make sure to run `self.propagate_rewards(); self.normalize_rewards()` before sampling.')\n",
      "   289      1500          0.0      0.0      0.0                      if k in ('advantages',) and self.storage[k][list_num] is None:\n",
      "   290                                                                   raise ValueError('Make sure to run `self.compute_advantages()` before sampling.')\n",
      "   291      1500          0.0      0.0      0.7                      if (len(self_idx) == self.storage[k][list_num].shape[0]) and (self_idx[:-1] < self_idx[1:]).all():\n",
      "   292      1400          0.0      0.0      0.0                          val = self.storage[k][list_num]\n",
      "   293       100          0.0      0.0      0.2                      else: val = self.storage[k][list_num][self_idx]\n",
      "   294                                           \n",
      "   295                                                           # Record\n",
      "   296      2100          0.0      0.0      0.1                  ret[k].append(val)\n",
      "   297                                           \n",
      "   298                                                       # Record memory indices and iterate\n",
      "   299                                                       # memory_indices += [(list_num, i) for i in self_idx]\n",
      "   300       300          0.0      0.0      0.0              if self.storage['staleness'][list_num] == 0:\n",
      "   301       300          0.0      0.0      0.2                  num_new_memories_recorded += num_memories_to_add\n",
      "   302                                                       else:\n",
      "   303                                                           num_replay_memories_recorded += num_memories_to_add\n",
      "   304                                           \n",
      "   305                                                       # Break if enough memories retrieved\n",
      "   306       300          0.0      0.0      0.1              if num_replay_memories_recorded + num_new_memories_recorded >= num_memories: break\n",
      "   307                                           \n",
      "   308                                                   # Catch if too few\n",
      "   309                                                   else: warnings.warn(\n",
      "   310                                                       f'Only able to gather {num_replay_memories_recorded + num_new_memories_recorded}'\n",
      "   311                                                       f' memories with current parameters, {num_memories} requested.')\n",
      "   312                                           \n",
      "   313                                                   # Stack tensors\n",
      "   314       160          0.0      0.0      0.0          for k in ret:\n",
      "   315       140          1.0      0.0     81.0              if k == 'states': ret[k] = self._concat_states(ret[k], efficient=efficient)\n",
      "   316       120          0.0      0.0      0.4              else: ret[k] = torch.concat(ret[k], dim=0)\n",
      "   317                                                   # memory_indices = torch.tensor(self._index_to_flat_index(memory_indices))\n",
      "   318                                           \n",
      "   319                                                   # Shuffle\n",
      "   320                                                   # NOTE: Not reproducible currently, takes maybe .1 seconds for 10k but\n",
      "   321                                                   #       is roughly Omega(N^1.1)\n",
      "   322        20          0.0      0.0      0.0          if shuffle:\n",
      "   323                                                       perm = torch.randperm(num_memories)\n",
      "   324                                                       for k in ret:\n",
      "   325                                                           if k == 'states': ret[k] = [s[perm] for s in ret[k]]\n",
      "   326                                                           else: ret[k] = ret[k][perm]\n",
      "   327                                                       # memory_indices = memory_indices[perm]\n",
      "   328                                           \n",
      "   329                                                   # Return\n",
      "   330        20          0.0      0.0      0.0          return dict(ret)  # memory_indices\n",
      "\n",
      "Total time: 1.01303 s\n",
      "File: /home/thema/repos/inept/celltrip/memory.py\n",
      "Function: _concat_states at line 575\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   575                                               def _concat_states(self, states, efficient=False):\n",
      "   576                                                   # Pad with duplicate nodes when not sufficient\n",
      "   577                                                   # NOTE: Inefficient, nested tensor doesn't have enough\n",
      "   578                                                   # functionality yet\n",
      "   579        20          0.0      0.0      0.0          if len(states[0]) == 2:\n",
      "   580                                                       # Regular case\n",
      "   581                                                       shapes = [s[1].shape[1] for s in states]\n",
      "   582                                                       max_nodes = max(shapes)\n",
      "   583                                                       states = [\n",
      "   584                                                           torch.concat([\n",
      "   585                                                               s[i]\n",
      "   586                                                               if i == 0 or np.ceil(max_nodes/s[i].shape[1]) == 1 else\n",
      "   587                                                               s[i].repeat(  # TODO: Maybe do NAN instead? Make sure policy can handle it\n",
      "   588                                                                   1, int(np.ceil(max_nodes/s[i].shape[1])), 1)[:, :max_nodes]\n",
      "   589                                                               for s in states],\n",
      "   590                                                           dim=0) for i in range(2)]\n",
      "   591                                                   # elif (np.array([len(s) for s in states]) == 1).all():\n",
      "   592        20          0.0      0.0      0.0          elif efficient:\n",
      "   593                                                       # Lite case (more memory and compute efficient)\n",
      "   594                                                       # NOTE: Shuffle currently incompatible\n",
      "   595                                                       # NOTE: Screws with policy indexing, need to use batch with no minibatches \n",
      "   596                                                       # TODO: Implement for else case, with more uneven node dims\n",
      "   597        20          0.3      0.0     33.6              states = [(s[0], s[0], torch.eye(s[0].shape[0], device=self.device)) if len(s) == 1 else s for s in states]\n",
      "   598        20          0.0      0.0      0.0              max_self_shape = max(s[0].shape[0] for s in states)\n",
      "   599        20          0.0      0.0      0.0              max_node_shape = max(s[1].shape[0] for s in states)\n",
      "   600        20          0.0      0.0      4.5              s0 = torch.zeros((len(states), max_self_shape, states[0][0].shape[-1]), device=self.device)\n",
      "   601       320          0.1      0.0      8.9              for i, s in enumerate(states): s = s[0]; s0[i, :s.shape[0], :s.shape[1]] = s\n",
      "   602        20          0.0      0.0      4.2              s1 = torch.zeros((len(states), max_node_shape, states[0][1].shape[-1]), device=self.device)\n",
      "   603       320          0.1      0.0      8.9              for i, s in enumerate(states): s = s[1]; s1[i, :s.shape[0], :s.shape[1]] = s\n",
      "   604        20          0.1      0.0     12.2              s2 = torch.ones((len(states), max_self_shape, max_node_shape), dtype=torch.bool, device=self.device)\n",
      "   605       320          0.3      0.0     27.7              for i, s in enumerate(states): s = s[2]; s2[i, :s.shape[0], :s.shape[1]] = s\n",
      "   606        20          0.0      0.0      0.0              states = [s0, s1, s2]\n",
      "   607                                                   else:\n",
      "   608                                                       # Lite case (not memory or compute efficient, but closer to previous format for non-lite)\n",
      "   609                                                       # Unfold to all 3 representations\n",
      "   610                                                       states = [(s[0], s[0], torch.eye(s[0].shape[0], device=self.device)) if len(s) == 1 else s for s in states]\n",
      "   611                                                       # Shape, Pad, and concat\n",
      "   612                                                       states = [(se.unsqueeze(1), no.expand(se.shape[0], *no.shape), ma.unsqueeze(1)) for se, no, ma in states]\n",
      "   613                                                       batch_num = sum(s[0].shape[0] for s in states)\n",
      "   614                                                       max_node_shape = max(s[1].shape[1] for s in states)\n",
      "   615                                                       max_mask_shape = max(s[2].shape[2] for s in states)\n",
      "   616                                                       s0 = torch.concat([s[0] for s in states], dim=0)\n",
      "   617                                                       # torch.concat([F.pad(s[1], (0, 0, 0, max_node_shape-s[1].shape[1]), value=0) for s in states], dim=0),  # Wanted to pad with nan, but nan*0=nan\n",
      "   618                                                       s1 = torch.zeros((batch_num, max_node_shape, states[0][1].shape[-1]), device=self.device)\n",
      "   619                                                       for i, s in enumerate(states):\n",
      "   620                                                           s = s[1]\n",
      "   621                                                           s1[i*s.shape[0]:(i+1)*s.shape[0], :s.shape[1], :s.shape[2]] = s[0]\n",
      "   622                                                       s2 = torch.zeros((batch_num, 1, max_mask_shape), device=self.device)\n",
      "   623                                                       # torch.concat([F.pad(s[2], (0, max_mask_shape-s[2].shape[2], 0, 0), value=True) for s in states], dim=0)\n",
      "   624                                                       for i, s in enumerate(states):\n",
      "   625                                                           s = s[2]\n",
      "   626                                                           s2[i*s.shape[0]:(i+1)*s.shape[0], :, :s.shape[2]] = s[0]\n",
      "   627                                                       states = [s0, s1, s2]\n",
      "   628                                           \n",
      "   629        20          0.0      0.0      0.0          return states\n",
      "\n",
      "Total time: 6.32858 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: update at line 916\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   916                                               def update(\n",
      "   917                                                   self,\n",
      "   918                                                   memory,\n",
      "   919                                                   update_iterations=None,\n",
      "   920                                                   # standardize_returns=False,  # Generally explodes with GAE\n",
      "   921                                                   verbose=False,\n",
      "   922                                                   # Collective args\n",
      "   923                                                   sync_iterations=None,\n",
      "   924                                                   **kwargs,\n",
      "   925                                               ):\n",
      "   926                                                   # NOTE: The number of epochs is spread across `world_size` workers\n",
      "   927                                                   # NOTE: Assumes col.init_collective_group has already been called if world_size > 1\n",
      "   928                                                   # Parameters\n",
      "   929         1          0.0      0.0      0.0          if update_iterations is None: update_iterations = self.update_iterations\n",
      "   930         1          0.0      0.0      0.0          if sync_iterations is None: sync_iterations = self.sync_iterations\n",
      "   931                                           \n",
      "   932                                                   # Collective operations\n",
      "   933         1          0.0      0.0      0.0          use_collective = col.is_group_initialized('default')\n",
      "   934                                           \n",
      "   935                                                   # Batch parameters\n",
      "   936         1          0.0      0.0      0.0          level_dict = {'pool': 0, 'epoch': 1, 'batch': 2, 'minibatch': 3}\n",
      "   937         1          0.0      0.0      0.0          load_level = level_dict[self.load_level]\n",
      "   938         1          0.0      0.0      0.0          cast_level = level_dict[self.cast_level]\n",
      "   939         1          0.0      0.0      0.0          assert cast_level >= load_level, 'Cannot cast without first loading'\n",
      "   940                                           \n",
      "   941                                                   # Determine level sizes\n",
      "   942         1          0.0      0.0      0.0          denominator = self.get_world_size('learners') if sync_iterations == 1 else 1  # Adjust sizes if gradients synchronized across GPUs\n",
      "   943         1          0.0      0.0      0.0          memory_size = len(memory)\n",
      "   944         1          0.0      0.0      0.0          pool_size = self.pool_size\n",
      "   945         1          0.0      0.0      0.0          if pool_size is not None: pool_size = min(pool_size, memory_size)\n",
      "   946         1          0.0      0.0      0.0          epoch_size = self.epoch_size if not (self.epoch_size is None and pool_size is not None) else pool_size\n",
      "   947         1          0.0      0.0      0.0          if epoch_size is not None and pool_size is not None: epoch_size = int(min(epoch_size, pool_size))\n",
      "   948         1          0.0      0.0      0.0          epoch_size = np.ceil(epoch_size / denominator).astype(int)\n",
      "   949         1          0.0      0.0      0.0          batch_size = self.batch_size if not (self.batch_size is None and epoch_size is not None) else epoch_size\n",
      "   950         1          0.0      0.0      0.0          if batch_size is not None and epoch_size is not None: batch_size = int(min(batch_size, epoch_size))\n",
      "   951         1          0.0      0.0      0.0          batch_size = np.ceil(batch_size / denominator).astype(int)\n",
      "   952         1          0.0      0.0      0.0          minibatch_size = self.minibatch_size if not (self.minibatch_size is None and batch_size is not None) else batch_size\n",
      "   953         1          0.0      0.0      0.0          if minibatch_size is not None and batch_size is not None: minibatch_size = int(min(minibatch_size, batch_size))\n",
      "   954                                                   # print(f'{pool_size} - {epoch_size} - {batch_size} - {minibatch_size}')\n",
      "   955                                           \n",
      "   956                                                   # Cap at max size to reduce redundancy for sequential samples\n",
      "   957                                                   # NOTE: Pool->epoch is the only non-sequential sample, and is thus not included here\n",
      "   958                                                   # max_unique_memories = batch_size * update_iterations\n",
      "   959                                                   # epoch_size = min(epoch_size, max_unique_memories)\n",
      "   960                                           \n",
      "   961                                                   # Update moving return mean\n",
      "   962                                                   # if standardize_returns:\n",
      "   963                                                   #     # TODO: Clean this up if it works\n",
      "   964                                                   #     advantages = torch.concat([memory.storage['advantages'][i] for i in range(memory.get_steps()) if memory.storage['staleness'][i]==0]).to(self.policy_iteration.device)\n",
      "   965                                                   #     state_vals = torch.concat([memory.storage['state_vals'][i] for i in range(memory.get_steps()) if memory.storage['staleness'][i]==0]).to(self.policy_iteration.device)\n",
      "   966                                                   #     self.return_standardization.update(advantages - state_vals)\n",
      "   967                                           \n",
      "   968                                                   # Load pool\n",
      "   969         1          0.0      0.0      0.0          total_losses = defaultdict(lambda: [])\n",
      "   970         1          0.0      0.0      0.0          total_statistics = defaultdict(lambda: [])\n",
      "   971         4          0.0      0.0      0.0          pool_data = _utility.processing.sample_and_cast(\n",
      "   972         1          0.0      0.0      0.0              memory, None, None, pool_size,\n",
      "   973         1          0.0      0.0      0.0              current_level=0, load_level=load_level, cast_level=cast_level,\n",
      "   974         2          0.0      0.0      0.0              device=self.policy_iteration.device, **kwargs)\n",
      "   975                                           \n",
      "   976                                                   # Train\n",
      "   977         1          0.0      0.0      0.0          iterations = 0; synchronized = True; escape = False\n",
      "   978         5          0.0      0.0      0.0          while True:\n",
      "   979                                                       # Load epoch\n",
      "   980        20          0.0      0.0      0.0              epoch_data = _utility.processing.sample_and_cast(\n",
      "   981         5          0.0      0.0      0.0                  memory, pool_data, pool_size, epoch_size,\n",
      "   982         5          0.0      0.0      0.0                  current_level=1, load_level=load_level, cast_level=cast_level,\n",
      "   983        10          0.0      0.0      0.0                  device=self.policy_iteration.device, **kwargs)\n",
      "   984         5          0.0      0.0      0.0              batches = np.ceil(epoch_size/batch_size).astype(int) if epoch_size is not None else 1\n",
      "   985         5          0.0      0.0      0.0              batch_returns = torch.zeros(0, device=self.policy_iteration.device)\n",
      "   986        10          0.0      0.0      0.0              for batch_num in range(batches):\n",
      "   987                                                           # Load batch\n",
      "   988         5          0.0      0.0      0.0                  batch_losses = defaultdict(lambda: 0)\n",
      "   989         5          0.0      0.0      0.0                  batch_statistics = defaultdict(lambda: 0)\n",
      "   990        20          0.0      0.0      0.0                  batch_data = _utility.processing.sample_and_cast(\n",
      "   991         5          0.0      0.0      0.0                      memory, epoch_data, epoch_size, batch_size,\n",
      "   992         5          0.0      0.0      0.0                      current_level=2, load_level=load_level, cast_level=cast_level,\n",
      "   993         5          0.0      0.0      0.0                      device=self.policy_iteration.device, sequential_num=batch_num,\n",
      "   994        10          0.0      0.0      0.0                      clip_sequential=False, **kwargs)\n",
      "   995         5          0.0      0.0      0.0                  minibatches = np.ceil(batch_size/minibatch_size).astype(int) if batch_size is not None else 1\n",
      "   996        25          0.0      0.0      0.0                  for minibatch_num in range(minibatches):\n",
      "   997                                                               # Load minibatch\n",
      "   998        80          1.6      0.0     25.6                      minibatch_data, minibatch_actual_size = _utility.processing.sample_and_cast(\n",
      "   999        20          0.0      0.0      0.0                          memory, batch_data, batch_size, minibatch_size,\n",
      "  1000        20          0.0      0.0      0.0                          current_level=3, load_level=load_level, cast_level=cast_level,\n",
      "  1001        20          0.0      0.0      0.0                          device=self.policy_iteration.device, sequential_num=minibatch_num,\n",
      "  1002        40          0.0      0.0      0.0                          clip_sequential=True, **kwargs)\n",
      "  1003                                           \n",
      "  1004                                                               # Get subset data\n",
      "  1005        20          0.0      0.0      0.0                      states = minibatch_data['states']\n",
      "  1006        20          0.0      0.0      0.0                      actions = minibatch_data['actions']\n",
      "  1007        20          0.0      0.0      0.0                      action_logs = minibatch_data['action_logs']\n",
      "  1008        20          0.0      0.0      0.0                      state_vals = minibatch_data['state_vals']\n",
      "  1009        20          0.0      0.0      0.0                      advantages = minibatch_data['advantages']\n",
      "  1010                                                               # rewards = minibatch_data['propagated_rewards']\n",
      "  1011                                           \n",
      "  1012                                                               # Perform backward\n",
      "  1013        40          2.4      0.1     37.3                      losses, statistics = self.calculate_losses(\n",
      "  1014        20          0.0      0.0      0.0                          states, actions, action_logs, state_vals, advantages=advantages, rewards=None)\n",
      "  1015        20          0.0      0.0      0.0                      loss, loss_ppo, loss_critic, loss_entropy, loss_kl = losses\n",
      "  1016        20          0.0      0.0      0.0                      exp_var, = statistics\n",
      "  1017                                           \n",
      "  1018                                                               # Scale and calculate gradient\n",
      "  1019        20          0.0      0.0      0.0                      accumulation_frac = minibatch_actual_size / batch_size\n",
      "  1020        20          0.0      0.0      0.0                      loss = loss * accumulation_frac\n",
      "  1021        20          2.3      0.1     36.2                      loss.backward()  # Longest computation\n",
      "  1022                                           \n",
      "  1023                                                               # Update moving return mean\n",
      "  1024        20          0.0      0.0      0.0                      batch_returns = torch.cat((batch_returns, (advantages-state_vals).mean(keepdim=True, dim=-1) * accumulation_frac), dim=0)\n",
      "  1025                                           \n",
      "  1026                                                               # Scale and record\n",
      "  1027        20          0.0      0.0      0.0                      batch_losses['Total'] += loss.detach()\n",
      "  1028        20          0.0      0.0      0.0                      batch_losses['PPO'] += loss_ppo.detach().mean() * accumulation_frac\n",
      "  1029        20          0.0      0.0      0.0                      batch_losses['critic'] += loss_critic.detach().mean() * accumulation_frac\n",
      "  1030        20          0.0      0.0      0.0                      batch_losses['entropy'] += loss_entropy.detach().mean() * accumulation_frac\n",
      "  1031        20          0.0      0.0      0.0                      batch_losses['KL'] += loss_kl.detach().mean() * accumulation_frac\n",
      "  1032                                                               # batch_statistics['Advantage Mean'] += self.return_standardization.mean.mean().item() * accumulation_frac\n",
      "  1033                                                               # batch_statistics['Advantage STD'] += self.return_standardization.std.mean().item() * accumulation_frac\n",
      "  1034        20          0.0      0.0      0.0                      batch_statistics['Advantage Mean'] += advantages.detach().mean().item() * accumulation_frac\n",
      "  1035        20          0.0      0.0      0.0                      batch_statistics['Advantage STD'] += advantages.detach().std().item() * accumulation_frac\n",
      "  1036        20          0.0      0.0      0.0                      batch_statistics['Log STD'] += self.get_log_std() * accumulation_frac\n",
      "  1037        20          0.0      0.0      0.0                      batch_statistics['Explained Variance'] += exp_var.detach().item() * accumulation_frac\n",
      "  1038                                           \n",
      "  1039                                                           # Update moving return mean\n",
      "  1040                                                           # if standardize_returns: self.return_standardization.update(batch_returns)\n",
      "  1041                                                           \n",
      "  1042                                                           # Record\n",
      "  1043        30          0.0      0.0      0.0                  for k, v in batch_losses.items(): total_losses[k].append(v)\n",
      "  1044        25          0.0      0.0      0.0                  for k, v in batch_statistics.items(): total_statistics[k].append(v)\n",
      "  1045                                           \n",
      "  1046                                                           # Synchronize GPU policies and step\n",
      "  1047                                                           # NOTE: Synchronize gradients every batch if =1, else synchronize whole model\n",
      "  1048                                                           # NOTE: =1 keeps optimizers in sync without need for whole-model synchronization\n",
      "  1049         5          0.0      0.0      0.0                  if sync_iterations == 1: self.synchronize('learners', grad=True)  # Sync only grad\n",
      "  1050         5          0.0      0.0      0.0                  if self.kl_early_stop and synchronized: self.copy_policy()\n",
      "  1051         5          0.0      0.0      0.1                  nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.grad_clip)\n",
      "  1052         5          0.0      0.0      0.4                  self.optimizer.step()\n",
      "  1053         5          0.0      0.0      0.0                  self.optimizer.zero_grad()\n",
      "  1054         5          0.0      0.0      0.0                  if sync_iterations != 1:\n",
      "  1055                                                               # Synchronize for offsets\n",
      "  1056                                                               sync_loop = (iterations) % sync_iterations == 0\n",
      "  1057                                                               last_epoch = iterations == update_iterations\n",
      "  1058                                                               if use_collective and (sync_loop or last_epoch):\n",
      "  1059                                                                   self.synchronize('learners')\n",
      "  1060                                                                   synchronized = True\n",
      "  1061                                                               else: synchronized = False\n",
      "  1062                                           \n",
      "  1063                                                           # Update KL beta\n",
      "  1064                                                           # NOTE: Same as Torch KLPENPPOLoss implementation\n",
      "  1065         5          0.0      0.0      0.0                  if self.kl_early_stop or self.kl_beta != 0:\n",
      "  1066                                                               loss_kl_mean = loss_kl.detach().mean()\n",
      "  1067                                                               self.synchronize('learners', sync_list=[loss_kl_mean])\n",
      "  1068                                                               if not self.kl_early_stop:\n",
      "  1069                                                                   exp_limit = 32\n",
      "  1070                                                                   if loss_kl_mean < self.kl_target / 1.5 and self.kl_beta > 2**-exp_limit: self.kl_beta.data *= self.kl_beta_increment[0]\n",
      "  1071                                                                   elif loss_kl_mean > self.kl_target * 1.5 and self.kl_beta < 2**exp_limit: self.kl_beta.data *= self.kl_beta_increment[1]\n",
      "  1072                                           \n",
      "  1073                                                           # Escape and roll back if KLD too high\n",
      "  1074         5          0.0      0.0      0.0                  if self.kl_early_stop:\n",
      "  1075                                                               if loss_kl_mean > 1.5 * self.kl_target:\n",
      "  1076                                                                   if iterations - sync_iterations > 0:\n",
      "  1077                                                                       # Revert to previous synchronized state within kl target\n",
      "  1078                                                                       self.revert_policy()\n",
      "  1079                                                                       # iterations -= sync_iterations\n",
      "  1080                                                                       escape = True; break\n",
      "  1081                                                                   else:\n",
      "  1082                                                                       warnings.warn(\n",
      "  1083                                                                           'Update exceeded KL target too fast! Proceeding with update, but may be unstable. '\n",
      "  1084                                                                           'Try lowering clip or learning rate parameters.')\n",
      "  1085                                                                       escape = True; break\n",
      "  1086                                                                   \n",
      "  1087                                                       # Iterate\n",
      "  1088         5          0.0      0.0      0.0              iterations += 1\n",
      "  1089         5          0.0      0.0      0.0              if iterations >= update_iterations: escape = True\n",
      "  1090                                           \n",
      "  1091                                                       # CLI\n",
      "  1092         5          0.0      0.0      0.0              if verbose and (iterations in (1, 5) or iterations % 10 == 0 or escape):\n",
      "  1093         4          0.0      0.0      0.0                  print(\n",
      "  1094         8          0.0      0.0      0.0                      f'Iteration {iterations:02} - '\n",
      "  1095         2          0.0      0.0      0.0                      + f' + '.join([f'{k} ({np.mean([v.item() for v in vl[-batches:]]):.5f})' for k, vl in total_losses.items()])\n",
      "  1096         2          0.0      0.0      0.0                      + f' :: '\n",
      "  1097         2          0.0      0.0      0.0                      + f', '.join([f'{k} ({np.mean([v for v in vl[-batches:]]):.5f})' for k, vl in total_statistics.items()]))\n",
      "  1098                                           \n",
      "  1099                                                       # Break\n",
      "  1100         5          0.0      0.0      0.0              if escape: break\n",
      "  1101                                           \n",
      "  1102                                                   # Update scheduler\n",
      "  1103         1          0.0      0.0      0.0          self.scheduler.step()\n",
      "  1104                                                   # Update records\n",
      "  1105         1          0.0      0.0      0.0          self.policy_iteration += 1\n",
      "  1106         1          0.0      0.0      0.1          self.copy_policy()\n",
      "  1107                                                   # Return\n",
      "  1108         1          0.0      0.0      0.0          return (\n",
      "  1109         1          0.0      0.0      0.0              iterations,\n",
      "  1110         1          0.0      0.0      0.0              {k: np.mean([v.item() for v in vl]) for k, vl in total_losses.items()},\n",
      "  1111         1          0.0      0.0      0.0              {k: np.mean([v for v in vl]) for k, vl in total_statistics.items()})\n",
      "\n",
      "Total time: 0.0580192 s\n",
      "File: /home/thema/repos/inept/celltrip/utility/processing.py\n",
      "Function: split_state at line 540\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   540                                           def split_state(\n",
      "   541                                               state,\n",
      "   542                                               idx=None,\n",
      "   543                                               sample_strategy='random-proximity',\n",
      "   544                                               # Strategy kwargs\n",
      "   545                                               max_nodes=None,\n",
      "   546                                               reproducible_strategy='mean',\n",
      "   547                                               sample_dim=None,  # Should be the dim of the env\n",
      "   548                                               return_mask=False,\n",
      "   549                                           ):\n",
      "   550                                               \"Split full state matrix into individual inputs, self_idx is an optional array\"\n",
      "   551                                               # Skip if indicated \n",
      "   552                                           \n",
      "   553                                               # Parameters\n",
      "   554       300          0.0      0.0      0.2      if idx is None: idx = np.arange(state.shape[0]).tolist()\n",
      "   555       300          0.0      0.0      1.4      if not _utility.general.is_list_like(idx): idx = [idx]\n",
      "   556       300          0.0      0.0      0.1      self_idx = idx\n",
      "   557       300          0.0      0.0      0.1      del idx\n",
      "   558       300          0.0      0.0      0.5      device = state.device\n",
      "   559                                           \n",
      "   560                                               # Batch input for Lite model\n",
      "   561       300          0.0      0.0      0.2      if sample_strategy is None:\n",
      "   562                                                   # All processing case\n",
      "   563       300          0.0      0.0      0.7          if len(self_idx) == state.shape[0]:\n",
      "   564                                                       # This optimization saves a lot of time\n",
      "   565       280          0.0      0.0      6.5              if (self_idx[:-1] < self_idx[1:]).all(): return state,\n",
      "   566                                                       elif (np.unique(self_idx) == np.arange(state.shape[0])).all(): return state[self_idx],\n",
      "   567                                           \n",
      "   568                                                   # Subset case\n",
      "   569        20          0.0      0.0      8.1          self_entity = state[self_idx]\n",
      "   570        20          0.0      0.0      0.0          node_entities = state  # Could remove the self_idx in the case len == 1, but doesn't really matter\n",
      "   571        20          0.0      0.0     82.2          mask = torch.eye(state.shape[0], dtype=torch.bool, device=device)[self_idx]\n",
      "   572        20          0.0      0.0      0.0          return self_entity, node_entities, mask\n",
      "   573                                           \n",
      "   574                                               # Get self features for each node\n",
      "   575                                               self_entity = state[self_idx]\n",
      "   576                                           \n",
      "   577                                               # Get node features for each state\n",
      "   578                                               node_mask = torch.eye(state.shape[0], dtype=torch.bool, device=device)\n",
      "   579                                               node_mask = ~node_mask\n",
      "   580                                           \n",
      "   581                                               # Enforce reproducibility\n",
      "   582                                               if reproducible_strategy is not None: generator = torch.Generator(device=device)\n",
      "   583                                               else: generator = None\n",
      "   584                                           \n",
      "   585                                               # Hashing method\n",
      "   586                                               if reproducible_strategy is None:\n",
      "   587                                                   pass\n",
      "   588                                               # Hashing method\n",
      "   589                                               # NOTE: Tensors are hashed by object, so would be unreliable to directly hash tensor\n",
      "   590                                               elif reproducible_strategy == 'hash':\n",
      "   591                                                   generator.manual_seed(hash(str(state.detach().numpy())))\n",
      "   592                                               # First number\n",
      "   593                                               elif reproducible_strategy == 'first':\n",
      "   594                                                   generator.manual_seed((2**16*state.flatten()[0]).to(torch.long).item())\n",
      "   595                                               # Mean value\n",
      "   596                                               elif reproducible_strategy == 'mean':\n",
      "   597                                                   generator.manual_seed((2**16*state.mean()).to(torch.long).item())\n",
      "   598                                               # Set seed (not recommended)\n",
      "   599                                               elif type(reproducible_strategy) != str:\n",
      "   600                                                   generator.manual_seed(reproducible_strategy)\n",
      "   601                                               else:\n",
      "   602                                                   raise ValueError(f'Reproducible strategy \\'{reproducible_strategy}\\' not found.')\n",
      "   603                                           \n",
      "   604                                               # Enforce max nodes\n",
      "   605                                               num_nodes = state.shape[0] - 1\n",
      "   606                                               use_mask = max_nodes is not None and max_nodes < num_nodes\n",
      "   607                                               if use_mask:\n",
      "   608                                                   # Set new num_nodes\n",
      "   609                                                   num_nodes = max_nodes - 1\n",
      "   610                                           \n",
      "   611                                                   # Random sample `num_nodes` to `max_nodes`\n",
      "   612                                                   if sample_strategy == 'random':\n",
      "   613                                                       # Filter nodes to `max_nodes` per idx\n",
      "   614                                                       probs = torch.empty_like(node_mask, dtype=torch.get_default_dtype()).normal_(generator=generator)\n",
      "   615                                                       probs[~node_mask] = 0\n",
      "   616                                                       selected_idx = probs.topk(num_nodes, dim=-1)[1]  # Take `num_nodes` highest values\n",
      "   617                                           \n",
      "   618                                                       # Create new mask\n",
      "   619                                                       node_mask = torch.zeros((state.shape[0], state.shape[0]), dtype=torch.bool, device=device)\n",
      "   620                                                       node_mask[torch.arange(node_mask.shape[0]).unsqueeze(-1).expand(node_mask.shape[0], num_nodes), selected_idx] = True\n",
      "   621                                           \n",
      "   622                                                   # Sample closest nodes\n",
      "   623                                                   elif sample_strategy == 'proximity':\n",
      "   624                                                       # Check for dim pass\n",
      "   625                                                       assert sample_dim is not None, (\n",
      "   626                                                           f'`sample_dim` argument must be passed if `sample_strategy` is \\'{sample_strategy}\\'')\n",
      "   627                                           \n",
      "   628                                                       # Get inter-node distances\n",
      "   629                                                       dist = _utility.distance.euclidean_distance(state[..., :sample_dim])\n",
      "   630                                                       dist[~node_mask] = -1  # Set self-dist lowest for case of ties\n",
      "   631                                           \n",
      "   632                                                       # Select `max_nodes` closest\n",
      "   633                                                       selected_idx = dist.topk(num_nodes+1, largest=False, dim=-1)[1][..., 1:]\n",
      "   634                                                       \n",
      "   635                                                       # Create new mask\n",
      "   636                                                       node_mask = torch.zeros((state.shape[0], state.shape[0]), dtype=torch.bool, device=device)\n",
      "   637                                                       node_mask[torch.arange(node_mask.shape[0]).unsqueeze(-1).expand(node_mask.shape[0], num_nodes), selected_idx] = True\n",
      "   638                                           \n",
      "   639                                                   # Randomly sample from a distribution of node distance\n",
      "   640                                                   elif sample_strategy == 'random-proximity':\n",
      "   641                                                       # Check for dim pass\n",
      "   642                                                       assert sample_dim is not None, (\n",
      "   643                                                           f'`sample_dim` argument must be passed if `sample_strategy` is \\'{sample_strategy}\\'')\n",
      "   644                                           \n",
      "   645                                                       # Get inter-node distances\n",
      "   646                                                       dist = _utility.distance.euclidean_distance(state[..., :sample_dim])\n",
      "   647                                                       prob = 1 / (dist+1)\n",
      "   648                                                       prob[~node_mask] = 0  # Remove self\n",
      "   649                                           \n",
      "   650                                                       # Randomly sample\n",
      "   651                                                       # NOTE: If you get an error `_assert_async_cuda_kernel`, weights probably exploded making `actions = [nan]`\n",
      "   652                                                       node_mask = torch.zeros((state.shape[0], state.shape[0]), dtype=torch.bool, device=device)\n",
      "   653                                                       idx = prob.multinomial(num_nodes, replacement=False, generator=generator)\n",
      "   654                                           \n",
      "   655                                                       # Apply sampling\n",
      "   656                                                       mat = torch.arange(node_mask.shape[0], device=device).unsqueeze(-1).expand(node_mask.shape[0], num_nodes)\n",
      "   657                                                       node_mask[mat, idx] = True\n",
      "   658                                                   else:\n",
      "   659                                                       # TODO: Verify works\n",
      "   660                                                       raise ValueError(f'Sample strategy \\'{sample_strategy}\\' not found.')\n",
      "   661                                                   \n",
      "   662                                               # Shrink mask to appropriate size\n",
      "   663                                               # NOTE: Randomization needs to be done on all samples for reproducibility\n",
      "   664                                               # print(node_mask)\n",
      "   665                                               node_mask = node_mask[self_idx]\n",
      "   666                                               \n",
      "   667                                               # Final formation\n",
      "   668                                               node_entities = state.unsqueeze(0).expand(len(self_idx), *state.shape)\n",
      "   669                                               node_entities = node_entities[node_mask].reshape(len(self_idx), num_nodes, state.shape[1])\n",
      "   670                                           \n",
      "   671                                               # Return\n",
      "   672                                               ret = (self_entity, node_entities)\n",
      "   673                                               if return_mask: ret += (node_mask,)\n",
      "   674                                               return ret\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update\n",
    "import line_profiler\n",
    "import numpy as np\n",
    "prof = line_profiler.LineProfiler(policy.update, memory.fast_sample, memory._concat_states, celltrip.utility.processing.split_state)\n",
    "# with torch.autograd.detect_anomaly():\n",
    "ret = prof.runcall(policy.update, memory, verbose=True)\n",
    "print(', '.join([f'{k}: {v:.3f}' for k, v in ret[1].items()]))\n",
    "prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     # Forward\n",
    "#     import line_profiler\n",
    "#     memory.mark_sampled()\n",
    "#     memory.cleanup()\n",
    "#     prof = line_profiler.LineProfiler(\n",
    "#         celltrip.train.simulate_until_completion,\n",
    "#         celltrip.policy.PPO.forward, celltrip.policy.EntitySelfAttentionLite.forward, celltrip.policy.ResidualAttention.forward,\n",
    "#         celltrip.environment.EnvironmentBase.step)\n",
    "#     ret = prof.runcall(celltrip.train.simulate_until_completion, env, policy, memory, max_memories=policy.epoch_size, reset_on_finish=True)\n",
    "#     print(f'total: {ret[2]:.3f}, ' + ', '.join([f'{k}: {v:.3f}' for k, v in ret[3].items()]))\n",
    "#     memory.feed_new(policy.reward_standardization)\n",
    "#     memory.compute_advantages(moving_standardization=policy.reward_standardization)\n",
    "#     # prof.print_stats(output_unit=1)\n",
    "\n",
    "#     # Update\n",
    "#     import line_profiler\n",
    "#     prof = line_profiler.LineProfiler(policy.update, memory.fast_sample, celltrip.utility.processing.split_state)\n",
    "#     ret = prof.runcall(policy.update, memory, verbose=True)\n",
    "#     # print(', '.join([f'{k}: {v:.3f}' for k, v in ret[1].items()]))\n",
    "#     # prof.print_stats(output_unit=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
