{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cython is active\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shlex\n",
    "\n",
    "import ray\n",
    "\n",
    "import celltrip\n",
    "\n",
    "# Detect Cython\n",
    "CYTHON_ACTIVE = os.path.splitext(celltrip.utility.general.__file__)[1] in ('.c', '.so')\n",
    "print(f'Cython is{\" not\" if not CYTHON_ACTIVE else \"\"} active')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train.py --merge_files s3://nkalafut-celltrip/Flysta3D/E14-16h_a_expression.h5ad s3://nkalafut-celltrip/Flysta3D/E16-18h_a_expression.h5ad s3://nkalafut-celltrip/Flysta3D/L1_a_expression.h5ad s3://nkalafut-celltrip/Flysta3D/L2_a_expression.h5ad s3://nkalafut-celltrip/Flysta3D/L3_b_expression.h5ad --merge_files s3://nkalafut-celltrip/Flysta3D/E14-16h_a_spatial.h5ad s3://nkalafut-celltrip/Flysta3D/E16-18h_a_spatial.h5ad s3://nkalafut-celltrip/Flysta3D/L1_a_spatial.h5ad s3://nkalafut-celltrip/Flysta3D/L2_a_spatial.h5ad s3://nkalafut-celltrip/Flysta3D/L3_b_spatial.h5ad --target_modalities 1 --partition_cols development --backed --dim 8 --train_split .8 --num_gpus 2 --num_learners 2 --num_runners 2 --update_timesteps 1_000_000 --max_timesteps 800_000_000 --dont_sync_across_nodes --logfile s3://nkalafut-celltrip/logs/Flysta3D-250729.log --flush_iterations 1 --checkpoint_iterations 25 --checkpoint_dir s3://nkalafut-celltrip/checkpoints --checkpoint_name Flysta3D-250729\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "# NOTE: It is not recommended to use s3 with credentials unless the creds are permanent, the bucket is public, or this is run on AWS\n",
    "parser = argparse.ArgumentParser(description='Train CellTRIP model', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "# Reading\n",
    "group = parser.add_argument_group('Input')\n",
    "group.add_argument('input_files', type=str, nargs='*', help='h5ad files to be used for input')\n",
    "group.add_argument('--merge_files', type=str, action='append', nargs='+', help='h5ad files to merge as input')\n",
    "group.add_argument('--partition_cols', type=str, nargs='+', help='Columns for data partitioning, found in `adata.obs` DataFrame')\n",
    "group.add_argument('--backed', action='store_true', help='Read data directly from disk or s3, saving memory at the cost of time')\n",
    "group.add_argument('--input_modalities', type=int, nargs='+', help='Input modalities to give to CellTRIP')\n",
    "group.add_argument('--target_modalities', type=int, nargs='+', help='Target modalities to emulate, dictates environment reward')\n",
    "# Algorithm\n",
    "group = parser.add_argument_group('Algorithm')\n",
    "group.add_argument('--dim', type=int, default=16, help='Dimensions in the output latent space')\n",
    "group.add_argument('--discrete', action='store_true', help='Use the discrete model rather than continuous')\n",
    "group.add_argument('--train_split', type=float, default=1., help='Fraction of input data to use as training')\n",
    "group.add_argument('--train_partitions', action='store_true', help='Split training/validation data across partitions rather than samples')\n",
    "# Computation\n",
    "group = parser.add_argument_group('Computation')\n",
    "group.add_argument('--num_gpus', type=int, default=1, help='Number of GPUs to use during computation')\n",
    "group.add_argument('--num_learners', type=int, default=1, help='Number of learners used in backward computation, cannot exceed GPUs')\n",
    "group.add_argument('--num_runners', type=int, default=1, help='Number of workers for environment simulation')\n",
    "# Training\n",
    "group = parser.add_argument_group('Training')\n",
    "group.add_argument('--update_timesteps', type=int, default=int(1e6), help='Number of timesteps recorded before each update')\n",
    "group.add_argument('--max_timesteps', type=int, default=int(2e9), help='Maximum number of timesteps to compute before exiting')\n",
    "group.add_argument('--dont_sync_across_nodes', action='store_true', help='Avoid memory sync across nodes, saving overhead time at the cost of stability')\n",
    "# File saves\n",
    "group = parser.add_argument_group('Logging')\n",
    "group.add_argument('--logfile', type=str, default='cli', help='Location for log file, can be `cli`, `<local_file>`, or `<s3 location>`')\n",
    "group.add_argument('--flush_iterations', default=25, type=int, help='Number of iterations to wait before flushing logs')\n",
    "group.add_argument('--checkpoint', type=str, help='Checkpoint to use for initializing model')\n",
    "group.add_argument('--checkpoint_iterations', type=int, default=100, help='Number of updates to wait before recording checkpoints')\n",
    "group.add_argument('--checkpoint_dir', type=str, default='./checkpoints', help='Directory for checkpoints')\n",
    "group.add_argument('--checkpoint_name', type=str, help='Run name, for checkpointing')\n",
    "\n",
    "# Notebook defaults and script handling\n",
    "if not celltrip.utility.notebook.is_notebook():\n",
    "    # ray job submit -- python train.py...\n",
    "    config = parser.parse_args()\n",
    "else:\n",
    "    experiment_name = 'Flysta3D-250729'\n",
    "    bucket_name = 'nkalafut-celltrip'\n",
    "    # bucket_name = 'arn:aws:s3:us-east-2:245432013314:accesspoint/ray-nkalafut-celltrip'\n",
    "    command = (\n",
    "        # MERFISH\n",
    "        # f's3://{bucket_name}/MERFISH/expression.h5ad s3://{bucket_name}/MERFISH/spatial.h5ad --target_modalities 1 '\n",
    "        # scGLUE\n",
    "        # f's3://{bucket_name}/scGLUE/Chen-2019-RNA.h5ad s3://{bucket_name}/scGLUE/Chen-2019-ATAC.h5ad '\n",
    "        # f's3://{bucket_name}/scGLUE/Chen-2019-RNA.h5ad s3://{bucket_name}/scGLUE/Chen-2019-ATAC.h5ad --input_modalities 0 --target_modalities 0 '\n",
    "        # f'../data/scglue/Chen-2019-RNA.h5ad ../data/scglue/Chen-2019-ATAC.h5ad --input_modalities 0 --target_modalities 0 '\n",
    "        # Flysta3D\n",
    "        f' '.join([f'--merge_files ' + ' ' .join([f's3://{bucket_name}/Flysta3D/{p}_{m}.h5ad' for p in ('E14-16h_a', 'E16-18h_a', 'L1_a', 'L2_a', 'L3_b')]) for m in ('expression', 'spatial')]) + ' '\n",
    "        f'--target_modalities 1 '\n",
    "        f'--partition_cols development '\n",
    "        # Particular stage Flysta\n",
    "        # f' '.join([f'--merge_files ' + ' ' .join([f's3://{bucket_name}/Flysta3D/{p}_{m}.h5ad' for p in ('E14-16h_a',)]) for m in ('expression', 'spatial')]) + ' '\n",
    "        # f'--target_modalities 1 '\n",
    "        # f'--partition_cols development '\n",
    "        # Tahoe-100M\n",
    "        # f'--merge_files ' + ' '.join([f's3://{bucket_name}/Tahoe/plate{i}_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab.h5ad' for i in range(1, 15)]) + ' '\n",
    "        # f'--partition_cols sample '\n",
    "        # scMultiSim\n",
    "        # f's3://{bucket_name}/scMultiSim/expression.h5ad s3://{bucket_name}/scMultiSim/peaks.h5ad '\n",
    "        # MERFISH Bench\n",
    "        # f's3://{bucket_name}/MERFISH_Bench/expression.h5ad s3://{bucket_name}/MERFISH_Bench/spatial.h5ad '\n",
    "        # f'--target_modalities 1 '\n",
    "        # TemporalBrain\n",
    "        # f's3://{bucket_name}/TemporalBrain/expression.h5ad s3://{bucket_name}/TemporalBrain/peaks.h5ad '\n",
    "        # f'--partition_cols \"Donor ID\" '\n",
    "        # Virtual Cell Challenge\n",
    "        # f's3://{bucket_name}/VirtualCell/vcc_flt_data.h5ad '\n",
    "        # f'--partition_cols target_gene batch '\n",
    "\n",
    "        f'--backed '\n",
    "        # f'--dim 2 '\n",
    "        # f'--dim 8 '\n",
    "        f'--dim 8 '\n",
    "        # f'--discrete '\n",
    "\n",
    "        # Sample split\n",
    "        f'--train_split .8 '\n",
    "        # Partition split\n",
    "        # f'--train_split .6 '\n",
    "        # f'--train_partitions '\n",
    "        # Single slice\n",
    "        # f'--train_split .0001 '\n",
    "        # f'--train_partitions '\n",
    "\n",
    "        f'--num_gpus 2 --num_learners 2 --num_runners 2 '\n",
    "        f'--update_timesteps 1_000_000 '\n",
    "        f'--max_timesteps 800_000_000 '\n",
    "        # f'--update_timesteps 100_000 '\n",
    "        # f'--max_timesteps 100_000_000 '\n",
    "        f'--dont_sync_across_nodes '\n",
    "        f'--logfile s3://{bucket_name}/logs/{experiment_name}.log '\n",
    "        f'--flush_iterations 1 '\n",
    "        # f'--checkpoint s3://nkalafut-celltrip/checkpoints/flysta3d-E14-16h_a-single-250702-d02-0400.weights '\n",
    "        f'--checkpoint_iterations 25 '\n",
    "        f'--checkpoint_dir s3://{bucket_name}/checkpoints '\n",
    "        f'--checkpoint_name {experiment_name}')\n",
    "    config = parser.parse_args(shlex.split(command))\n",
    "    print(f'python train.py {command}')\n",
    "    \n",
    "# Defaults\n",
    "if config.checkpoint_name is None:\n",
    "    config.checkpoint_name = f'RUN_{random.randint(0, 2**32):0>10}'\n",
    "    print(f'Run Name: {config.checkpoint_name}')\n",
    "# print(config)  # CLI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Ray\n",
    "ray.shutdown()\n",
    "a = ray.init(\n",
    "    # address='ray://100.85.187.118:10001',\n",
    "    address='ray://localhost:10001',\n",
    "    runtime_env={\n",
    "        'py_modules': [celltrip],\n",
    "        'pip': '../requirements.txt',\n",
    "        'env_vars': {\n",
    "            # **access_keys,\n",
    "            'RAY_DEDUP_LOGS': '0'}},\n",
    "        # 'NCCL_SOCKET_IFNAME': 'tailscale',  # lo,en,wls,docker,tailscale\n",
    "    _system_config={'enable_worker_prestart': True})  # Doesn't really work for scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_cpus=1e-4)\n",
    "def train(config):\n",
    "    import celltrip\n",
    "\n",
    "    # Initialization\n",
    "    dataloader_kwargs = {\n",
    "        'num_nodes': [2**9, 2**11], 'mask': config.train_split,\n",
    "        'mask_partitions': config.train_partitions}  # {'num_nodes': 20, 'pca_dim': 128}\n",
    "    environment_kwargs = {\n",
    "        'input_modalities': config.input_modalities,\n",
    "        'target_modalities': config.target_modalities, 'dim': config.dim,\n",
    "        'discrete': config.discrete}  # , 'spherical': config.discrete\n",
    "    policy_kwargs = {'discrete': config.discrete}\n",
    "    memory_kwargs = {'device': 'cuda:0'}\n",
    "    initializers = celltrip.train.get_initializers(\n",
    "        input_files=config.input_files, merge_files=config.merge_files,\n",
    "        backed=config.backed, partition_cols=config.partition_cols,\n",
    "        dataloader_kwargs=dataloader_kwargs,\n",
    "        environment_kwargs=environment_kwargs,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        memory_kwargs=memory_kwargs)  # Skips casting, cutting time significantly for relatively small batch sizes\n",
    "\n",
    "    # Stages\n",
    "    stage_functions = [\n",
    "        # lambda w: w.env.set_delta(.1),\n",
    "        # lambda w: w.env.set_delta(.05),\n",
    "        # lambda w: w.env.set_delta(.01),\n",
    "        # lambda w: w.env.set_delta(.005),\n",
    "    ]\n",
    "\n",
    "    # Run function\n",
    "    celltrip.train.train_celltrip(\n",
    "        initializers=initializers,\n",
    "        num_gpus=config.num_gpus, num_learners=config.num_learners,\n",
    "        num_runners=config.num_runners, max_timesteps=config.max_timesteps,\n",
    "        update_timesteps=config.update_timesteps, sync_across_nodes=not config.dont_sync_across_nodes,\n",
    "        flush_iterations=config.flush_iterations,\n",
    "        checkpoint_iterations=config.checkpoint_iterations, checkpoint_dir=config.checkpoint_dir,\n",
    "        checkpoint=config.checkpoint, checkpoint_name=config.checkpoint_name,\n",
    "        stage_functions=stage_functions, logfile=config.logfile)\n",
    "\n",
    "ray.get(train.remote(config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/repos/inept/celltrip/utility/processing.py:169: RuntimeWarning: Modality 1 too small for PCA (2 features), skipping\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# torch.random.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # Initialize locally\n",
    "# os.environ['AWS_PROFILE'] = 'waisman-admin'\n",
    "# config.update_timesteps = 100_000\n",
    "# config.max_timesteps = 20_000_000\n",
    "\n",
    "# dataloader_kwargs = {'num_nodes': [2**9, 2**11], 'mask': config.train_split}  # {'num_nodes': [2**9, 2**11], 'pca_dim': 128}\n",
    "# environment_kwargs = {\n",
    "#     'input_modalities': config.input_modalities,\n",
    "#     'target_modalities': config.target_modalities, 'dim': config.dim}\n",
    "# env_init, policy_init, memory_init = celltrip.train.get_initializers(\n",
    "#     input_files=config.input_files, merge_files=config.merge_files,\n",
    "#     partition_cols=config.partition_cols,\n",
    "#     backed=config.backed, dataloader_kwargs=dataloader_kwargs,\n",
    "#     policy_kwargs={'minibatch_size': 10_000},\n",
    "#     # memory_kwargs={'device': 'cuda:0'},  # Skips casting, cutting time significantly for relatively small batch sizes\n",
    "#     environment_kwargs=environment_kwargs)\n",
    "\n",
    "# # Environment\n",
    "# # os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "# try: env\n",
    "# except: env = env_init().to('cuda')\n",
    "\n",
    "# # Policy\n",
    "# policy = policy_init(env).to('cuda')\n",
    "\n",
    "# # Memory\n",
    "# memory = memory_init(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROLLOUT: total: -1015.019, distance: 0.000, pinning: -13.561, origin: 0.000, bound: 0.000, velocity: -999.738, action: -1.720\n",
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 0.84055 s\n",
      "File: /home/thema/repos/inept/celltrip/environment.py\n",
      "Function: step at line 200\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   200                                               def step(self, actions=None, *, delta=None, pinning_func_list=None, return_itemized_rewards=False):\n",
      "   201                                                   # Defaults\n",
      "   202       470          0.0      0.0      0.0          if actions is None: actions = torch.zeros_like(self.vel, device=self.device)\n",
      "   203       470          0.0      0.0      0.0          if delta is None: delta = self.delta\n",
      "   204                                           \n",
      "   205                                                   # Check dimensions\n",
      "   206                                                   # assert actions.shape == self.vel.shape\n",
      "   207                                           \n",
      "   208                                                   # Hypersphere or hypercube constraints\n",
      "   209                                                   # NOTE: Hypersphere is technically more correct, but also harder on the model\n",
      "   210                                           \n",
      "   211                                                   ### Pre-step calculations\n",
      "   212       470          0.0      0.0      0.0          if self.compute_rewards:\n",
      "   213                                                       # Distance reward (Emulate combined intra-modal distances)\n",
      "   214       470          0.0      0.0      0.0              get_reward_distance = lambda: self.get_distance_match(use_cache=True)\n",
      "   215                                                       # get_reward_distance = lambda: (self.get_distance_match()+self.epsilon).log().mean(dim=-1)\n",
      "   216                                                       # get_reward_distance = lambda: 1 / (1+self.get_distance_match())\n",
      "   217       470          0.0      0.0      0.0              if self.reward_scales['reward_distance'] != 0:\n",
      "   218                                                           reward_distance = get_reward_distance()\n",
      "   219                                                           # reward_distance = 0\n",
      "   220       470          0.0      0.0      1.3              else: reward_distance = torch.zeros(actions.shape[0], device=self.device)\n",
      "   221                                                       # Pinning reward\n",
      "   222       470          0.0      0.0      0.0              get_reward_pinning = lambda: self.get_pinning(use_cache=True, pinning_func_list=pinning_func_list)\n",
      "   223                                                       # get_reward_pinning = lambda: (self.get_pinning(use_cache=True)+self.epsilon).log().mean(dim=-1)\n",
      "   224       470          0.0      0.0      0.0              if self.reward_scales['reward_pinning'] != 0:\n",
      "   225       470          0.0      0.0      0.2                  reward_pinning = get_reward_pinning()\n",
      "   226                                                       else: reward_pinning = torch.zeros(actions.shape[0], device=self.device)\n",
      "   227                                                       # Origin penalty\n",
      "   228       470          0.0      0.0      0.0              get_reward_origin = lambda: self.get_distance_from_origin()\n",
      "   229                                                       # get_reward_origin = lambda: (self.get_distance_from_origin()+self.epsilon).log()\n",
      "   230       470          0.0      0.0      0.0              if self.reward_scales['reward_origin'] != 0:\n",
      "   231                                                           reward_origin = get_reward_origin()\n",
      "   232                                                           # reward_origin = 0\n",
      "   233       470          0.0      0.0      0.8              else: reward_origin = torch.zeros(actions.shape[0], device=self.device)\n",
      "   234                                                       # Velocity penalty\n",
      "   235       470          0.0      0.0      0.0              if self.spherical:\n",
      "   236       470          0.0      0.0      0.0                  get_penalty_velocity = lambda: self.vel.norm(dim=-1)\n",
      "   237                                                           # get_penalty_velocity = lambda: (self.vel.norm(dim=-1)+self.epsilon).log()\n",
      "   238                                                       else:\n",
      "   239                                                           get_penalty_velocity = lambda: self.vel.square().mean(dim=-1)\n",
      "   240                                                           get_penalty_velocity = lambda: self.vel.mean(dim=-1)\n",
      "   241       470          0.0      0.0      0.0              if self.reward_scales['penalty_velocity'] != 0:\n",
      "   242       470          0.0      0.0      1.7                  penalty_velocity = get_penalty_velocity()\n",
      "   243                                                           # penalty_velocity = 0\n",
      "   244                                                       else: penalty_velocity = torch.zeros(actions.shape[0], device=self.device)\n",
      "   245                                           \n",
      "   246                                                   ### Step positions\n",
      "   247                                                   # Old storage\n",
      "   248                                                   # old_vel = self.vel.clone()\n",
      "   249                                                   # old_bound_hit_mask = self.pos.abs() == self.pos_bound\n",
      "   250                                                   # Clamp actions\n",
      "   251                                                   # actions = actions.clamp(-self.force_bound, self.force_bound)\n",
      "   252                                                   # Convert actions to velocity\n",
      "   253                                                   # NOTE: It would be nice to use polar here, but couldn't find a general method which didn't require unreasonable precision for later axes\n",
      "   254                                                   # magnitude = (.5*actions[..., [-1]]+.5).clamp(0, self.force_bound)\n",
      "   255                                                   # direction = actions[..., :-1] / actions[..., :-1].norm(keepdim=True, dim=-1)\n",
      "   256                                                   # force = magnitude * direction\n",
      "   257       470          0.0      0.0      0.0          if self.discrete:\n",
      "   258                                                       force = self.discrete_force * (actions - 1)\n",
      "   259                                                       if self.spherical:\n",
      "   260                                                           force_norm = force.norm(keepdim=True, dim=-1)\n",
      "   261                                                           force_norm[force_norm==0] = 1\n",
      "   262                                                           force = self.discrete_force * force / force_norm\n",
      "   263       470          0.0      0.0      0.0          else: force = actions\n",
      "   264       470          0.0      0.0      0.0          if self.spherical:\n",
      "   265       470          0.0      0.0      1.2              force_norm = force.norm(keepdim=True, dim=-1)\n",
      "   266       470          0.0      0.0      1.1              strong_mask = force_norm.squeeze(-1) > self.force_bound\n",
      "   267       470          0.1      0.0      7.6              force[strong_mask] = self.force_bound * force[strong_mask] / force_norm[strong_mask]\n",
      "   268                                                   else:\n",
      "   269                                                       force = force.clamp(-self.force_bound, self.force_bound)\n",
      "   270                                                   # Add velocity and apply friction\n",
      "   271       470          0.1      0.0     11.4          self.add_velocities(delta * force)\n",
      "   272       470          0.0      0.0      5.1          self.apply_friction(realized_force=delta*self.friction_force)\n",
      "   273                                                   # Iterate positions\n",
      "   274       470          0.0      0.0      1.2          self.pos = self.pos + delta * self.vel  # .square()  # TODO: Experimental square\n",
      "   275                                                   # Clip by bounds\n",
      "   276                                                   # self.pos = torch.clamp(self.pos, -self.pos_bound, self.pos_bound)\n",
      "   277                                                   # Adjust nodes on bound-hits\n",
      "   278                                                   # bound_hit_mask = self.pos.abs() == self.pos_bound\n",
      "   279                                                   # self.pos[bound_hit_mask.sum(dim=1) > 0] = 0  # Send to center\n",
      "   280                                                   # self.vel[bound_hit_mask] = 0  # Kill velocity\n",
      "   281                                                   # self.vel[bound_hit_mask] = -self.vel[bound_hit_mask]  # Bounce\n",
      "   282                                                   # Out of bounds\n",
      "   283       470          0.0      0.0      2.2          if self.spherical: oob = self.pos.norm(dim=-1)-self.pos_bound\n",
      "   284                                                   else: oob = (self.pos.abs() >= self.pos_bound).sum(dim=-1)\n",
      "   285                                                   # oob[oob < 0] = 0\n",
      "   286       470          0.0      0.0      1.8          self.vel[oob > 0] = 0\n",
      "   287       470          0.0      0.0      0.0          if self.spherical:\n",
      "   288       470          0.1      0.0     10.5              self.pos[oob > 0] = self.pos_bound * self.pos[oob > 0] / self.pos[oob > 0].norm(keepdim=True, dim=-1)\n",
      "   289                                                   else: self.pos = self.pos.clamp(-self.pos_bound, self.pos_bound)\n",
      "   290                                                   # reward_distance[oob > 0] = 0\n",
      "   291                                                   # Reset cache\n",
      "   292       470          0.0      0.0      0.2          self.reset_cache()\n",
      "   293                                           \n",
      "   294                                                   ### Post-step calculations\n",
      "   295                                                   # Finished\n",
      "   296       470          0.0      0.0      0.0          self.time += delta\n",
      "   297       470          0.0      0.0      0.8          finished, _ = self.finished()\n",
      "   298       470          0.0      0.0      0.0          if self.compute_rewards:\n",
      "   299                                                       # Distance reward\n",
      "   300       470          0.0      0.0      0.0              if self.reward_scales['reward_distance'] != 0: reward_distance -= get_reward_distance()\n",
      "   301                                                       # Pinning reward\n",
      "   302       470          0.1      0.0     12.0              if self.reward_scales['reward_pinning'] != 0: reward_pinning -= get_reward_pinning()\n",
      "   303                                                       # Origin reward\n",
      "   304       470          0.0      0.0      0.0              if self.reward_scales['reward_origin'] != 0: reward_origin -= get_reward_origin()\n",
      "   305                                                       # Velocity penalty (Apply to ending velocity)\n",
      "   306       470          0.0      0.0      2.1              if self.reward_scales['penalty_velocity'] != 0: penalty_velocity -= get_penalty_velocity()\n",
      "   307                                                       # Boundary penalty\n",
      "   308       470          0.0      0.0      1.0              penalty_bound = torch.zeros(self.pos.shape[0], device=self.device)\n",
      "   309       470          0.0      0.0      1.6              penalty_bound[oob > 0] = -1\n",
      "   310                                                       # Action penalty (Smooth movements)\n",
      "   311                                                       # NOTE: Calculated on unclipped forces\n",
      "   312       470          0.0      0.0      0.0              if self.spherical:\n",
      "   313                                                           # penalty_action = -force_norm.squeeze(-1).square()  # WRONG\n",
      "   314       470          0.0      0.0      2.0                  penalty_action = -force.norm(dim=-1)\n",
      "   315                                                       else: penalty_action = -actions.square().mean(dim=-1)\n",
      "   316                                           \n",
      "   317                                                       ### Management\n",
      "   318       470          0.1      0.0     15.4              if self.get_distance_match().mean() < self.best: self.lapses += delta\n",
      "   319       445          0.1      0.0     13.0              else: self.best = self.get_distance_match().mean(); self.lapses = 0\n",
      "   320                                           \n",
      "   321       470          0.0      0.0      0.7              reward_distance     *=  self.reward_scales['reward_distance']    * 1e-1/delta\n",
      "   322       470          0.0      0.0      0.5              reward_pinning       *=  self.reward_scales['reward_pinning']      * 1e-1/delta\n",
      "   323       470          0.0      0.0      0.4              reward_origin       *=  self.reward_scales['reward_origin']      * 1e-1/delta\n",
      "   324       470          0.0      0.0      0.4              penalty_bound       *=  self.reward_scales['penalty_bound']      * 1e0\n",
      "   325       470          0.0      0.0      0.4              penalty_velocity    *=  self.reward_scales['penalty_velocity']   * 1e0/delta\n",
      "   326       470          0.0      0.0      0.4              penalty_action      *=  self.reward_scales['penalty_action']     * 1e-3\n",
      "   327                                                       # self.steps += 1\n",
      "   328                                                   else:\n",
      "   329                                                       placeholder = torch.zeros(self.num_nodes, device=self.device)\n",
      "   330                                                       reward_distance = placeholder\n",
      "   331                                                       reward_pinning = placeholder\n",
      "   332                                                       reward_origin = placeholder\n",
      "   333                                                       penalty_bound = placeholder\n",
      "   334                                                       penalty_velocity = placeholder\n",
      "   335                                                       penalty_action = placeholder\n",
      "   336                                           \n",
      "   337                                                   # Compute total reward\n",
      "   338       470          0.0      0.0      0.0          rwd = (\n",
      "   339      2820          0.0      0.0      2.2              reward_distance\n",
      "   340       470          0.0      0.0      0.0              + reward_pinning\n",
      "   341       470          0.0      0.0      0.0              + reward_origin\n",
      "   342       470          0.0      0.0      0.0              + penalty_bound\n",
      "   343       470          0.0      0.0      0.0              + penalty_velocity\n",
      "   344       470          0.0      0.0      0.0              + penalty_action)\n",
      "   345                                           \n",
      "   346       470          0.0      0.0      0.0          ret = (rwd, finished)\n",
      "   347       940          0.0      0.0      0.1          if return_itemized_rewards: ret += {\n",
      "   348       470          0.0      0.0      0.0              'distance': reward_distance,\n",
      "   349       470          0.0      0.0      0.0              'pinning': reward_pinning,\n",
      "   350       470          0.0      0.0      0.0              'origin': reward_origin,\n",
      "   351       470          0.0      0.0      0.0              'bound': penalty_bound,\n",
      "   352       470          0.0      0.0      0.0              'velocity': penalty_velocity,\n",
      "   353       470          0.0      0.0      0.0              'action': penalty_action},\n",
      "   354       470          0.0      0.0      0.0          return ret\n",
      "\n",
      "Total time: 0 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: forward at line 169\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   169                                               def forward(self, x, kv=None, mask=None):\n",
      "   170                                                   # Parameters\n",
      "   171                                                   x1 = x\n",
      "   172                                                   layer_norm_idx = 0\n",
      "   173                                           \n",
      "   174                                                   # Apply residual self attention\n",
      "   175                                                   x2 = self.layer_norms[layer_norm_idx](x1)\n",
      "   176                                                   # x2 = x1\n",
      "   177                                                   layer_norm_idx += 1\n",
      "   178                                                   if kv is None: kv = x2\n",
      "   179                                                   x3, _ = self.attention(x2, kv, kv, attn_mask=mask)\n",
      "   180                                                   x1 = x1 + x3\n",
      "   181                                           \n",
      "   182                                                   # Apply residual mlps\n",
      "   183                                                   for mlp in self.mlps:\n",
      "   184                                                       x2 = self.layer_norms[layer_norm_idx](x1)\n",
      "   185                                                       # x2 = x1\n",
      "   186                                                       layer_norm_idx += 1\n",
      "   187                                                       x3 = mlp(x2)\n",
      "   188                                                       x1 = x1 + x3\n",
      "   189                                           \n",
      "   190                                                   # Final layer norm\n",
      "   191                                                   xf = self.layer_norms[layer_norm_idx](x1)\n",
      "   192                                                   # xf = x1\n",
      "   193                                           \n",
      "   194                                                   return xf\n",
      "\n",
      "Total time: 0.809882 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: forward at line 500\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   500                                               def forward(\n",
      "   501                                                       self, self_entities, node_entities=None, mask=None,\n",
      "   502                                                       actor=True, action=None, entropy=False, critic=False,\n",
      "   503                                                       feature_embeds=None, return_feature_embeds=False,\n",
      "   504                                                       squeeze=True, fit_and_strip=True):\n",
      "   505                                                   # Formatting\n",
      "   506       473          0.0      0.0      0.0          if node_entities is None: node_entities = self_entities\n",
      "   507       473          0.0      0.0      0.0          if mask is None:\n",
      "   508       473          0.0      0.0      1.6              mask = torch.eye(self_entities.shape[-2], dtype=torch.bool, device=self_entities.device)\n",
      "   509       473          0.0      0.0      0.1              if self_entities.dim() > 2: mask = mask.repeat((self_entities.shape[0], 1, 1))\n",
      "   510                                           \n",
      "   511                                                   # Workaround for https://github.com/pytorch/pytorch/issues/41508\n",
      "   512                                                   # Essentially, totally masked entries in attn_mask will cause NAN on backprop\n",
      "   513                                                   # regardless of if they're even considered in loss, so, we allow masked entries\n",
      "   514                                                   # to attend to themselves\n",
      "   515       473          0.0      0.0      2.6          include_mask = mask.sum(dim=-1) < mask.shape[-1]\n",
      "   516       473          0.1      0.0      6.8          mask[~include_mask, torch.argwhere(~include_mask)[:, -1]] = False\n",
      "   517                                                   \n",
      "   518                                                   # More formatting\n",
      "   519       473          0.0      0.0      0.1          if self_entities.dim() > 2:\n",
      "   520                                                       # NOTE: Grouped batches (i.e. (>1, >1, ...) shape) are possible with squeeze=False\n",
      "   521                                                       if mask.dim() < 3: mask.unsqueeze(0)\n",
      "   522                                                       # mask = mask.repeat((self.heads, 1, 1))\n",
      "   523                                                       mask = mask[[i for i in range(mask.shape[0]) for _ in range(self.heads)]]\n",
      "   524       473          0.0      0.0      0.0          if feature_embeds is not None:\n",
      "   525       470          0.0      0.0      0.0              feature_embeds_ret = feature_embeds\n",
      "   526       470          0.0      0.0      0.0              feature_embeds = feature_embeds.copy()\n",
      "   527         3          0.0      0.0      0.0          else: feature_embeds_ret = []\n",
      "   528                                           \n",
      "   529                                                   # Actor block\n",
      "   530       473          0.0      0.0      0.0          if actor or not self.independent_critic:\n",
      "   531                                                       # Positional embedding\n",
      "   532       473          0.0      0.0      3.0              self_pos_embeds = self.self_pos_embed(self_entities[..., :self.positional_dim])\n",
      "   533       473          0.0      0.0      2.5              node_pos_embeds = self.node_pos_embed(node_entities[..., :self.positional_dim])\n",
      "   534                                                       # Feature embedding\n",
      "   535       473          0.0      0.0      0.1              if feature_embeds is not None: self_feat_embeds, node_feat_embeds = feature_embeds.pop(0)\n",
      "   536                                                       else:\n",
      "   537         3          0.0      0.0      0.0                  self_feat_embeds = self.self_feat_embed(self_entities[..., self.positional_dim:])\n",
      "   538         3          0.0      0.0      0.0                  node_feat_embeds = self.node_feat_embed(node_entities[..., self.positional_dim:])\n",
      "   539         3          0.0      0.0      0.0                  feature_embeds_ret.append((self_feat_embeds, node_feat_embeds))\n",
      "   540                                                       # Self embeddings\n",
      "   541       473          0.0      0.0      5.6              self_embeds = self.self_embed(self_pos_embeds+self_feat_embeds)\n",
      "   542                                                       # self_embeds = self.self_embed(self_pos_embeds)\n",
      "   543                                                       # Node embeddings\n",
      "   544       473          0.0      0.0      5.1              node_embeds = self.node_embed(node_pos_embeds+node_feat_embeds)\n",
      "   545                                                       # node_embeds = self.self_embed(node_pos_embeds)\n",
      "   546                                                       # Attention\n",
      "   547       946          0.0      0.0      0.3              for block in self.residual_attention_blocks:\n",
      "   548       473          0.3      0.0     34.2                  self_embeds = block(self_embeds, kv=node_embeds, mask=mask)\n",
      "   549       473          0.0      0.0      0.0              actor_self_embeds = self_embeds\n",
      "   550                                           \n",
      "   551                                                   # Critic block\n",
      "   552       473          0.0      0.0      0.0          if self.independent_critic and critic:\n",
      "   553                                                       # Positional embedding\n",
      "   554                                                       self_pos_embeds = self.critic_self_pos_embed(self_entities[..., :self.positional_dim])\n",
      "   555                                                       node_pos_embeds = self.critic_node_pos_embed(node_entities[..., :self.positional_dim])\n",
      "   556                                                       # Feature embedding\n",
      "   557                                                       if feature_embeds is not None: self_feat_embeds, node_feat_embeds = feature_embeds.pop(0)\n",
      "   558                                                       else:\n",
      "   559                                                           self_feat_embeds = self.critic_self_feat_embed(self_entities[..., self.positional_dim:])\n",
      "   560                                                           node_feat_embeds = self.critic_node_feat_embed(node_entities[..., self.positional_dim:])\n",
      "   561                                                           feature_embeds_ret.append((self_feat_embeds, node_feat_embeds))\n",
      "   562                                                       # Self embeddings\n",
      "   563                                                       self_embeds = self.critic_self_embed(self_pos_embeds+self_feat_embeds)\n",
      "   564                                                       # Node embeddings\n",
      "   565                                                       node_embeds = self.critic_node_embed(node_pos_embeds+node_feat_embeds)\n",
      "   566                                                       # Attention\n",
      "   567                                                       for block in self.critic_residual_attention_blocks:\n",
      "   568                                                           self_embeds = block(self_embeds, kv=node_embeds, mask=mask)\n",
      "   569                                                       critic_self_embeds = self_embeds\n",
      "   570       473          0.0      0.0      0.0          else: critic_self_embeds = actor_self_embeds\n",
      "   571                                           \n",
      "   572                                                   # NOTE: fit_and_strip breaks compatibility on batch-batch native programs (Grouped batches)\n",
      "   573       473          0.0      0.0      0.1          if fit_and_strip and self_entities.dim() > 2:\n",
      "   574                                                       # Strip padded entries with workaround\n",
      "   575                                                       if actor: actor_self_embeds = actor_self_embeds[include_mask]\n",
      "   576                                                       if critic: critic_self_embeds = critic_self_embeds[include_mask]\n",
      "   577                                                       # # Strip padded entries\n",
      "   578                                                       # actor_self_embeds = actor_self_embeds[~actor_self_embeds.sum(dim=-1).isnan()]\n",
      "   579                                                       # if critic: critic_self_embeds = critic_self_embeds[~critic_self_embeds.sum(dim=-1).isnan()]\n",
      "   580                                           \n",
      "   581                                                   # Decisions, samples, and returns\n",
      "   582       473          0.0      0.0      0.0          ret = ()\n",
      "   583       473          0.0      0.0      0.0          if actor:  # action_means\n",
      "   584                                                       # Dot Method\n",
      "   585                                                       # self_action_embeds = self.actor_decider(actor_self_embeds)\n",
      "   586                                                       # action_embeds = self.action_embed(self.actions)\n",
      "   587                                                       # # Norms\n",
      "   588                                                       # # NOTE: Norm causes NAN, could use eps but might as well remove to avoid vanish\n",
      "   589                                                       # self_action_embeds = self_action_embeds  / (self_action_embeds.norm(p=2, keepdim=True, dim=-1) + 1e-8)\n",
      "   590                                                       # action_embeds = action_embeds   / (action_embeds.norm(p=2, keepdim=True, dim=-1) + 1e-8)\n",
      "   591                                                       # # Dot/cosine\n",
      "   592                                                       # if self_action_embeds.dim() > 2:\n",
      "   593                                                       #     action_means = torch.einsum('bik,jk->bij', self_action_embeds, action_embeds)\n",
      "   594                                                       # else: action_means = torch.einsum('ik,jk->ij', self_action_embeds, action_embeds)\n",
      "   595                                           \n",
      "   596                                                       # Regular method\n",
      "   597       470          0.3      0.0     33.1              ret += self.actor_decider(actor_self_embeds, action=action, return_entropy=entropy)  # action, action_log, dist_entropy\n",
      "   598                                                       \n",
      "   599                                                       # Magnitude method\n",
      "   600                                                       # action_direction = self.actor_decider_direction(actor_self_embeds)\n",
      "   601                                                       # action_direction = action_direction / action_direction.norm(keepdim=True, dim=-1)  # Get direction unit\n",
      "   602                                                       # action_magnitude = self.actor_decider_magnitude(actor_self_embeds)\n",
      "   603                                                       # action_means = action_magnitude * action_direction\n",
      "   604                                                       # # action_direction = np.sqrt(action_direction.shape[-1]) * action_direction / action_direction.std(keepdim=True, dim=-1)  # Get into comparable range for sampling\n",
      "   605                                                       # # action_means = torch.concat((action_direction, action_magnitude), dim=-1)\n",
      "   606       473          0.0      0.0      4.7          if critic: ret += (self.critic_decider(critic_self_embeds).squeeze(-1),)  # state_vals\n",
      "   607       473          0.0      0.0      0.1          if squeeze and self_entities.dim() > 2 and not fit_and_strip:\n",
      "   608                                                       ret = tuple(t.flatten(0, 1) for t in ret)\n",
      "   609       473          0.0      0.0      0.0          if return_feature_embeds: ret += feature_embeds_ret,\n",
      "   610       473          0.0      0.0      0.0          return ret\n",
      "\n",
      "Total time: 1.02776 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: forward at line 1144\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "  1144                                               def forward(\n",
      "  1145                                                   self, compressed_state, *,\n",
      "  1146                                                   keys=None, memory=None, forward_batch_size=None, terminal=False,\n",
      "  1147                                                   feature_embeds=None, return_feature_embeds=False):\n",
      "  1148                                                   # NOTE: `feature_embeds` will not re-randomize vision if applicable to `split_state` (i.e. do not use non-lite model with vision culling and feature embed caching)\n",
      "  1149                                                   # Data Checks\n",
      "  1150       473          0.0      0.0      0.1          assert compressed_state.shape[0] > 0, 'Empty state matrix passed'\n",
      "  1151       473          0.0      0.0      0.0          if keys is not None: assert len(keys) == compressed_state.shape[0], (\n",
      "  1152                                                       f'Length of keys vector must equal state dimension 0 ({compressed_state.shape[0]}), '\n",
      "  1153                                                       f'got {len(keys)} instead.'\n",
      "  1154                                                   )\n",
      "  1155                                                       \n",
      "  1156                                                   # Defaults\n",
      "  1157       473          0.0      0.0      0.0          if forward_batch_size is None: forward_batch_size = self.forward_batch_size\n",
      "  1158       473          0.0      0.0      0.0          feature_embeds_arg = feature_embeds\n",
      "  1159       473          0.0      0.0      0.0          construct_feature_embeds = feature_embeds is None and return_feature_embeds\n",
      "  1160                                           \n",
      "  1161                                                   # Act\n",
      "  1162       473          0.0      0.0      0.7          action = torch.zeros(0, device=self.policy_iteration.device)\n",
      "  1163       473          0.0      0.0      0.3          action_log = torch.zeros(0, device=self.policy_iteration.device)\n",
      "  1164       473          0.0      0.0      0.3          state_val = torch.zeros(0, device=self.policy_iteration.device)\n",
      "  1165       946          0.0      0.0      0.1          for start_idx in range(0, compressed_state.shape[0], forward_batch_size):\n",
      "  1166       473          0.0      0.0      0.2              self_idx = np.arange(start_idx, min(start_idx+forward_batch_size, compressed_state.shape[0]))\n",
      "  1167      1892          0.0      0.0      0.9              state = _utility.processing.split_state(\n",
      "  1168       473          0.0      0.0      2.3                  self.input_standardization.apply(compressed_state),\n",
      "  1169       473          0.0      0.0      0.0                  idx=self_idx,\n",
      "  1170       473          0.0      0.0      0.0                  **self.split_args)\n",
      "  1171       473          0.0      0.0      2.4              feature_embeds_arg_use = [(sfe[self_idx], nfe) for sfe, nfe in feature_embeds_arg] if feature_embeds_arg is not None else None\n",
      "  1172       473          0.0      0.0      0.0              if not terminal:\n",
      "  1173       940          0.8      0.0     80.1                  action_sub, action_log_sub, state_val_sub, feature_embeds_sub = self.actor_critic(\n",
      "  1174       470          0.0      0.0      0.0                      *state, critic=True, feature_embeds=feature_embeds_arg_use, return_feature_embeds=True)\n",
      "  1175       470          0.0      0.0      0.9                  action = torch.concat((action, action_sub), dim=0)\n",
      "  1176       470          0.0      0.0      0.6                  action_log = torch.concat((action_log, action_log_sub), dim=0)\n",
      "  1177         6          0.0      0.0      0.3              else: state_val_sub, feature_embeds_sub = self.actor_critic(\n",
      "  1178         3          0.0      0.0      0.0                  *state, actor=False, critic=True, feature_embeds=feature_embeds_arg_use, return_feature_embeds=True)\n",
      "  1179       473          0.0      0.0      0.5              state_val = torch.concat((state_val, state_val_sub), dim=0)\n",
      "  1180       473          0.0      0.0      0.0              if construct_feature_embeds:\n",
      "  1181         3          0.0      0.0      0.0                  if feature_embeds is None: feature_embeds = feature_embeds_sub\n",
      "  1182                                                           else:\n",
      "  1183                                                               # feature_embeds = [\n",
      "  1184                                                               #     tuple(torch.concat((feature_embeds[i][j], t)) for j, t in enumerate(feat_tensors))\n",
      "  1185                                                               #     for i, feat_tensors in enumerate(feature_embeds_sub)]\n",
      "  1186                                                               for i in range(len(feature_embeds)):\n",
      "  1187                                                                   # NOTE: 0 is the self embeddings, which are the only ones subset in the Lite model\n",
      "  1188                                                                   # feature_embeds[i][0] = torch.concat((feature_embeds[i][0], feature_embeds_sub[i][0]))\n",
      "  1189                                                                   assert (feature_embeds[i][1] == feature_embeds_sub[i][1]).all(), 'Unexpected node embedding changes, make sure no vision culling is occurring.'\n",
      "  1190                                                                   feature_embeds[i] = (torch.concat((feature_embeds[i][0], feature_embeds_sub[i][0])), feature_embeds[i][1])\n",
      "  1191                                           \n",
      "  1192                                                   # Unstandardize state_val\n",
      "  1193       473          0.0      0.0      2.2          state_val = self.return_standardization.remove(state_val)\n",
      "  1194                                                   \n",
      "  1195                                                   # Record\n",
      "  1196                                                   # NOTE: `reward` and `is_terminal` are added outside of the class, calculated\n",
      "  1197                                                   # after stepping the environment\n",
      "  1198       473          0.0      0.0      0.0          if memory is not None and keys is not None:  #  and self.training\n",
      "  1199       473          0.0      0.0      0.0              if not terminal:\n",
      "  1200       940          0.1      0.0      7.9                  memory.record_buffer(\n",
      "  1201       470          0.0      0.0      0.0                      keys=keys,\n",
      "  1202       470          0.0      0.0      0.0                      states=compressed_state,\n",
      "  1203       470          0.0      0.0      0.0                      actions=action,\n",
      "  1204       470          0.0      0.0      0.0                      action_logs=action_log,\n",
      "  1205       470          0.0      0.0      0.0                      state_vals=state_val)\n",
      "  1206                                                       else:\n",
      "  1207         6          0.0      0.0      0.1                  memory.record_buffer(\n",
      "  1208         3          0.0      0.0      0.0                      terminal_states=compressed_state,\n",
      "  1209         3          0.0      0.0      0.0                      terminal_state_vals=state_val)\n",
      "  1210       473          0.0      0.0      0.0          ret = ()\n",
      "  1211       473          0.0      0.0      0.0          if not terminal: ret += action,\n",
      "  1212       473          0.0      0.0      0.0          if return_feature_embeds: ret += feature_embeds,\n",
      "  1213                                                   # print(action)\n",
      "  1214       473          0.0      0.0      0.1          return _utility.general.clean_return(ret)\n",
      "\n",
      "Total time: 2.24259 s\n",
      "File: /home/thema/repos/inept/celltrip/train.py\n",
      "Function: simulate_until_completion at line 20\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    20                                           def simulate_until_completion(\n",
      "    21                                               env, policy, memory=None, keys=None,\n",
      "    22                                               max_timesteps=np.inf, max_memories=np.inf, reset_on_finish=False,\n",
      "    23                                               cache_feature_embeds=True, store_states=False, flush=True,\n",
      "    24                                               dummy=False, verbose=False):\n",
      "    25                                               # NOTE: Does not flush buffer\n",
      "    26                                               # Params\n",
      "    27         1          0.0      0.0      0.0      assert not (keys is not None and reset_on_finish), 'Cannot manually set keys while `reset_on_finish` is `True`'\n",
      "    28         1          0.0      0.0      0.0      if keys is None: keys = env.get_keys()\n",
      "    29         1          0.0      0.0      0.2      target_modalities = torch.concat(env.get_target_modalities(), dim=-1)\n",
      "    30                                           \n",
      "    31                                               # Store states\n",
      "    32         1          0.0      0.0      0.0      if store_states: state_storage = [env.get_state()]\n",
      "    33                                           \n",
      "    34                                               # Simulation\n",
      "    35         1          0.0      0.0      0.0      ep_timestep = 0; ep_memories = 0; ep_reward = 0; ep_itemized_reward = defaultdict(lambda: 0); finished = False\n",
      "    36         1          0.0      0.0      0.0      feature_embeds = None\n",
      "    37         2          0.0      0.0      0.0      with torch.inference_mode():\n",
      "    38       470          0.0      0.0      0.0          while True:\n",
      "    39                                                       # Get current state\n",
      "    40       470          0.0      0.0      0.7              state = env.get_state(include_modalities=True)\n",
      "    41                                           \n",
      "    42                                                       # Get actions from policy\n",
      "    43       940          1.0      0.0     46.3              actions = policy(\n",
      "    44       470          0.0      0.0      0.0                  state, keys=keys, memory=memory,\n",
      "    45       470          0.0      0.0      0.0                  feature_embeds=feature_embeds, return_feature_embeds=cache_feature_embeds)\n",
      "    46       470          0.0      0.0      0.0              if cache_feature_embeds: actions, feature_embeds = actions\n",
      "    47                                           \n",
      "    48                                                       # Step environment and get reward\n",
      "    49       470          0.9      0.0     38.4              rewards, finished, itemized_rewards = env.step(actions, return_itemized_rewards=True, pinning_func_list=policy.pinning)\n",
      "    50                                           \n",
      "    51                                                       # Store states\n",
      "    52       470          0.0      0.0      0.0              if store_states: state_storage.append(env.get_state())\n",
      "    53                                           \n",
      "    54                                                       # Tracking\n",
      "    55       470          0.0      0.0      0.8              ts_reward = rewards.cpu().mean()\n",
      "    56       470          0.0      0.0      0.1              ep_reward = ep_reward + ts_reward\n",
      "    57      3290          0.0      0.0      0.1              for k, v in itemized_rewards.items():\n",
      "    58      2820          0.1      0.0      3.5                  ep_itemized_reward[k] += v.cpu().mean()\n",
      "    59       470          0.0      0.0      0.0              ep_timestep += 1\n",
      "    60       470          0.0      0.0      0.0              ep_memories += env.num_nodes\n",
      "    61                                           \n",
      "    62                                                       # Record rewards\n",
      "    63       470          0.0      0.0      0.0              continue_condition = ep_timestep < max_timesteps\n",
      "    64       470          0.0      0.0      0.0              if memory is not None: continue_condition *= ep_memories < max_memories\n",
      "    65       940          0.1      0.0      5.7              if memory is not None: memory.record_buffer(\n",
      "    66       470          0.0      0.0      0.0                  rewards=rewards, target_modalities=target_modalities,\n",
      "    67       470          0.0      0.0      0.0                  is_truncateds=finished or not continue_condition,\n",
      "    68       470          0.0      0.0      0.0                  is_naturals=finished)\n",
      "    69                                           \n",
      "    70                                                       # Dummy return for testing\n",
      "    71                                                       # if dummy:\n",
      "    72                                                       #     if not finished:\n",
      "    73                                                       #         # Fill all but first and last\n",
      "    74                                                       #         ep_timestep = env.max_timesteps\n",
      "    75                                                       #         memory.flush_buffer()\n",
      "    76                                                       #         for _ in range(ep_timestep-2):\n",
      "    77                                                       #             if memory is not None: memory.append_memory({k: v[-1:] for k, v in memory.storage.items()})\n",
      "    78                                                       #             if store_states: state_storage.append(env.get_state)\n",
      "    79                                                       #         memory.storage['is_terminals'][-1] = True\n",
      "    80                                                   \n",
      "    81                                                       # CLI\n",
      "    82       470          0.0      0.0      0.0              if verbose and ((ep_timestep % 200 == 0) or ep_timestep in (100,) or finished):\n",
      "    83                                                           print(f'Timestep {ep_timestep:>4} - Reward {ts_reward:.3f}')\n",
      "    84                                                   \n",
      "    85                                                       # Record terminals and reset if needed\n",
      "    86       470          0.0      0.0      0.0              if finished or not continue_condition:\n",
      "    87         3          0.0      0.0      0.0                  state = env.get_state(include_modalities=True)\n",
      "    88         3          0.0      0.0      0.2                  policy(state, keys=keys, memory=memory, terminal=True, feature_embeds=feature_embeds)\n",
      "    89                                                       # Reset if needed\n",
      "    90       470          0.0      0.0      0.0              if finished:\n",
      "    91         2          0.0      0.0      0.0                  if reset_on_finish:\n",
      "    92         2          0.1      0.0      3.9                      env.reset()\n",
      "    93         2          0.0      0.0      0.0                      keys = env.get_keys()\n",
      "    94         2          0.0      0.0      0.0                      target_modalities = torch.concat(env.get_target_modalities(), dim=-1)\n",
      "    95         2          0.0      0.0      0.0                      feature_embeds = None\n",
      "    96                                                           else: break\n",
      "    97                                                       # Escape\n",
      "    98       470          0.0      0.0      0.0              if not continue_condition: break\n",
      "    99                                           \n",
      "   100                                               # Flush\n",
      "   101         1          0.0      0.0      0.0      if flush and memory: memory.flush_buffer()\n",
      "   102                                           \n",
      "   103                                               # Summarize and return\n",
      "   104         1          0.0      0.0      0.0      denominator = 1  # env.max_timesteps if env.max_timesteps is not None else ep_timestep\n",
      "   105         1          0.0      0.0      0.0      ep_reward = (ep_reward / denominator).item()  # Standard mean\n",
      "   106         1          0.0      0.0      0.0      ep_itemized_reward = {k: (v / denominator).item() for k, v in ep_itemized_reward.items()}\n",
      "   107         1          0.0      0.0      0.0      ret = (ep_timestep, ep_memories, ep_reward, ep_itemized_reward)\n",
      "   108         1          0.0      0.0      0.0      if store_states:\n",
      "   109                                                   state_storage = torch.stack(state_storage)\n",
      "   110                                                   ret += (state_storage,)\n",
      "   111         1          0.0      0.0      0.0      return ret\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Forward\n",
    "# import line_profiler\n",
    "# memory.mark_sampled()\n",
    "# memory.cleanup()\n",
    "# prof = line_profiler.LineProfiler(\n",
    "#     celltrip.train.simulate_until_completion,\n",
    "#     celltrip.policy.PPO.forward,\n",
    "#     celltrip.policy.EntitySelfAttentionLite.forward,\n",
    "#     celltrip.policy.ResidualAttention.forward,\n",
    "#     celltrip.environment.EnvironmentBase.step)\n",
    "# ret = prof.runcall(celltrip.train.simulate_until_completion, env, policy, memory, max_memories=config.update_timesteps, reset_on_finish=True)\n",
    "# print('ROLLOUT: ' + f'total: {ret[2]:.3f}, ' + ', '.join([f'{k}: {v:.3f}' for k, v in ret[3].items()]))\n",
    "# # memory.feed_new(policy.reward_standardization)\n",
    "# memory.compute_advantages()  # moving_standardization=policy.reward_standardization\n",
    "# prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Memory pull\n",
    "# import line_profiler\n",
    "# prof = line_profiler.LineProfiler(\n",
    "#     celltrip.memory.AdvancedMemoryBuffer.__getitem__)\n",
    "# ret = prof.runcall(memory.__getitem__, np.random.choice(len(memory), 10_000, replace=False))\n",
    "# memory.compute_advantages()\n",
    "# prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 01 - Total (1.78284) + PPO (0.00354) + critic (1.79071) + entropy (-11.41106) + KL (0.00561) :: Moving Return Mean (-6.02726), Moving Return STD (15.58578), Return Mean (-75.90559), Return STD (23.96271), Moving Input Mean (0.05253), Moving Input STD (2.47622), Input Mean (0.38201), Input STD (14.43343), Explained Variance (0.84414)\n",
      "Iteration 05 - Total (0.39777) + PPO (0.03785) + critic (0.37137) + entropy (-11.44544) + KL (0.06093) :: Moving Return Mean (-13.94043), Moving Return STD (27.79181), Return Mean (-75.90774), Return STD (23.96801), Moving Input Mean (0.08994), Moving Input STD (3.08722), Input Mean (0.38017), Input STD (14.40699), Explained Variance (0.56307)\n",
      "426\n",
      "UPDATE: Total: 0.832, PPO: 0.025, critic: 0.818, entropy: -11.431, KL: 0.039, Pinning Loss 0: 762.422, Moving Return Mean: -10.043, Moving Return STD: 22.386, Return Mean: -75.906, Return STD: 23.964, Moving Input Mean: 0.072, Moving Input STD: 2.798, Input Mean: 0.382, Input STD: 14.439, Explained Variance: 0.753\n",
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 0.563426 s\n",
      "File: /home/thema/repos/inept/celltrip/memory.py\n",
      "Function: __getitem__ at line 197\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   197                                               def __getitem__(self, idx):\n",
      "   198                                                   # Params\n",
      "   199        50          0.0      0.0      0.0          pre_append = True  # Append node features once during concatenation stage then never again\n",
      "   200        50          0.0      0.0      0.0          pre_cast = True  # Cast to tensor once during concatenation stage and never again\n",
      "   201        50          0.0      0.0      0.0          pad_nodes = True  # Pad node array to avoid indexing\n",
      "   202        50          0.0      0.0      0.0          general_storage_keys = ('actions', 'action_logs', 'state_vals', 'advantages', 'propagated_rewards')\n",
      "   203                                               \n",
      "   204                                                   # Pre concatenate vars\n",
      "   205        50          0.0      0.0      0.0          if 'concatenated_buffer' not in self.variable_storage:\n",
      "   206         1          0.0      0.0      0.0              self.variable_storage['concatenated_buffer'] = {}\n",
      "   207                                                       # General case\n",
      "   208         6          0.0      0.0      0.0              for k in general_storage_keys:\n",
      "   209         5          0.0      0.0      0.2                  self.variable_storage['concatenated_buffer'][k] = np.concat(self.storage[k], axis=0)\n",
      "   210                                                       # Get key indices\n",
      "   211         1          0.0      0.0      1.8              self.variable_storage['concatenated_buffer']['keys'] = np.concat(self.storage['keys'])\n",
      "   212         1          0.1      0.1     14.1              unique_keys, new_suffix_idx = np.unique(self.variable_storage['concatenated_buffer']['keys'], return_inverse=True, axis=-1)\n",
      "   213         1          0.0      0.0      0.0              try:\n",
      "   214         3          0.0      0.0      0.1                  self.variable_storage['concatenated_buffer']['suffixes'] = np.stack([\n",
      "   215         2          0.0      0.0      0.0                      self.persistent_storage['suffixes'][k] for k in unique_keys], axis=0)\n",
      "   216                                                       except:\n",
      "   217                                                           print(unique_keys)\n",
      "   218                                                           print(self.persistent_storage.keys())\n",
      "   219                                                           raise RuntimeError('Something went wrong with suffix concatenation.')\n",
      "   220         1          0.0      0.0      0.0              self.variable_storage['concatenated_buffer']['idx_to_suffix'] = new_suffix_idx\n",
      "   221         1          0.0      0.0      0.0              state_lens = np.array(list(map(len, self.storage['keys'])))\n",
      "   222         1          0.0      0.0      0.0              suffix_keys_padded = -np.ones((state_lens.shape[0], state_lens.max()), dtype=int)\n",
      "   223         1          0.0      0.0      0.0              suffix_keys_padded_mask = np.expand_dims(np.arange(state_lens.max()), axis=0) < np.expand_dims(state_lens, axis=-1)\n",
      "   224         1          0.0      0.0      0.0              suffix_keys_padded[suffix_keys_padded_mask] = new_suffix_idx\n",
      "   225         1          0.0      0.0      0.0              self.variable_storage['concatenated_buffer']['suffix_indices'] = suffix_keys_padded\n",
      "   226                                                       # Get state indices\n",
      "   227         1          0.0      0.0      2.0              mask = _utility.general.padded_stack(list(map(lambda x: np.ones_like(x, dtype=bool), self.storage['keys'])), method='false')\n",
      "   228         1          0.0      0.0      0.0              grid = np.meshgrid(*[np.arange(s) for s in mask.shape], indexing='ij')\n",
      "   229         1          0.0      0.0      0.1              self.variable_storage['concatenated_buffer']['idx_to_buffer'] = np.stack([arr[mask] for arr in grid], axis=-1)  # TODO: Try on variable-sized envs, should work\n",
      "   230                                                       # Concatenate states\n",
      "   231         1          0.0      0.0      4.4              if pre_append: states = [self._append_suffix(s, keys=k) for k, s in zip(self.storage['keys'], self.storage['states'])]\n",
      "   232                                                       else: states = self.storage['states']\n",
      "   233         1          0.1      0.1     11.5              s01 = _utility.general.padded_stack(states, method='zero')\n",
      "   234         1          0.0      0.0      2.9              s2 =  _utility.general.padded_stack(list(map(lambda x: np.eye(x.shape[0], dtype=bool), states)), method='true')\n",
      "   235         1          0.0      0.0      0.0              if pre_cast:\n",
      "   236         1          0.0      0.0      5.2                  s01 = torch.from_numpy(s01).to(torch.get_default_dtype()).to(self.device)  # Cast to tensor early to avoid repeated casting cost\n",
      "   237         1          0.0      0.0      0.0                  s2 = torch.from_numpy(s2).to(torch.bool).to(self.device)\n",
      "   238         1          0.0      0.0      0.0              states = [s01, s01, s2]\n",
      "   239         1          0.0      0.0      0.0              self.variable_storage['concatenated_buffer']['states'] = states\n",
      "   240         1          0.0      0.0      0.0              self.out = None\n",
      "   241                                           \n",
      "   242                                                   # Automatically determine idx if int\n",
      "   243        50          0.0      0.0      0.0          if isinstance(idx, int):\n",
      "   244                                                       idx = [idx]\n",
      "   245                                                       # TODO: Make clustered version of this\n",
      "   246        50          0.0      0.0      1.4          idx = np.sort(np.array(idx))\n",
      "   247                                                   # return self.fast_sample(chosen=idx)\n",
      "   248                                           \n",
      "   249                                                   # Get indices to padded buffer from idx\n",
      "   250        50          0.0      0.0      2.2          buffer_idx = self.variable_storage['concatenated_buffer']['idx_to_buffer'][idx]\n",
      "   251        50          0.0      0.0      1.8          unique_list_nums, unique_list_counts = np.unique(buffer_idx[:, 0], return_counts=True)\n",
      "   252        50          0.0      0.0      0.0          if pad_nodes:\n",
      "   253        50          0.0      0.0      0.0              size = self.variable_storage['concatenated_buffer']['states'][1].shape[0]\n",
      "   254        50          0.0      0.0      0.0              new_unique_list_nums = np.arange(size, dtype=int)\n",
      "   255        50          0.0      0.0      0.8              mask = np.isin(new_unique_list_nums, unique_list_nums)\n",
      "   256        50          0.0      0.0      0.0              new_unique_list_counts = np.zeros(size, dtype=int)\n",
      "   257        50          0.0      0.0      0.0              new_unique_list_counts[mask] = unique_list_counts\n",
      "   258        50          0.0      0.0      0.0              unique_list_nums = new_unique_list_nums\n",
      "   259        50          0.0      0.0      0.0              unique_list_counts = new_unique_list_counts\n",
      "   260        50          0.0      0.0      0.3          grouped_indices = -np.ones((unique_list_nums.shape[0], unique_list_counts.max()), dtype=int)\n",
      "   261        50          0.0      0.0      0.5          grouped_sample_mask = np.expand_dims(np.arange(unique_list_counts.max()), axis=0) < np.expand_dims(unique_list_counts, axis=-1)\n",
      "   262        50          0.0      0.0      0.2          grouped_indices[grouped_sample_mask] = buffer_idx[:, 1]  # np.argsort(unique_list_inverse) doesn't respect initial ordering\n",
      "   263        50          0.0      0.0      0.1          overall_indices = grouped_indices.copy()\n",
      "   264        50          0.0      0.0      0.2          overall_indices[grouped_sample_mask] = idx\n",
      "   265                                           \n",
      "   266                                                   # Format output\n",
      "   267                                                   # feature_dim = self.storage['states'][0].shape[-1] + self.suffix_len\n",
      "   268                                                   # if self.out is None:\n",
      "   269                                                   #     self.out = [\n",
      "   270                                                   #         np.zeros((*grouped_indices.shape, feature_dim)),\n",
      "   271                                                   #         np.zeros((grouped_indices.shape[0], self.variable_storage['concatenated_buffer']['states'][1].shape[1], feature_dim)),\n",
      "   272                                                   #         np.zeros((*grouped_indices.shape, self.variable_storage['concatenated_buffer']['states'][1].shape[1])),\n",
      "   273                                                   #     ]\n",
      "   274                                                   # states = self.out\n",
      "   275                                                   # Format states\n",
      "   276                                                   # states[0][..., :-self.suffix_len] = self.variable_storage['concatenated_buffer']['states'][0][np.expand_dims(unique_list_nums, axis=-1), grouped_indices]\n",
      "   277                                                   # states[1][..., :-self.suffix_len] = self.variable_storage['concatenated_buffer']['states'][1][unique_list_nums]\n",
      "   278                                                   # states[2][:] = self.variable_storage['concatenated_buffer']['states'][2][np.expand_dims(unique_list_nums, axis=-1), grouped_indices]\n",
      "   279                                           \n",
      "   280                                                   # Format states\n",
      "   281        50          0.0      0.0      0.0          states = [\n",
      "   282        50          0.1      0.0     26.0              self.variable_storage['concatenated_buffer']['states'][0][np.expand_dims(unique_list_nums, axis=-1), grouped_indices],\n",
      "   283        50          0.0      0.0      0.3              self.variable_storage['concatenated_buffer']['states'][1][unique_list_nums if not pad_nodes else slice(unique_list_nums.shape[0])],\n",
      "   284        50          0.0      0.0      7.5              self.variable_storage['concatenated_buffer']['states'][2][np.expand_dims(unique_list_nums, axis=-1), grouped_indices]]\n",
      "   285        50          0.0      0.0      4.2          if pre_cast: states[2] = states[2].clone()\n",
      "   286                                                   else: states[2] = states[2].copy()\n",
      "   287                                                   # Post-append\n",
      "   288        50          0.0      0.0      0.0          if not pre_append:\n",
      "   289                                                       suffix_idx = self.variable_storage['concatenated_buffer']['idx_to_suffix'][idx]\n",
      "   290                                                       suffix_grouped_indices = grouped_indices.copy()\n",
      "   291                                                       suffix_grouped_indices[grouped_sample_mask] = suffix_idx\n",
      "   292                                                       # states[0][..., -self.suffix_len:] = self.variable_storage['concatenated_buffer']['suffixes'][suffix_grouped_indices.flatten()].reshape((*suffix_grouped_indices.shape, -1))\n",
      "   293                                                       # states[1][..., -self.suffix_len:] = self.variable_storage['concatenated_buffer']['suffixes'][self.variable_storage['concatenated_buffer']['suffix_indices'][unique_list_nums]]\n",
      "   294                                                       s0_suffix = self.variable_storage['concatenated_buffer']['suffixes'][suffix_grouped_indices.flatten()].reshape((*suffix_grouped_indices.shape, -1))\n",
      "   295                                                       s1_suffix = self.variable_storage['concatenated_buffer']['suffixes'][self.variable_storage['concatenated_buffer']['suffix_indices'][unique_list_nums]]\n",
      "   296                                                       # states[0] = np.concatenate((states[0], s0_suffix), axis=-1)\n",
      "   297                                                       # states[1] = np.concatenate((states[1], s1_suffix), axis=-1)\n",
      "   298                                                   # Mask ragged fillers, uses -1 as indicator\n",
      "   299        50          0.0      0.0      5.5          states[2][tuple(np.argwhere(grouped_indices==-1).T)] = True\n",
      "   300                                           \n",
      "   301                                                   # Pull from padded buffer and cast to tensors\n",
      "   302        50          0.0      0.0      0.0          ret = {}\n",
      "   303                                                   # General case\n",
      "   304       300          0.0      0.0      0.0          for k in general_storage_keys:\n",
      "   305       250          0.0      0.0      6.4              ret[k] = torch.from_numpy(self.variable_storage['concatenated_buffer'][k][idx]).to(torch.get_default_dtype()).to(self.device)\n",
      "   306                                                   # State case\n",
      "   307        50          0.0      0.0      0.0          if pre_cast: ret['states'] = states\n",
      "   308                                                   else:\n",
      "   309                                                       ret['states'] = [\n",
      "   310                                                           torch.from_numpy(states[0]).to(torch.get_default_dtype()).to(self.device),\n",
      "   311                                                           torch.from_numpy(states[1]).to(torch.get_default_dtype()).to(self.device),\n",
      "   312                                                           torch.from_numpy(states[2]).to(torch.bool).to(self.device)]\n",
      "   313        50          0.0      0.0      0.0          return overall_indices, ret  # indices, data\n",
      "\n",
      "Total time: 0.0463708 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: forward at line 392\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   392                                               def forward(self, x, kv=None, mask=None):\n",
      "   393                                                   # Parameters\n",
      "   394        50          0.0      0.0      0.1          if kv is None: x = kv\n",
      "   395                                           \n",
      "   396                                                   # Residual Attention\n",
      "   397        50          0.0      0.0      6.9          x1 = self.norms[0](x)\n",
      "   398        50          0.0      0.0      4.4          kv1 = self.norms[1](kv)\n",
      "   399                                                   # kv1 = kv\n",
      "   400        50          0.0      0.0     64.8          x2, _ = self.attention(x1, kv1, kv1, attn_mask=mask)\n",
      "   401        50          0.0      0.0      1.8          x = x + x2\n",
      "   402        50          0.0      0.0      5.4          x1 = self.norms[2](x)\n",
      "   403        50          0.0      0.0     14.7          x2 = self.mlp(x1)\n",
      "   404        50          0.0      0.0      1.7          x = x + x2\n",
      "   405        50          0.0      0.0      0.0          return x\n",
      "\n",
      "Total time: 4.01907 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: update at line 1237\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "  1237                                               def update(\n",
      "  1238                                                   self,\n",
      "  1239                                                   memory,\n",
      "  1240                                                   update_iterations=None,\n",
      "  1241                                                   standardize_inputs=True,\n",
      "  1242                                                   standardize_returns=True,\n",
      "  1243                                                   verbose=False,\n",
      "  1244                                                   # Collective args\n",
      "  1245                                                   sync_iterations=None,\n",
      "  1246                                                   **kwargs,\n",
      "  1247                                               ):\n",
      "  1248                                                   # NOTE: The number of epochs is spread across `world_size` workers\n",
      "  1249                                                   # NOTE: Assumes col.init_collective_group has already been called if world_size > 1\n",
      "  1250                                                   # Parameters\n",
      "  1251         1          0.0      0.0      0.0          if update_iterations is None: update_iterations = self.update_iterations\n",
      "  1252         1          0.0      0.0      0.0          if sync_iterations is None: sync_iterations = self.sync_iterations\n",
      "  1253                                           \n",
      "  1254                                                   # Collective operations\n",
      "  1255         1          0.0      0.0      0.0          use_collective = col.is_group_initialized('default')\n",
      "  1256                                           \n",
      "  1257                                                   # Batch parameters\n",
      "  1258                                                   # level_dict = {'pool': 0, 'epoch': 1, 'batch': 2, 'minibatch': 3}\n",
      "  1259                                                   # load_level = level_dict[self.load_level]\n",
      "  1260                                                   # cast_level = level_dict[self.cast_level]\n",
      "  1261                                                   # assert cast_level >= load_level, 'Cannot cast without first loading'\n",
      "  1262                                           \n",
      "  1263                                                   # Determine level sizes\n",
      "  1264         1          0.0      0.0      0.0          memory_size = len(memory)\n",
      "  1265                                                   \n",
      "  1266                                                   # Pool size\n",
      "  1267         1          0.0      0.0      0.0          pool_size = self.pool_size\n",
      "  1268                                                   # if pool_size is not None:\n",
      "  1269         1          0.0      0.0      0.0          pool_size = min(pool_size, memory_size)\n",
      "  1270                                                   \n",
      "  1271                                                   # Epoch size\n",
      "  1272         1          0.0      0.0      0.0          epoch_size = self.epoch_size if not (self.epoch_size is None and pool_size is not None) else pool_size\n",
      "  1273                                                   # if epoch_size is not None and pool_size is not None:\n",
      "  1274         1          0.0      0.0      0.0          epoch_size = int(min(epoch_size, pool_size))\n",
      "  1275                                                   \n",
      "  1276                                                   # Batch size\n",
      "  1277         1          0.0      0.0      0.0          batch_size = self.batch_size if not (self.batch_size is None and epoch_size is not None) else epoch_size\n",
      "  1278                                                   # if batch_size is not None and epoch_size is not None:\n",
      "  1279         1          0.0      0.0      0.0          batch_size = int(min(batch_size, epoch_size))\n",
      "  1280                                           \n",
      "  1281                                                   # Level adjustment\n",
      "  1282         1          0.0      0.0      0.0          denominator = get_world_size('learners') if sync_iterations == 1 else 1  # Adjust sizes if gradients synchronized across GPUs\n",
      "  1283                                                   # if epoch_size is not None:\n",
      "  1284         1          0.0      0.0      0.0          if not np.isinf(self.epoch_size): epoch_size = np.ceil(epoch_size / denominator).astype(int)\n",
      "  1285                                                   # if batch_size is not None:\n",
      "  1286         1          0.0      0.0      0.0          if not np.isinf(self.batch_size): batch_size = np.ceil(batch_size / denominator).astype(int)\n",
      "  1287                                           \n",
      "  1288                                                   # Minibatch size\n",
      "  1289         1          0.0      0.0      0.0          minibatch_size = self.minibatch_size if not (self.minibatch_size is None and batch_size is not None) else batch_size\n",
      "  1290         1          0.0      0.0      0.0          if minibatch_size is not None and batch_size is not None: minibatch_size = int(min(minibatch_size, batch_size))\n",
      "  1291                                           \n",
      "  1292                                                   # Load pool\n",
      "  1293         1          0.0      0.0      0.0          total_losses = defaultdict(lambda: [])\n",
      "  1294         1          0.0      0.0      0.0          total_statistics = defaultdict(lambda: [])\n",
      "  1295         1          0.0      0.0      0.0          pool_idx = np.random.choice(memory_size, pool_size, replace=False) if pool_size < memory_size else memory_size\n",
      "  1296                                           \n",
      "  1297                                                   # Train\n",
      "  1298         1          0.0      0.0      0.0          iterations = 0; synchronized = True; escape = False\n",
      "  1299         5          0.0      0.0      0.0          while True:\n",
      "  1300                                                       # Load epoch\n",
      "  1301         5          0.0      0.0      0.2              epoch_idx = np.random.choice(pool_idx, epoch_size, replace=False)  # Also shuffles\n",
      "  1302         5          0.0      0.0      0.0              batches = np.floor(epoch_size/batch_size).astype(int) if epoch_size is not None else 1  # Drop any smaller batches\n",
      "  1303        55          0.0      0.0      0.0              for batch_num in range(batches):\n",
      "  1304                                                           # Load batch\n",
      "  1305        50          0.0      0.0      0.0                  batch_losses = defaultdict(lambda: 0)\n",
      "  1306        50          0.0      0.0      0.0                  batch_statistics = defaultdict(lambda: 0)\n",
      "  1307        50          0.0      0.0      0.0                  batch_idx = epoch_idx[batch_num*batch_size:(batch_num+1)*batch_size]\n",
      "  1308        50          0.0      0.0      0.0                  batch_state_vals_new = torch.zeros(0, device=self.policy_iteration.device)\n",
      "  1309        50          0.0      0.0      0.0                  batch_inputs = torch.zeros(0, device=self.policy_iteration.device)\n",
      "  1310        50          0.0      0.0      0.0                  batch_returns = torch.zeros(0, device=self.policy_iteration.device)\n",
      "  1311        50          0.0      0.0      0.0                  minibatches = np.ceil(batch_size/minibatch_size).astype(int) if batch_size is not None else 1\n",
      "  1312       100          0.0      0.0      0.0                  for minibatch_num in range(minibatches):\n",
      "  1313                                                               # Load minibatch\n",
      "  1314        50          0.0      0.0      0.0                      minibatch_idx = batch_idx[minibatch_num*minibatch_size:(minibatch_num+1)*minibatch_size]\n",
      "  1315        50          0.6      0.0     14.1                      minibatch_indices, minibatch_data = memory[minibatch_idx]\n",
      "  1316                                           \n",
      "  1317                                                               # Normalize advantages\n",
      "  1318        50          0.0      0.0      0.2                      minibatch_data['normalized_advantages'] = (minibatch_data['advantages'] - minibatch_data['advantages'].mean()) / (minibatch_data['advantages'].std() + 1e-8)\n",
      "  1319                                           \n",
      "  1320                                                               # Cast\n",
      "  1321        50          1.1      0.0     26.3                      minibatch_data = _utility.processing.dict_map_recursive_tensor_idx_to(minibatch_data, None, self.policy_iteration.device)\n",
      "  1322                                           \n",
      "  1323                                                               # Ministeps\n",
      "  1324        50          0.0      0.0      0.0                      minibatch_memories = self.minibatch_memories if self.minibatch_memories is not None else np.prod(minibatch_data['states'][1].shape[:-1])\n",
      "  1325        50          0.0      0.0      0.0                      ministep_size = np.maximum(np.floor(minibatch_memories / minibatch_data['states'][1].shape[1]), 1).astype(int)\n",
      "  1326        50          0.0      0.0      0.0                      ministeps = np.ceil(minibatch_data['states'][1].shape[0] / ministep_size).astype(int)\n",
      "  1327        50          0.0      0.0      0.1                      cumsum_indices = (minibatch_indices > -1).flatten().cumsum(0).reshape(minibatch_indices.shape)\n",
      "  1328        50          0.0      0.0      0.0                      proc_mems = 0\n",
      "  1329       100          0.0      0.0      0.0                      for ministep_num in range(ministeps):\n",
      "  1330                                                                   # Subsample\n",
      "  1331        50          0.0      0.0      0.0                          double_idx = slice(ministep_num*ministep_size, (ministep_num+1)*ministep_size)\n",
      "  1332        50          0.0      0.0      0.0                          single_idx = cumsum_indices[double_idx]\n",
      "  1333        50          0.0      0.0      0.0                          first_true = int(minibatch_indices[double_idx][0, 0] > -1)\n",
      "  1334        50          0.0      0.0      0.0                          num_memories = single_idx.max() - single_idx.min() + first_true\n",
      "  1335        50          0.0      0.0      0.0                          single_idx = slice(single_idx.min() - first_true, single_idx.max())  # NOTE: No +1 here because cumsum is always one ahead\n",
      "  1336        50          0.0      0.0      0.0                          if num_memories == 0: continue\n",
      "  1337        50          0.0      0.0      0.0                          proc_mems += num_memories\n",
      "  1338                                           \n",
      "  1339                                                                   # Get subset data\n",
      "  1340        50          0.0      0.0      0.0                          states = [s[double_idx] for s in minibatch_data['states']]\n",
      "  1341        50          0.0      0.0      0.0                          actions = minibatch_data['actions'][single_idx]\n",
      "  1342        50          0.0      0.0      0.0                          action_logs = minibatch_data['action_logs'][single_idx]\n",
      "  1343        50          0.0      0.0      0.0                          state_vals = minibatch_data['state_vals'][single_idx]\n",
      "  1344        50          0.0      0.0      0.0                          advantages = minibatch_data['advantages'][single_idx]\n",
      "  1345        50          0.0      0.0      0.0                          normalized_advantages = minibatch_data['normalized_advantages'][single_idx]\n",
      "  1346                                                                   # rewards = minibatch_data['propagated_rewards'][single_idx]\n",
      "  1347                                           \n",
      "  1348                                                                   # Perform backward\n",
      "  1349       100          1.0      0.0     23.7                          loss, loss_ppo, loss_critic, loss_entropy, loss_kl, state_vals_new = self.calculate_losses(\n",
      "  1350        50          0.0      0.0      0.0                              states, actions, action_logs, state_vals, advantages=advantages, normalized_advantages=normalized_advantages, rewards=None)\n",
      "  1351                                           \n",
      "  1352                                                                   # Scale and calculate gradient\n",
      "  1353                                                                   # accumulation_frac = minibatch_actual_size / batch_size\n",
      "  1354                                                                   # accumulation_frac = minibatch_idx.shape[0] / batch_size\n",
      "  1355        50          0.0      0.0      0.0                          accumulation_frac = num_memories / batch_size\n",
      "  1356        50          0.0      0.0      0.0                          loss = loss * accumulation_frac\n",
      "  1357        50          0.2      0.0      4.6                          loss.backward()  # Longest computation\n",
      "  1358                                           \n",
      "  1359                                                                   # Record required logs\n",
      "  1360        50          0.0      0.0      0.0                          batch_inputs = torch.cat((batch_inputs, states[0].view(-1, states[0].shape[-1])), dim=0)  # Only use first to save memory\n",
      "  1361        50          0.0      0.0      0.0                          batch_returns = torch.cat((batch_returns, advantages+state_vals), dim=0)\n",
      "  1362        50          0.0      0.0      0.0                          batch_state_vals_new = torch.cat((batch_state_vals_new, state_vals_new), dim=0)\n",
      "  1363                                           \n",
      "  1364                                                                   # Scale and record\n",
      "  1365        50          0.0      0.0      0.0                          batch_losses['Total'] += loss.detach()\n",
      "  1366        50          0.0      0.0      0.1                          batch_losses['PPO'] += loss_ppo.detach().mean() * accumulation_frac\n",
      "  1367        50          0.0      0.0      0.1                          batch_losses['critic'] += loss_critic.detach().mean() * accumulation_frac\n",
      "  1368        50          0.0      0.0      0.1                          batch_losses['entropy'] += loss_entropy.detach().mean() * accumulation_frac\n",
      "  1369        50          0.0      0.0      0.1                          batch_losses['KL'] += loss_kl.detach().mean() * accumulation_frac\n",
      "  1370                                           \n",
      "  1371                                                           # Calculate explained variance\n",
      "  1372        50          0.0      0.0      0.1                  normalized_returns = self.return_standardization.apply(batch_returns)\n",
      "  1373        50          0.0      0.0      0.2                  exp_var = (1- (normalized_returns - batch_state_vals_new).var() / normalized_returns.var()).clamp(min=-1)\n",
      "  1374                                           \n",
      "  1375                                                           # Statistics\n",
      "  1376        50          0.0      0.0      0.1                  batch_statistics['Moving Return Mean'] += self.return_standardization.mean.detach().mean()\n",
      "  1377        50          0.0      0.0      0.0                  batch_statistics['Moving Return STD'] += self.return_standardization.std.detach().mean()\n",
      "  1378        50          0.0      0.0      0.1                  batch_statistics['Return Mean'] += batch_returns.detach().mean() * accumulation_frac\n",
      "  1379        50          0.0      0.0      0.1                  batch_statistics['Return STD'] += batch_returns.detach().std() * accumulation_frac\n",
      "  1380        50          0.0      0.0      0.0                  batch_statistics['Moving Input Mean'] += self.input_standardization.mean.detach().mean()\n",
      "  1381        50          0.0      0.0      0.0                  batch_statistics['Moving Input STD'] += self.input_standardization.std.detach().mean()\n",
      "  1382        50          0.0      0.0      0.1                  batch_statistics['Input Mean'] += batch_inputs.detach().mean() * accumulation_frac\n",
      "  1383        50          0.0      0.0      0.1                  batch_statistics['Input STD'] += batch_inputs.detach().std() * accumulation_frac\n",
      "  1384                                                           # batch_statistics['Moving Reward Mean'] += self.reward_standardization.mean.mean() * accumulation_frac\n",
      "  1385                                                           # batch_statistics['Moving Reward STD'] += self.reward_standardization.std.mean() * accumulation_frac\n",
      "  1386        50          0.0      0.0      0.0                  batch_statistics['Explained Variance'] += exp_var.detach()\n",
      "  1387                                                           # batch_statistics['Advantage Mean'] += advantages.detach().mean() * accumulation_frac\n",
      "  1388                                                           # batch_statistics['Advantage STD'] += advantages.detach().std() * accumulation_frac\n",
      "  1389                                                           # batch_statistics['Log STD'] += self.get_log_std() * accumulation_frac\n",
      "  1390                                                           \n",
      "  1391                                                           # Record\n",
      "  1392       300          0.0      0.0      0.0                  for k, v in batch_losses.items(): total_losses[k].append(v)\n",
      "  1393       500          0.0      0.0      0.0                  for k, v in batch_statistics.items(): total_statistics[k].append(v)\n",
      "  1394                                           \n",
      "  1395                                                           # Synchronize GPU policies and step\n",
      "  1396                                                           # NOTE: Synchronize gradients every batch if =1, else synchronize whole model\n",
      "  1397                                                           # NOTE: =1 keeps optimizers in sync without need for whole-model synchronization\n",
      "  1398        50          0.0      0.0      0.0                  if sync_iterations == 1: synchronize(self, 'learners', grad=True)  # Sync only grad\n",
      "  1399        50          0.0      0.0      0.0                  if self.kl_early_stop and synchronized: self.copy_policy()\n",
      "  1400        50          0.0      0.0      0.8                  nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.grad_clip)\n",
      "  1401        50          0.0      0.0      1.2                  self.optimizer.step()\n",
      "  1402        50          0.0      0.0      0.1                  self.optimizer.zero_grad()\n",
      "  1403        50          0.0      0.0      0.0                  if sync_iterations != 1:\n",
      "  1404                                                               # Synchronize for offsets\n",
      "  1405                                                               sync_loop = (iterations) % sync_iterations == 0\n",
      "  1406                                                               last_epoch = iterations == update_iterations\n",
      "  1407                                                               if use_collective and (sync_loop or last_epoch):\n",
      "  1408                                                                   synchronize(self, 'learners')\n",
      "  1409                                                                   synchronized = True\n",
      "  1410                                                               else: synchronized = False\n",
      "  1411                                           \n",
      "  1412                                                           # Update moving return mean\n",
      "  1413        50          0.0      0.0      0.0                  if standardize_inputs:\n",
      "  1414        50          0.8      0.0     20.8                      self.input_standardization.update(batch_inputs)\n",
      "  1415        50          0.0      0.0      0.0                  if standardize_returns:\n",
      "  1416        50          0.0      0.0      0.8                      self.return_standardization.update(batch_returns)\n",
      "  1417        50          0.0      0.0      0.0                      synchronize(self, 'learners', sync_list=self.return_standardization.parameters())\n",
      "  1418                                           \n",
      "  1419                                                           # Update KL beta\n",
      "  1420                                                           # NOTE: Same as Torch KLPENPPOLoss implementation\n",
      "  1421        50          0.0      0.0      0.0                  if self.kl_early_stop or self.kl_beta != 0:\n",
      "  1422                                                               loss_kl_mean = loss_kl.detach().mean()\n",
      "  1423                                                               synchronize(self, 'learners', sync_list=[loss_kl_mean])\n",
      "  1424                                                               if not self.kl_early_stop:\n",
      "  1425                                                                   exp_limit = 32\n",
      "  1426                                                                   if loss_kl_mean < self.kl_target / 1.5 and self.kl_beta > 2**-exp_limit: self.kl_beta.data *= self.kl_beta_increment[0]\n",
      "  1427                                                                   elif loss_kl_mean > self.kl_target * 1.5 and self.kl_beta < 2**exp_limit: self.kl_beta.data *= self.kl_beta_increment[1]\n",
      "  1428                                           \n",
      "  1429                                                           # Escape and roll back if KLD too high\n",
      "  1430        50          0.0      0.0      0.0                  if self.kl_early_stop:\n",
      "  1431                                                               if loss_kl_mean > 1.5 * self.kl_target:\n",
      "  1432                                                                   if iterations - sync_iterations > 0:\n",
      "  1433                                                                       # Revert to previous synchronized state within kl target\n",
      "  1434                                                                       self.revert_policy()\n",
      "  1435                                                                       # iterations -= sync_iterations\n",
      "  1436                                                                       escape = True; break\n",
      "  1437                                                                   else:\n",
      "  1438                                                                       warnings.warn(\n",
      "  1439                                                                           'Update exceeded KL target too fast! Proceeding with update, but may be unstable. '\n",
      "  1440                                                                           'Try lowering clip or learning rate parameters.')\n",
      "  1441                                                                       escape = True; break\n",
      "  1442                                                                   \n",
      "  1443                                                       # Iterate\n",
      "  1444         5          0.0      0.0      0.0              iterations += 1\n",
      "  1445         5          0.0      0.0      0.0              if iterations >= update_iterations: escape = True\n",
      "  1446                                           \n",
      "  1447                                                       # CLI\n",
      "  1448         5          0.0      0.0      0.0              if verbose and (iterations in (1, 5) or iterations % 10 == 0 or escape):\n",
      "  1449         4          0.0      0.0      0.0                  print(\n",
      "  1450         8          0.0      0.0      0.0                      f'Iteration {iterations:02} - '\n",
      "  1451         2          0.0      0.0      0.0                      + f' + '.join([f'{k} ({np.mean([v.item() for v in vl[-batches:]]):.5f})' for k, vl in total_losses.items()])\n",
      "  1452         2          0.0      0.0      0.0                      + f' :: '\n",
      "  1453         2          0.0      0.0      0.0                      + f', '.join([f'{k} ({np.mean([v.item() for v in vl[-batches:]]):.5f})' for k, vl in total_statistics.items()]))\n",
      "  1454                                           \n",
      "  1455                                                       # Break\n",
      "  1456         5          0.0      0.0      0.0              if escape: break\n",
      "  1457                                           \n",
      "  1458                                                   # Update scheduler\n",
      "  1459         1          0.0      0.0      0.0          self.scheduler.step()\n",
      "  1460                                           \n",
      "  1461                                                   # Train pinning\n",
      "  1462         1          0.0      0.0      0.0          X, Y = memory.get_terminal_pairs()\n",
      "  1463         1          0.0      0.0      0.0          has_memories = torch.tensor(0., device=self.policy_iteration.device)\n",
      "  1464         1          0.0      0.0      0.0          if X is not None: has_memories += 1\n",
      "  1465         1          0.0      0.0      0.0          synchronize(self, 'learners', sync_list=[has_memories])\n",
      "  1466         1          0.0      0.0      0.0          pinning_losses = {}\n",
      "  1467         1          0.0      0.0      0.0          running_feat = 0\n",
      "  1468         2          0.0      0.0      0.0          for i, (pinning, pinning_modal_dim) in enumerate(zip(self.pinning, self.pinning_modal_dims)):\n",
      "  1469         1          0.0      0.0      0.0              if X is None: pinning.update(None, None, world_size=has_memories)\n",
      "  1470         2          0.2      0.1      5.3              else: pinning_losses[f'Pinning Loss {i}'] = pinning.update(\n",
      "  1471         1          0.0      0.0      0.0                  X[..., :self.pinning_dim].to(self.policy_iteration.device),\n",
      "  1472         1          0.0      0.0      0.0                  Y[..., running_feat:running_feat+pinning_modal_dim].to(self.policy_iteration.device),\n",
      "  1473         1          0.0      0.0      0.0                  world_size=has_memories.item())\n",
      "  1474         1          0.0      0.0      0.0              running_feat += pinning_modal_dim\n",
      "  1475         1          0.0      0.0      0.0          assert running_feat == Y.shape[1], (\n",
      "  1476                                                       f'`pinning_modal_dims` sum ({running_feat}) does not match target modality combined length ({Y.shape[1]})')\n",
      "  1477                                           \n",
      "  1478                                                   # Update records\n",
      "  1479         1          0.0      0.0      0.0          self.policy_iteration += 1\n",
      "  1480         1          0.0      0.0      0.1          self.copy_policy()\n",
      "  1481                                                   # Return\n",
      "  1482         1          0.0      0.0      0.1          total_losses_ret = {k: np.mean([v.item() for v in vl]) for k, vl in total_losses.items()}\n",
      "  1483         1          0.0      0.0      0.0          total_losses_ret = {**total_losses_ret, **pinning_losses}  # TODO: Maybe revise\n",
      "  1484         1          0.0      0.0      0.1          total_statistics_ret = {k: np.mean([v.item() for v in vl]) for k, vl in total_statistics.items()}\n",
      "  1485         1          0.0      0.0      0.0          return (\n",
      "  1486         1          0.0      0.0      0.0              iterations,\n",
      "  1487         1          0.0      0.0      0.0              total_losses_ret,\n",
      "  1488         1          0.0      0.0      0.0              total_statistics_ret)\n",
      "\n",
      "Total time: 0.949025 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: calculate_losses at line 1490\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "  1490                                               def calculate_losses(\n",
      "  1491                                                   self,\n",
      "  1492                                                   states,\n",
      "  1493                                                   actions,\n",
      "  1494                                                   action_logs,\n",
      "  1495                                                   state_vals,\n",
      "  1496                                                   advantages=None,\n",
      "  1497                                                   normalized_advantages=None,\n",
      "  1498                                                   rewards=None):\n",
      "  1499                                                   # TODO: Maybe implement PFO https://github.com/CLAIRE-Labo/no-representation-no-trust\n",
      "  1500        50          0.0      0.0      0.0          if advantages is not None:\n",
      "  1501                                                       # Get inferred rewards\n",
      "  1502        50          0.0      0.0      0.2              rewards = advantages + state_vals\n",
      "  1503                                                   elif rewards is not None:\n",
      "  1504                                                       # Get advantages\n",
      "  1505                                                       advantages = rewards - state_vals\n",
      "  1506                                                   # Get normalized advantages\n",
      "  1507        50          0.0      0.0      0.0          if normalized_advantages is None:\n",
      "  1508                                                       advantages_mean, advantages_std = advantages.mean(), advantages.std() + 1e-8\n",
      "  1509                                                       normalized_advantages = (advantages - advantages_mean) / advantages_std\n",
      "  1510                                                   # Clip advantages for stability\n",
      "  1511                                                   # normalized_advantages =  normalized_advantages.clamp(\n",
      "  1512                                                   #     normalized_advantages.quantile(.05), normalized_advantages.quantile(.95))\n",
      "  1513                                           \n",
      "  1514                                                   # Get normalized returns/rewards (sensitive to minibatch)\n",
      "  1515                                                   # NOTE: Action STD explosion/instability points to a normalization and/or critic fitting issue\n",
      "  1516                                                   #       or, possibly, the returns are too homogeneous - i.e. the problem is solved\n",
      "  1517        50          0.0      0.0      0.4          normalized_rewards = self.return_standardization.apply(rewards)  # , mean=False\n",
      "  1518                                           \n",
      "  1519                                                   # Evaluate actions and states\n",
      "  1520        50          0.0      0.0      0.5          normalized_states = [self.input_standardization.apply(states[0]), self.input_standardization.apply(states[1]), states[2]]\n",
      "  1521        50          0.9      0.0     96.7          action_logs_new, dist_entropy, state_vals_new = self.actor_critic(*normalized_states, action=actions, entropy=True, critic=True)\n",
      "  1522                                                   # action_logs_new = action_logs_new.clamp(-20, 0)\n",
      "  1523                                                   \n",
      "  1524                                                   # Calculate PPO loss\n",
      "  1525        50          0.0      0.0      0.1          log_ratios = action_logs_new - action_logs\n",
      "  1526                                                   # log_ratios = log_ratios.clamp(-20, 2)\n",
      "  1527        50          0.0      0.0      0.1          ratios = log_ratios.exp()\n",
      "  1528        50          0.0      0.0      0.1          unclipped_ppo = ratios * normalized_advantages\n",
      "  1529        50          0.0      0.0      0.2          clipped_ppo = ratios.clamp(1-self.epsilon_ppo, 1+self.epsilon_ppo) * normalized_advantages\n",
      "  1530        50          0.0      0.0      0.2          loss_ppo = -torch.min(unclipped_ppo, clipped_ppo)\n",
      "  1531                                           \n",
      "  1532                                                   # Calculate KL divergence\n",
      "  1533                                                   # NOTE: A bit odd when it comes to replay\n",
      "  1534                                                   # Discrete\n",
      "  1535                                                   # loss_kl = F.kl_div(action_logs, action_logs_new, reduction='batchmean', log_target=True)\n",
      "  1536                                                   # loss_kl = ((action_logs - action_logs_new)  # * action_logs.exp()).sum(-1)  # Approximation\n",
      "  1537                                                   # Continuous (http://joschu.net/blog/kl-approx.html)\n",
      "  1538        50          0.0      0.0      0.2          loss_kl = (log_ratios.exp() - 1) - log_ratios\n",
      "  1539                                                   # Mask and scale where needed (for replay)\n",
      "  1540                                                   # loss_kl[~new_memories] = 0\n",
      "  1541                                                   # loss_kl = loss_kl * loss_kl.shape[0] / new_memories.sum()\n",
      "  1542                                           \n",
      "  1543                                                   # Calculate critic loss\n",
      "  1544        50          0.0      0.0      0.0          criteria = F.smooth_l1_loss\n",
      "  1545                                                   # criteria = F.mse_loss\n",
      "  1546        50          0.0      0.0      0.3          unclipped_critic = criteria(state_vals_new, normalized_rewards)\n",
      "  1547        50          0.0      0.0      0.2          clipped_state_vals_new = torch.clamp(state_vals_new, state_vals-self.epsilon_critic, state_vals+self.epsilon_critic)\n",
      "  1548        50          0.0      0.0      0.2          clipped_critic = criteria(clipped_state_vals_new, normalized_rewards, reduction='none')\n",
      "  1549        50          0.0      0.0      0.1          loss_critic = torch.max(unclipped_critic, clipped_critic)\n",
      "  1550                                           \n",
      "  1551                                                   # Calculate entropy bonus\n",
      "  1552                                                   # NOTE: Not included in training grad if action_std is constant\n",
      "  1553                                                   # dist_entropy = -action_logs_new  # Approximation\n",
      "  1554        50          0.0      0.0      0.1          loss_entropy = -dist_entropy\n",
      "  1555                                           \n",
      "  1556                                                   # Construct final loss\n",
      "  1557        50          0.0      0.0      0.0          loss = (\n",
      "  1558       200          0.0      0.0      0.2              loss_ppo\n",
      "  1559        50          0.0      0.0      0.1              + self.critic_weight * loss_critic\n",
      "  1560        50          0.0      0.0      0.1              + self.entropy_weight * loss_entropy\n",
      "  1561        50          0.0      0.0      0.1              + self.kl_beta * loss_kl)\n",
      "  1562        50          0.0      0.0      0.1          loss = loss.mean()\n",
      "  1563                                           \n",
      "  1564        50          0.0      0.0      0.0          return loss, loss_ppo, loss_critic, loss_entropy, loss_kl, state_vals_new\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Updates\n",
    "# import line_profiler\n",
    "# prof = line_profiler.LineProfiler(\n",
    "#     # memory.fast_sample, policy.actor_critic.forward,\n",
    "#     celltrip.policy.ResidualAttentionBlock.forward,\n",
    "#     policy.calculate_losses, policy.update,\n",
    "#     celltrip.memory.AdvancedMemoryBuffer.__getitem__)\n",
    "# ret = prof.runcall(policy.update, memory, verbose=True)\n",
    "# print('UPDATE: ' + ', '.join([f'{k}: {v:.3f}' for ret_dict in ret[1:] for k, v in ret_dict.items()]))\n",
    "# prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(int(config.max_timesteps / config.update_timesteps)):\n",
    "#     # Forward\n",
    "#     memory.mark_sampled()\n",
    "#     memory.cleanup()\n",
    "#     ret = celltrip.train.simulate_until_completion(\n",
    "#         env, policy, memory,\n",
    "#         max_memories=config.update_timesteps,\n",
    "#         # max_timesteps=100,\n",
    "#         reset_on_finish=True)\n",
    "#     print('ROLLOUT: ' + f'iterations: {ret[0]: 5.0f}, ' + f'total: {ret[2]: 5.3f}, ' + ', '.join([f'{k}: {v: 5.3f}' for k, v in ret[3].items()]))\n",
    "#     memory.compute_advantages()\n",
    "\n",
    "#     # Update\n",
    "#     # NOTE: Training often only improves when PopArt and actual distribution match\n",
    "#     ret = policy.update(memory, verbose=False)\n",
    "#     print('UPDATE: ' + ', '.join([f'{k}: {v: 5.3f}' for ret_dict in ret[1:] for k, v in ret_dict.items()]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
