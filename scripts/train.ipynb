{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sometimes model will be chaotic if it finds how to early-finish early on, if so, it waits until action_std is lower to implement\n",
    "- Train/test\n",
    "- Save preprocessing\n",
    "- Early stopping for `train_celltrip` based on action_std and/or KL\n",
    "- Maybe [this](https://arxiv.org/abs/2102.09430) but probably not\n",
    "- [EFS on clusters maybe](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/aws.html#start-ray-with-the-ray-cluster-launcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cython is not active\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import ray\n",
    "\n",
    "import celltrip\n",
    "\n",
    "# Detect Cython\n",
    "CYTHON_ACTIVE = os.path.splitext(celltrip.utility.general.__file__)[1] in ('.c', '.so')\n",
    "print(f'Cython is{\" not\" if not CYTHON_ACTIVE else \"\"} active')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python train.py s3://nkalafut-celltrip/MERFISH/expression.h5ad s3://nkalafut-celltrip/MERFISH/spatial.h5ad --backed --target_modalities 1 --num_gpus 3 --num_learners 2 --num_runners 6 --steps 5000 --max_updates 200 --dont_sync_across_nodes --logfile s3://nkalafut-celltrip/logs/3gpu-new-arch-10-norms-log.log --checkpoint_iterations 20 --checkpoint_dir s3://nkalafut-celltrip/checkpoints --checkpoint_name 3gpu-new-arch-10-norms-log\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "# NOTE: It is not recommended to use s3 with credentials unless the creds are permanent, the bucket is public, or this is run on AWS\n",
    "parser = argparse.ArgumentParser(description='Train CellTRIP model', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "# Reading\n",
    "group = parser.add_argument_group('Input')\n",
    "group.add_argument('input_files', type=str, nargs='*', help='h5ad files to be used for input')\n",
    "group.add_argument('--merge_files', type=str, action='append', nargs='+', help='h5ad files to merge as input')\n",
    "group.add_argument('--partition_cols', type=str, action='append', nargs='+', help='Columns for data partitioning, found in `adata.obs` DataFrame')\n",
    "group.add_argument('--backed', action='store_true', help='Read data directly from disk or s3, saving memory at the cost of time')\n",
    "group.add_argument('--input_modalities', type=int, nargs='+', help='Input modalities to give to CellTRIP')\n",
    "group.add_argument('--target_modalities', type=int, nargs='+', help='Target modalities to emulate, dictates environment reward')\n",
    "# Computation\n",
    "group = parser.add_argument_group('Computation')\n",
    "group.add_argument('--num_gpus', type=int, default=1, help='Number of GPUs to use during computation')\n",
    "group.add_argument('--num_learners', type=int, default=1, help='Number of learners used in backward computation, cannot exceed GPUs')\n",
    "group.add_argument('--num_runners', type=int, default=1, help='Number of workers for environment simulation')\n",
    "# Training\n",
    "group = parser.add_argument_group('Training')\n",
    "group.add_argument('--steps', type=int, default=int(2.5e3), help='Number of steps recorded before each update')\n",
    "group.add_argument('--max_updates', type=int, default=200, help='Maximum number of policy updates to compute before exiting')\n",
    "group.add_argument('--dont_sync_across_nodes', action='store_true', help='Avoid memory sync across nodes, saving overhead time at the cost of stability')\n",
    "# File saves\n",
    "group = parser.add_argument_group('Logging')\n",
    "group.add_argument('--logfile', type=str, default='cli', help='Location for log file, can be `cli`, `<local_file>`, or `<s3 location>`')\n",
    "group.add_argument('--flush_iterations', type=int, help='Number of iterations to wait before flushing logs')\n",
    "group.add_argument('--checkpoint', type=str, help='Checkpoint to use for initializing model')\n",
    "group.add_argument('--checkpoint_iterations', type=int, default=50, help='Number of iterations to wait before recording checkpoints')\n",
    "group.add_argument('--checkpoint_dir', type=str, default='./checkpoints', help='Directory for checkpoints')\n",
    "group.add_argument('--checkpoint_name', type=str, help='Run name, for checkpointing')\n",
    "\n",
    "# Notebook defaults and script handling\n",
    "if not celltrip.utility.notebook.is_notebook():\n",
    "    # ray job submit -- python train.py...\n",
    "    config = parser.parse_args()\n",
    "else:\n",
    "    experiment_name = '3gpu-new-arch-10-norms-log'\n",
    "    command = (\n",
    "        f's3://nkalafut-celltrip/MERFISH/expression.h5ad s3://nkalafut-celltrip/MERFISH/spatial.h5ad '\n",
    "        # f'/home/nck/repos/INEPT/data/MERFISH/expression.h5ad /home/nck/repos/INEPT/data/MERFISH/spatial.h5ad '\n",
    "        f'--backed '\n",
    "        f'--target_modalities 1 '\n",
    "        f'--num_gpus 3 --num_learners 2 --num_runners 6 '\n",
    "        f'--steps 5000 '\n",
    "        f'--max_updates 200 '\n",
    "        f'--dont_sync_across_nodes '\n",
    "        f'--logfile s3://nkalafut-celltrip/logs/{experiment_name}.log '\n",
    "        # f'--checkpoint s3://nkalafut-celltrip/checkpoints/3gpu-1k-0100.weights '\n",
    "        f'--checkpoint_iterations 20 '\n",
    "        f'--checkpoint_dir s3://nkalafut-celltrip/checkpoints '\n",
    "        f'--checkpoint_name {experiment_name}')\n",
    "    config = parser.parse_args(command.split(' '))\n",
    "    print(f'python train.py {command}')\n",
    "    \n",
    "# Defaults\n",
    "if config.checkpoint_name is None:\n",
    "    config.checkpoint_name = f'RUN_{random.randint(0, 2**32):0>10}'\n",
    "    print(f'Run Name: {config.checkpoint_name}')\n",
    "# print(config)  # CLI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start Ray\n",
    "# ray.shutdown()\n",
    "# a = ray.init(\n",
    "#     # address='ray://100.85.187.118:10001',\n",
    "#     address='ray://localhost:10001',\n",
    "#     runtime_env={\n",
    "#         'py_modules': [celltrip],\n",
    "#         'pip': '../requirements.txt',\n",
    "#         'env_vars': {\n",
    "#             # **access_keys,\n",
    "#             'RAY_DEDUP_LOGS': '0'}},\n",
    "#         # 'NCCL_SOCKET_IFNAME': 'tailscale',  # lo,en,wls,docker,tailscale\n",
    "#     _system_config={'enable_worker_prestart': True})  # Doesn't really work for scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ray.remote(num_cpus=1e-4)\n",
    "# def train(config):\n",
    "#     import celltrip\n",
    "\n",
    "#     # Initialization\n",
    "#     dataloader_kwargs = {'num_nodes': 20, 'pca_dim': 128}\n",
    "#     environment_kwargs = {\n",
    "#         'input_modalities': config.input_modalities,\n",
    "#         'target_modalities': config.target_modalities, 'dim': 3}\n",
    "#     initializers = celltrip.train.get_initializers(\n",
    "#         input_files=config.input_files, merge_files=config.merge_files,\n",
    "#         backed=config.backed, dataloader_kwargs=dataloader_kwargs,\n",
    "#         environment_kwargs=environment_kwargs)\n",
    "\n",
    "#     stage_functions = [\n",
    "#         # lambda w: w.env.set_rewards(penalty_velocity=1, penalty_action=1),\n",
    "#         # lambda w: w.env.set_rewards(reward_origin=1),\n",
    "#         # lambda w: w.env.set_rewards(reward_origin=0, reward_distance=1),\n",
    "#         lambda w: w.env.dataloader.preprocessing.set_num_nodes(500),\n",
    "#         lambda w: w.env.dataloader.preprocessing.set_num_nodes(1000),\n",
    "#         lambda w: w.env.dataloader.preprocessing.set_num_nodes(2000),\n",
    "#         lambda w: w.env.dataloader.preprocessing.set_num_nodes(3000),\n",
    "#     ]\n",
    "\n",
    "#     # Run function\n",
    "#     celltrip.train.train_celltrip(\n",
    "#         initializers=initializers,\n",
    "#         num_gpus=config.num_gpus, num_learners=config.num_learners,\n",
    "#         num_runners=config.num_runners, max_updates=config.max_updates,\n",
    "#         sync_across_nodes=not config.dont_sync_across_nodes,\n",
    "#         checkpoint_iterations=config.checkpoint_iterations, checkpoint_dir=config.checkpoint_dir,\n",
    "#         checkpoint=config.checkpoint, checkpoint_name=config.checkpoint_name,\n",
    "#         stage_functions=stage_functions, logfile=config.logfile)\n",
    "\n",
    "# ray.get(train.remote(config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get AWS keys\n",
    "# import boto3\n",
    "# os.environ['AWS_PROFILE'] = 'waisman-admin'\n",
    "# session = boto3.Session()\n",
    "# creds = session.get_credentials()\n",
    "# access_keys = {\n",
    "#     'AWS_ACCESS_KEY_ID': creds.access_key,\n",
    "#     'AWS_SECRET_ACCESS_KEY': creds.secret_key,\n",
    "#     'AWS_DEFAULT_REGION': 'us-east-2'}\n",
    "\n",
    "# # Check s3\n",
    "# import os\n",
    "# import s3fs\n",
    "# os.environ['AWS_PROFILE'] = 'waisman-admin'\n",
    "# s3 = s3fs.S3FileSystem(skip_instance_cache=True)\n",
    "# s3.ls('s3://nkalafut-celltrip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize locally\n",
    "os.environ['AWS_PROFILE'] = 'waisman-admin'\n",
    "dataloader_kwargs = {'num_nodes': 2000, 'pca_dim': 128}\n",
    "environment_kwargs = {\n",
    "    'input_modalities': config.input_modalities,\n",
    "    'target_modalities': config.target_modalities, 'dim': 3}\n",
    "env_init, policy_init, memory_init = celltrip.train.get_initializers(\n",
    "    input_files=config.input_files, merge_files=config.merge_files,\n",
    "    backed=config.backed, # policy_kwargs={'minibatch_size': 8*2_000},\n",
    "    dataloader_kwargs=dataloader_kwargs,\n",
    "    environment_kwargs=environment_kwargs)\n",
    "# env = env_init().to('cuda')\n",
    "# policy = policy_init(env).to('cuda')\n",
    "# memory = memory_init(policy)\n",
    "# celltrip.train.simulate_until_completion(env, policy, memory)\n",
    "# memory.compute_advantages()\n",
    "# policy.update(memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/repos/inept/celltrip/utility/processing.py:108: RuntimeWarning: Modality 1 too small for PCA (2 features), skipping\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "try: env\n",
    "except: env = env_init().to('cuda')\n",
    "# policy.split_args['max_nodes'] = 2000\n",
    "# policy.forward_batch_size = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = policy_init(env).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = memory_init(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance: -1397.939, origin: 0.000, bound: -20986.576, velocity: -279.673, action: -1043.473\n",
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 1.13604 s\n",
      "File: /home/thema/repos/inept/celltrip/environment.py\n",
      "Function: step at line 131\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   131                                               def step(self, actions=None, *, delta=None, return_itemized_rewards=False):\n",
      "   132                                                   # Defaults\n",
      "   133      1000          0.0      0.0      0.0          if actions is None: actions = torch.zeros_like(self.vel, device=self.device)\n",
      "   134      1000          0.0      0.0      0.0          if delta is None: delta = self.delta\n",
      "   135                                           \n",
      "   136                                                   # Check dimensions\n",
      "   137                                                   # assert actions.shape == self.vel.shape\n",
      "   138                                           \n",
      "   139                                                   ### Pre-step calculations\n",
      "   140                                                   # Distance reward (Emulate combined intra-modal distances)\n",
      "   141      1000          0.0      0.0      0.0          if self.reward_scales['reward_distance'] != 0:\n",
      "   142      1000          0.0      0.0      0.2              reward_distance = self.get_distance_match()  # .log()  # CHANGED, ADDED LOG\n",
      "   143                                                   else: reward_distance = torch.zeros(actions.shape[0], device=self.device)\n",
      "   144                                                   # Origin penalty\n",
      "   145      1000          0.0      0.0      0.0          if self.reward_scales['reward_origin'] != 0: reward_origin = self.get_distance_from_origin()\n",
      "   146      1000          0.0      0.0      2.1          else: reward_origin = torch.zeros(actions.shape[0], device=self.device)\n",
      "   147                                           \n",
      "   148                                                   ### Step positions\n",
      "   149                                                   # Old storage\n",
      "   150                                                   # old_vel = self.vel.clone()\n",
      "   151                                                   # old_bound_hit_mask = self.pos.abs() == self.pos_bound\n",
      "   152                                                   # Add velocity\n",
      "   153      1000          0.0      0.0      3.8          self.add_velocities(delta * actions)\n",
      "   154                                                   # Iterate positions\n",
      "   155      1000          0.0      0.0      1.9          self.pos = self.pos + delta * self.vel\n",
      "   156                                                   # Clip by bounds\n",
      "   157      1000          0.0      0.0      1.0          self.pos = torch.clamp(self.pos, -self.pos_bound, self.pos_bound)\n",
      "   158                                                   # Erase velocity of bound-hits\n",
      "   159      1000          0.0      0.0      3.2          bound_hit_mask = self.pos.abs() == self.pos_bound\n",
      "   160      1000          0.0      0.0      1.4          self.vel[bound_hit_mask] = 0\n",
      "   161                                                   # self.pos[bound_hit_mask.sum(dim=1) > 0] = 0  # Send to center\n",
      "   162                                                   # Reset cache\n",
      "   163      1000          0.0      0.0      0.2          self.reset_cache()\n",
      "   164                                           \n",
      "   165                                                   ### Post-step calculations\n",
      "   166                                                   # Finished\n",
      "   167      1000          0.0      0.0      0.0          self.timestep += 1\n",
      "   168      1000          0.0      0.0      0.1          finished = self.finished()\n",
      "   169                                                   # Distance reward\n",
      "   170      1000          0.0      0.0      0.0          if self.reward_scales['reward_distance'] != 0:\n",
      "   171      1000          0.2      0.0     19.3              reward_distance -= self.get_distance_match()  # .log()  # CHANGED, ADDED LOG\n",
      "   172                                                   # Origin reward\n",
      "   173      1000          0.0      0.0      0.1          if self.reward_scales['reward_origin'] != 0:\n",
      "   174                                                       reward_origin -= self.get_distance_from_origin()\n",
      "   175                                                   # Boundary penalty\n",
      "   176                                                   # penalty_bound = -(bound_hit_mask*~old_bound_hit_mask).sum(dim=1).float()\n",
      "   177      1000          0.0      0.0      1.3          penalty_bound = torch.zeros(self.pos.shape[0], device=self.device)\n",
      "   178      1000          0.1      0.0      5.2          penalty_bound[bound_hit_mask.sum(dim=1) > 0] = -1\n",
      "   179                                                   # Velocity penalty (Apply to ending velocity)\n",
      "   180      1000          0.0      0.0      3.4          penalty_velocity = -self.vel.square().mean(dim=1)  #  * finished\n",
      "   181                                                   # penalty_velocity = (old_vel.square() - self.vel.square()).mean(dim=1)\n",
      "   182                                                   # Action penalty (Smooth movements)\n",
      "   183      1000          0.0      0.0      2.4          penalty_action = -actions.square().mean(dim=1)\n",
      "   184                                                   # if finished: print(reason)\n",
      "   185                                                   # print(actions)\n",
      "   186                                           \n",
      "   187                                                   ### Management\n",
      "   188      1000          0.5      0.0     46.9          if self.get_distance_match().mean() < self.best: self.lapses += delta\n",
      "   189       199          0.0      0.0      0.3          else: self.best = self.get_distance_match().mean(); self.lapses = 0\n",
      "   190                                           \n",
      "   191                                                   # Scale rewards\n",
      "   192                                                   # def get_coef_from_step(step, in_step, top_step, out_step=None, factor=5000):\n",
      "   193                                                   #     step = step / factor\n",
      "   194                                                   #     if step < top_step or out_step is None:\n",
      "   195                                                   #         if in_step == top_step: return 1\n",
      "   196                                                   #         return np.clip((step - in_step) / (top_step - in_step), 0, 1)\n",
      "   197                                                   #     else:\n",
      "   198                                                   #         if top_step == out_step: return 1\n",
      "   199                                                   #         return np.clip((step - top_step) / (out_step - top_step), 1, 0)\n",
      "   200                                           \n",
      "   201      1000          0.0      0.0      1.3          reward_distance     *=  self.reward_scales['reward_distance']    * 1e0/delta\n",
      "   202      1000          0.0      0.0      0.8          reward_origin       *=  self.reward_scales['reward_origin']      * 1e0/delta\n",
      "   203      1000          0.0      0.0      0.7          penalty_bound       *=  self.reward_scales['penalty_bound']      * 1e2\n",
      "   204      1000          0.0      0.0      0.7          penalty_velocity    *=  self.reward_scales['penalty_velocity']   * 1e0\n",
      "   205      1000          0.0      0.0      0.7          penalty_action      *=  self.reward_scales['penalty_action']     * 1e0\n",
      "   206                                                   # self.steps += 1\n",
      "   207                                           \n",
      "   208                                                   # Compute total reward\n",
      "   209      1000          0.0      0.0      0.0          rwd = (\n",
      "   210      5000          0.0      0.0      2.7              reward_distance\n",
      "   211      1000          0.0      0.0      0.0              + reward_origin\n",
      "   212      1000          0.0      0.0      0.0              + penalty_bound\n",
      "   213      1000          0.0      0.0      0.0              + penalty_velocity\n",
      "   214      1000          0.0      0.0      0.0              + penalty_action)\n",
      "   215                                           \n",
      "   216      1000          0.0      0.0      0.0          ret = (rwd, finished)\n",
      "   217      2000          0.0      0.0      0.1          if return_itemized_rewards: ret += {\n",
      "   218      1000          0.0      0.0      0.0              'distance': reward_distance,\n",
      "   219      1000          0.0      0.0      0.0              'origin': reward_origin,\n",
      "   220      1000          0.0      0.0      0.0              'bound': penalty_bound,\n",
      "   221      1000          0.0      0.0      0.0              'velocity': penalty_velocity,\n",
      "   222      1000          0.0      0.0      0.0              'action': penalty_action},\n",
      "   223      1000          0.0      0.0      0.0          return ret\n",
      "\n",
      "Total time: 1.11936 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: forward at line 392\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   392                                               def forward(self, x, kv=None, mask=None):\n",
      "   393                                                   # Parameters\n",
      "   394      2002          0.0      0.0      0.1          if kv is None: x = kv\n",
      "   395                                           \n",
      "   396                                                   # Residual Attention\n",
      "   397      2002          0.1      0.0      6.6          x1 = self.norms[0](x)\n",
      "   398                                                   # kv1 = self.norms[1](kv)\n",
      "   399      2002          0.0      0.0      0.0          kv1 = kv\n",
      "   400      2002          0.7      0.0     64.8          x2, _ = self.attention(x1, kv1, kv1, attn_mask=mask)\n",
      "   401      2002          0.0      0.0      2.0          x = x + x2\n",
      "   402      2002          0.1      0.0      7.2          x1 = self.norms[2](x)\n",
      "   403      2002          0.2      0.0     17.0          x2 = self.mlp(x1)\n",
      "   404      2002          0.0      0.0      2.2          x = x + x2\n",
      "   405      2002          0.0      0.0      0.0          return x\n",
      "\n",
      "Total time: 2.23852 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: forward at line 467\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   467                                               def forward(\n",
      "   468                                                       self, self_entities, node_entities=None, mask=None,\n",
      "   469                                                       actor=True, sample=False, action=None, entropy=False, critic=False,\n",
      "   470                                                       feature_embeds=None, return_feature_embeds=False,\n",
      "   471                                                       squeeze=True, fit_and_strip=True):\n",
      "   472                                                   # Formatting\n",
      "   473      1001          0.0      0.0      0.0          if node_entities is None: node_entities = self_entities\n",
      "   474      1001          0.0      0.0      0.0          if mask is None:\n",
      "   475      1001          0.0      0.0      1.3              mask = torch.eye(self_entities.shape[-2], dtype=torch.bool, device=self_entities.device)\n",
      "   476      1001          0.0      0.0      0.1              if self_entities.dim() > 2: mask = mask.repeat((self_entities.shape[0], 1, 1))\n",
      "   477      1001          0.0      0.0      0.0          if self_entities.dim() > 2:\n",
      "   478                                                       # NOTE: Grouped batches (i.e. (>1, >1, ...) shape) are possible with squeeze=False\n",
      "   479                                                       if mask.dim() < 3: mask.unsqueeze(0)\n",
      "   480                                                       # mask = mask.repeat((self.heads, 1, 1))\n",
      "   481                                                       mask = mask[[i for i in range(mask.shape[0]) for _ in range(self.heads)]]\n",
      "   482      1001          0.0      0.0      0.0          if feature_embeds is not None:\n",
      "   483      1000          0.0      0.0      0.0              feature_embeds_ret = feature_embeds\n",
      "   484      1000          0.0      0.0      0.0              feature_embeds = feature_embeds.copy()\n",
      "   485         1          0.0      0.0      0.0          else: feature_embeds_ret = []\n",
      "   486                                           \n",
      "   487                                                   # Actor block\n",
      "   488      1001          0.0      0.0      0.0          if actor or not self.independent_critic:\n",
      "   489                                                       # Feature embedding\n",
      "   490      1001          0.1      0.0      2.4              self_pos_embeds = self.self_pos_embed(self_entities[..., :self.positional_dim])\n",
      "   491      1001          0.0      0.0      2.0              node_pos_embeds = self.node_pos_embed(node_entities[..., :self.positional_dim])\n",
      "   492      1001          0.0      0.0      0.0              if feature_embeds is not None: self_feat_embeds, node_feat_embeds = feature_embeds.pop(0)\n",
      "   493                                                       else:\n",
      "   494         1          0.0      0.0      0.0                  self_feat_embeds = self.self_feat_embed(self_entities[..., self.positional_dim:])\n",
      "   495         1          0.0      0.0      0.0                  node_feat_embeds = self.node_feat_embed(node_entities[..., self.positional_dim:])\n",
      "   496         1          0.0      0.0      0.0                  feature_embeds_ret.append((self_feat_embeds, node_feat_embeds))\n",
      "   497                                                       # Self embeddings\n",
      "   498      1001          0.1      0.0      4.2              self_embeds = self.self_embed(self_pos_embeds+self_feat_embeds)\n",
      "   499                                                       # Node embeddings\n",
      "   500      1001          0.1      0.0      3.8              node_embeds = self.node_embed(node_pos_embeds+node_feat_embeds)\n",
      "   501                                                       # Attention\n",
      "   502      3003          0.0      0.0      0.3              for block in self.residual_attention_blocks:\n",
      "   503      2002          1.1      0.0     51.4                  self_embeds = block(self_embeds, kv=node_embeds, mask=mask)\n",
      "   504      1001          0.0      0.0      0.0              actor_self_embeds = self_embeds\n",
      "   505                                           \n",
      "   506                                                   # Critic block\n",
      "   507      1001          0.0      0.0      0.0          if self.independent_critic and critic:\n",
      "   508                                                       # Feature embedding\n",
      "   509                                                       self_pos_embeds = self.critic_self_pos_embed(self_entities[..., :self.positional_dim])\n",
      "   510                                                       node_pos_embeds = self.critic_node_pos_embed(node_entities[..., :self.positional_dim])\n",
      "   511                                                       if feature_embeds is not None: self_feat_embeds, node_feat_embeds = feature_embeds.pop(0)\n",
      "   512                                                       else:\n",
      "   513                                                           self_feat_embeds = self.critic_self_feat_embed(self_entities[..., self.positional_dim:])\n",
      "   514                                                           node_feat_embeds = self.critic_node_feat_embed(node_entities[..., self.positional_dim:])\n",
      "   515                                                           feature_embeds_ret.append((self_feat_embeds, node_feat_embeds))\n",
      "   516                                                       # Self embeddings\n",
      "   517                                                       self_embeds = self.critic_self_embed(self_pos_embeds+self_feat_embeds)\n",
      "   518                                                       # Node embeddings\n",
      "   519                                                       node_embeds = self.critic_node_embed(node_pos_embeds+node_feat_embeds)\n",
      "   520                                                       # Attention\n",
      "   521                                                       for block in self.critic_residual_attention_blocks:\n",
      "   522                                                           self_embeds = block(self_embeds, kv=node_embeds, mask=mask)\n",
      "   523                                                       actor_self_embeds = self_embeds\n",
      "   524      1001          0.0      0.0      0.0          else: critic_self_embeds = actor_self_embeds\n",
      "   525                                           \n",
      "   526                                                   # NOTE: fit_and_strip breaks compatibility on batch-batch native programs (Grouped batches)\n",
      "   527      1001          0.0      0.0      0.0          if fit_and_strip and self_entities.dim() > 2:\n",
      "   528                                                       # Strip padded entries\n",
      "   529                                                       actor_self_embeds = actor_self_embeds[~actor_self_embeds.sum(dim=-1).isnan()]\n",
      "   530                                                       if critic: critic_self_embeds = critic_self_embeds[~critic_self_embeds.sum(dim=-1).isnan()]\n",
      "   531                                           \n",
      "   532                                                   # Decisions, samples, and returns\n",
      "   533      1001          0.0      0.0      0.0          ret = ()\n",
      "   534      1001          0.0      0.0      0.0          if actor:  # action_means\n",
      "   535      1000          0.1      0.0      2.9              self_action_embeds = self.actor_decider(actor_self_embeds)\n",
      "   536      1000          0.1      0.0      4.8              action_embeds = self.action_embed(self.actions)\n",
      "   537                                                       # Norms\n",
      "   538      1000          0.1      0.0      2.5              self_action_embeds = self_action_embeds / self_action_embeds.norm(p=2, keepdim=True, dim=-1)\n",
      "   539      1000          0.0      0.0      1.5              action_embeds = action_embeds / action_embeds.norm(p=2, keepdim=True, dim=-1)\n",
      "   540                                                       # Dot/cosine\n",
      "   541      1000          0.0      0.0      0.0              if self_action_embeds.dim() > 2:\n",
      "   542                                                           action_means = torch.einsum('bik,jk->bij', self_action_embeds, action_embeds)\n",
      "   543      1000          0.0      0.0      2.0              else: action_means = torch.einsum('ik,jk->ij', self_action_embeds, action_embeds)\n",
      "   544      1000          0.0      0.0      0.0              ret += (action_means,)\n",
      "   545      1000          0.4      0.0     17.0              if sample: ret += self.select_action(action_means, action=action, return_entropy=entropy)  # action, action_log, dist_entropy\n",
      "   546      1001          0.1      0.0      3.5          if critic: ret += (self.critic_decider(critic_self_embeds).squeeze(-1),)  # state_vals\n",
      "   547      1001          0.0      0.0      0.0          if squeeze and self_entities.dim() > 2 and not fit_and_strip:\n",
      "   548                                                       ret = tuple(t.flatten(0, 1) for t in ret)\n",
      "   549      1001          0.0      0.0      0.0          if return_feature_embeds: ret += feature_embeds_ret,\n",
      "   550      1001          0.0      0.0      0.0          return ret\n",
      "\n",
      "Total time: 8.54417 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: forward at line 1147\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "  1147                                               def forward(\n",
      "  1148                                                   self, compressed_state, *,\n",
      "  1149                                                   keys=None, memory=None, forward_batch_size=None, terminal=False,\n",
      "  1150                                                   feature_embeds=None, return_feature_embeds=False):\n",
      "  1151                                                   # Data Checks\n",
      "  1152      1001          0.0      0.0      0.0          assert compressed_state.shape[0] > 0, 'Empty state matrix passed'\n",
      "  1153      1001          0.0      0.0      0.0          if keys is not None: assert len(keys) == compressed_state.shape[0], (\n",
      "  1154                                                       f'Length of keys vector must equal state dimension 0 ({compressed_state.shape[0]}), '\n",
      "  1155                                                       f'got {len(keys)} instead.'\n",
      "  1156                                                   )\n",
      "  1157                                                       \n",
      "  1158                                                   # Defaults\n",
      "  1159      1001          0.0      0.0      0.0          if forward_batch_size is None: forward_batch_size = self.forward_batch_size\n",
      "  1160      1001          0.0      0.0      0.0          feature_embeds_arg = feature_embeds\n",
      "  1161      1001          0.0      0.0      0.0          construct_feature_embeds = feature_embeds is None and return_feature_embeds\n",
      "  1162                                           \n",
      "  1163                                                   # Act\n",
      "  1164      1001          0.0      0.0      0.2          action = torch.zeros(0, device=self.policy_iteration.device)\n",
      "  1165      1001          0.0      0.0      0.1          action_log = torch.zeros(0, device=self.policy_iteration.device)\n",
      "  1166      1001          0.0      0.0      0.1          state_val = torch.zeros(0, device=self.policy_iteration.device)\n",
      "  1167      2002          0.0      0.0      0.0          for start_idx in range(0, compressed_state.shape[0], forward_batch_size):\n",
      "  1168      4004          0.0      0.0      0.2              state = _utility.processing.split_state(\n",
      "  1169      1001          0.0      0.0      0.0                  compressed_state,\n",
      "  1170      1001          0.0      0.0      0.1                  idx=np.arange(start_idx, min(start_idx+forward_batch_size, compressed_state.shape[0])),\n",
      "  1171      1001          0.0      0.0      0.0                  **self.split_args)\n",
      "  1172      1001          0.0      0.0      0.0              if not terminal:\n",
      "  1173      2000          2.3      0.0     26.6                  _, action_sub, action_log_sub, state_val_sub, feature_embeds_sub = self.actor_critic(\n",
      "  1174      1000          0.0      0.0      0.0                      *state, sample=True, critic=True, feature_embeds=feature_embeds_arg, return_feature_embeds=True)\n",
      "  1175      1000          0.0      0.0      0.2                  action = torch.concat((action, action_sub), dim=0)\n",
      "  1176      1000          0.0      0.0      0.2                  action_log = torch.concat((action_log, action_log_sub), dim=0)\n",
      "  1177         2          0.0      0.0      0.0              else: state_val_sub, feature_embeds_sub = self.actor_critic(\n",
      "  1178         1          0.0      0.0      0.0                  *state, actor=False, critic=True, feature_embeds=feature_embeds_arg, return_feature_embeds=True)\n",
      "  1179      1001          0.0      0.0      0.1              state_val = torch.concat((state_val, state_val_sub), dim=0)\n",
      "  1180      1001          0.0      0.0      0.0              if construct_feature_embeds:\n",
      "  1181         1          0.0      0.0      0.0                  if feature_embeds is None: feature_embeds = feature_embeds_sub\n",
      "  1182                                                           else:\n",
      "  1183                                                               feature_embeds = [\n",
      "  1184                                                                   tuple(torch.concat((feature_embeds[i][j], t)) for j, t in enumerate(feat_tensors))\n",
      "  1185                                                                   for i, feat_tensors in enumerate(feature_embeds_sub)]\n",
      "  1186                                           \n",
      "  1187                                                   # Unstandardize state_val\n",
      "  1188      1001          0.0      0.0      0.3          state_val = self.return_standardization.remove(state_val, mean=False)\n",
      "  1189                                                   \n",
      "  1190                                                   # Record\n",
      "  1191                                                   # NOTE: `reward` and `is_terminal` are added outside of the class, calculated\n",
      "  1192                                                   # after stepping the environment\n",
      "  1193      1001          0.0      0.0      0.0          if memory is not None and keys is not None:  #  and self.training\n",
      "  1194      1001          0.0      0.0      0.0              if not terminal:\n",
      "  1195      2000          6.1      0.0     71.7                  memory.record_buffer(\n",
      "  1196      1000          0.0      0.0      0.0                      keys=keys,\n",
      "  1197      1000          0.0      0.0      0.0                      states=compressed_state,\n",
      "  1198      1000          0.0      0.0      0.0                      actions=action,\n",
      "  1199      1000          0.0      0.0      0.0                      action_logs=action_log,\n",
      "  1200      1000          0.0      0.0      0.0                      state_vals=state_val)\n",
      "  1201                                                       else:\n",
      "  1202         2          0.0      0.0      0.1                  memory.record_buffer(\n",
      "  1203         1          0.0      0.0      0.0                      terminal_states=compressed_state,\n",
      "  1204         1          0.0      0.0      0.0                      terminal_state_vals=state_val)\n",
      "  1205      1001          0.0      0.0      0.0          ret = action,\n",
      "  1206      1001          0.0      0.0      0.0          if return_feature_embeds: ret += feature_embeds,\n",
      "  1207      1001          0.0      0.0      0.0          return _utility.general.clean_return(ret)\n",
      "\n",
      "Total time: 10.9632 s\n",
      "File: /home/thema/repos/inept/celltrip/train.py\n",
      "Function: simulate_until_completion at line 19\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    19                                           def simulate_until_completion(\n",
      "    20                                               env, policy, memory=None, keys=None,\n",
      "    21                                               cache_feature_embeds=True, store_states=False, flush=True,\n",
      "    22                                               dummy=False, verbose=False):\n",
      "    23                                               # NOTE: Does not flush buffer\n",
      "    24                                               # Params\n",
      "    25         1          0.0      0.0      0.0      if keys is None: keys = env.get_keys()\n",
      "    26                                           \n",
      "    27                                               # Store states\n",
      "    28         1          0.0      0.0      0.0      if store_states: state_storage = [env.get_state()]\n",
      "    29                                           \n",
      "    30                                               # Simulation\n",
      "    31         1          0.0      0.0      0.0      ep_timestep = 0; ep_reward = 0; ep_itemized_reward = defaultdict(lambda: 0); finished = False\n",
      "    32         1          0.0      0.0      0.0      feature_embeds = None\n",
      "    33         2          0.0      0.0      0.0      with torch.inference_mode():\n",
      "    34      1001          0.0      0.0      0.0          while not finished:\n",
      "    35                                                       # Get current state\n",
      "    36      1000          0.0      0.0      0.3              state = env.get_state(include_modalities=True)\n",
      "    37                                           \n",
      "    38                                                       # Get actions from policy\n",
      "    39      2000          8.6      0.0     78.1              actions = policy(\n",
      "    40      1000          0.0      0.0      0.0                  state, keys=keys, memory=memory,\n",
      "    41      1000          0.0      0.0      0.0                  feature_embeds=feature_embeds, return_feature_embeds=cache_feature_embeds)\n",
      "    42      1000          0.0      0.0      0.0              if cache_feature_embeds: actions, feature_embeds = actions\n",
      "    43                                           \n",
      "    44                                                       # Step environment and get reward\n",
      "    45      1000          1.2      0.0     10.6              rewards, finished, itemized_rewards = env.step(actions, return_itemized_rewards=True)\n",
      "    46                                           \n",
      "    47                                                       # Store states\n",
      "    48      1000          0.0      0.0      0.0              if store_states: state_storage.append(env.get_state())\n",
      "    49                                           \n",
      "    50                                                       # Record rewards\n",
      "    51      1000          1.0      0.0      9.2              if memory is not None: memory.record_buffer(rewards=rewards, is_terminals=finished)\n",
      "    52                                           \n",
      "    53                                                       # Tracking\n",
      "    54      1000          0.0      0.0      0.3              ts_reward = rewards.cpu().mean()\n",
      "    55      1000          0.0      0.0      0.0              ep_reward = ep_reward + ts_reward\n",
      "    56      6000          0.0      0.0      0.0              for k, v in itemized_rewards.items():\n",
      "    57      5000          0.1      0.0      1.3                  ep_itemized_reward[k] += v.cpu().mean()\n",
      "    58      1000          0.0      0.0      0.0              ep_timestep += 1\n",
      "    59                                           \n",
      "    60                                                       # Dummy return for testing\n",
      "    61      1000          0.0      0.0      0.0              if dummy:\n",
      "    62                                                           if not finished:\n",
      "    63                                                               # Fill all but first and last\n",
      "    64                                                               ep_timestep = env.max_timesteps\n",
      "    65                                                               memory.flush_buffer()\n",
      "    66                                                               for _ in range(ep_timestep-2):\n",
      "    67                                                                   if memory is not None: memory.append_memory({k: v[-1:] for k, v in memory.storage.items()})\n",
      "    68                                                                   if store_states: state_storage.append(env.get_state)\n",
      "    69                                                               memory.storage['is_terminals'][-1] = True\n",
      "    70                                                   \n",
      "    71                                                       # CLI\n",
      "    72      1000          0.0      0.0      0.0              if verbose and ((ep_timestep % 200 == 0) or ep_timestep in (100,) or finished):\n",
      "    73                                                           print(f'Timestep {ep_timestep:>4} - Reward {ts_reward:.3f}')\n",
      "    74                                                   \n",
      "    75                                                   # Record terminals\n",
      "    76         1          0.0      0.0      0.0          state = env.get_state(include_modalities=True)\n",
      "    77         1          0.0      0.0      0.1          policy(state, keys=keys, memory=memory, terminal=True, feature_embeds=feature_embeds)\n",
      "    78                                           \n",
      "    79                                               # Flush\n",
      "    80         1          0.0      0.0      0.0      if flush and memory: memory.flush_buffer()\n",
      "    81                                           \n",
      "    82                                               # Summarize and return\n",
      "    83         1          0.0      0.0      0.0      denominator = 1  # env.max_timesteps if env.max_timesteps is not None else ep_timestep\n",
      "    84         1          0.0      0.0      0.0      ep_reward = (ep_reward / denominator).item()  # Standard mean\n",
      "    85         1          0.0      0.0      0.0      ep_itemized_reward = {k: (v / denominator).item() for k, v in ep_itemized_reward.items()}\n",
      "    86         1          0.0      0.0      0.0      ret = (ep_timestep, ep_reward, ep_itemized_reward)\n",
      "    87         1          0.0      0.0      0.0      if store_states:\n",
      "    88                                                   state_storage = torch.stack(state_storage)\n",
      "    89                                                   ret += (state_storage,)\n",
      "    90         1          0.0      0.0      0.0      return ret\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Forward\n",
    "import line_profiler\n",
    "memory.mark_sampled()\n",
    "prof = line_profiler.LineProfiler(\n",
    "    celltrip.train.simulate_until_completion,\n",
    "    celltrip.policy.PPO.forward, celltrip.policy.EntitySelfAttentionLite.forward, celltrip.policy.ResidualAttentionBlock.forward,\n",
    "    celltrip.environment.EnvironmentBase.step)\n",
    "while memory.get_new_steps() < 1_000:\n",
    "    env.reset()\n",
    "    ret = prof.runcall(celltrip.train.simulate_until_completion, env, policy, memory)\n",
    "    print(', '.join([f'{k}: {v:.3f}' for k, v in ret[2].items()]))\n",
    "memory.compute_advantages()\n",
    "prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pull from memory\n",
    "# import line_profiler\n",
    "# prof = line_profiler.LineProfiler(\n",
    "#     celltrip.memory.AdvancedMemoryBuffer.fast_sample,\n",
    "#     celltrip.memory.AdvancedMemoryBuffer._concat_states)\n",
    "# ret = prof.runcall(memory.fast_sample, 512, shuffle=False, max_samples_per_state=np.inf)\n",
    "# prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy.minibatch_size = 8*2_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/repos/inept/celltrip/policy.py:1411: UserWarning: No group \"learners\" found.\n",
      "  warnings.warn(f'No group \"{group}\" found.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 01 - Total (0.00164) + PPO (-0.00000) + critic (0.00332) + entropy (-0.00213) + KL (0.00001) :: Log STD (0.00192) :: Advantage STD (1.00000)\n",
      "Iteration 05 - Total (0.00032) + PPO (-0.00000) + critic (0.00068) + entropy (-0.00214) + KL (0.00001) :: Log STD (0.00952) :: Advantage STD (1.00000)\n",
      "Iteration 10 - Total (0.00023) + PPO (-0.00001) + critic (0.00052) + entropy (-0.00215) + KL (0.00001) :: Log STD (0.01262) :: Advantage STD (1.00000)\n",
      "Total: 0.000, PPO: -0.000, critic: 0.001, entropy: -0.002, KL: 0.000\n",
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 0.395045 s\n",
      "File: /home/thema/repos/inept/celltrip/memory.py\n",
      "Function: fast_sample at line 178\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   178                                               def fast_sample(\n",
      "   179                                                   self, num_memories, replay_frac=None, max_samples_per_state=None,\n",
      "   180                                                   uniform=None, shuffle=None, efficient=True, round_sample='down'):\n",
      "   181                                                   # NOTE: Shuffle should only be used when sequential sampling is taking place\n",
      "   182                                                   # Parameters\n",
      "   183        80          0.0      0.0      0.0          if replay_frac is None: replay_frac = self.replay_frac\n",
      "   184        80          0.0      0.0      0.0          if max_samples_per_state is None: max_samples_per_state = self.max_samples_per_state\n",
      "   185        80          0.0      0.0      0.0          if uniform is None: uniform = self.uniform\n",
      "   186        80          0.0      0.0      0.0          if shuffle is None: shuffle = self.shuffle\n",
      "   187        80          0.0      0.0      0.0          num_replay_memories = int(replay_frac * num_memories)\n",
      "   188        80          0.0      0.0      0.0          num_new_memories = num_memories - num_replay_memories\n",
      "   189                                           \n",
      "   190                                                   # Adjust proportions if needed\n",
      "   191        80          0.0      0.0      0.3          total_new_memories = self.get_new_len(max_per_step=max_samples_per_state)\n",
      "   192        80          0.0      0.0      0.1          total_replay_memories = self.get_replay_len(max_per_step=max_samples_per_state)\n",
      "   193        80          0.0      0.0      0.0          if total_new_memories+total_replay_memories < num_memories:\n",
      "   194                                                       raise RuntimeError(\n",
      "   195                                                           f'Only {total_new_memories+total_replay_memories} possible samples'\n",
      "   196                                                           f' with current parameters, {num_memories} requested')\n",
      "   197        80          0.0      0.0      0.0          adjusted = False\n",
      "   198        80          0.0      0.0      0.0          if total_new_memories < num_new_memories:\n",
      "   199                                                       num_replay_memories += num_new_memories - total_new_memories\n",
      "   200                                                       num_new_memories = total_new_memories\n",
      "   201                                                       adjusted = True\n",
      "   202        80          0.0      0.0      0.0          elif total_replay_memories < num_replay_memories:\n",
      "   203                                                       num_new_memories += num_replay_memories - total_replay_memories\n",
      "   204                                                       num_replay_memories = total_replay_memories\n",
      "   205                                                       adjusted = True\n",
      "   206        80          0.0      0.0      0.0          if adjusted:\n",
      "   207                                                       new_replay_frac = num_replay_memories / (num_replay_memories+num_new_memories)\n",
      "   208                                                       warnings.warn(\n",
      "   209                                                           f'Current `replay_frac` ({replay_frac:.3f}) infeasible,'\n",
      "   210                                                           f' adjusting to {new_replay_frac:.3f}')\n",
      "   211                                           \n",
      "   212                                                   # Initialization\n",
      "   213        80          0.0      0.0      0.0          ret = defaultdict(lambda: [])\n",
      "   214                                                   # memory_indices = []\n",
      "   215                                           \n",
      "   216                                                   # List order\n",
      "   217        80          0.0      0.0      0.3          list_order = np.arange(len(self.storage['keys']))\n",
      "   218        80          0.0      0.0      0.0          if not uniform:\n",
      "   219                                                       # Get random list order\n",
      "   220        80          0.0      0.0      0.5              np.random.shuffle(list_order)\n",
      "   221                                                   else:\n",
      "   222                                                       # Uniform sampling (could also work with duplicates)\n",
      "   223                                                       new_memories_to_record = np.random.choice(self.get_new_len(), num_new_memories, replace=False)\n",
      "   224                                                       replay_memories_to_record = np.random.choice(self.get_replay_len(), num_replay_memories, replace=False)\n",
      "   225                                           \n",
      "   226                                                   # Search for index\n",
      "   227        80          0.0      0.0      0.1          num_replay_memories_recorded, num_new_memories_recorded = np.array(0), np.array(0)\n",
      "   228        80          0.0      0.0      0.0          for list_num in list_order:\n",
      "   229                                                       # Check if should sample\n",
      "   230        80          0.0      0.0      0.0              if self.storage['staleness'][list_num] == 0:\n",
      "   231        80          0.0      0.0      0.0                  working_memories_recorded = num_new_memories_recorded\n",
      "   232        80          0.0      0.0      0.0                  working_memories = num_new_memories\n",
      "   233        80          0.0      0.0      0.0                  if uniform: working_memories_to_record = new_memories_to_record\n",
      "   234                                                       else:\n",
      "   235                                                           working_memories_recorded = num_replay_memories_recorded\n",
      "   236                                                           working_memories = num_replay_memories\n",
      "   237                                                           if uniform: working_memories_to_record = replay_memories_to_record\n",
      "   238        80          0.0      0.0      0.1              if working_memories_recorded >= working_memories: continue\n",
      "   239                                           \n",
      "   240                                                       # Choose random samples\n",
      "   241        80          0.0      0.0      0.0              list_len = len(self.storage['keys'][list_num])\n",
      "   242        80          0.0      0.0      0.0              if not uniform:\n",
      "   243                                                           # Greedily add\n",
      "   244        80          0.0      0.0      0.0                  num_memories_to_add = min(list_len, max_samples_per_state)\n",
      "   245        80          0.0      0.0      0.1                  if working_memories_recorded + num_memories_to_add > working_memories:\n",
      "   246                                                               num_memories_to_add = working_memories-working_memories_recorded\n",
      "   247        80          0.0      0.0      0.0                  if list_len != num_memories_to_add and round_sample != 'up':\n",
      "   248                                                               if round_sample is None:\n",
      "   249                                                                   self_idx = np.random.choice(list_len, num_memories_to_add, replace=False)\n",
      "   250                                                               elif round_sample == 'down': break\n",
      "   251                                                               else: raise RuntimeError(f'`round_sample` method `{round_sample}` not implemented.')\n",
      "   252        80          0.0      0.0      0.1                  else: self_idx = np.arange(list_len)\n",
      "   253                                                       else:\n",
      "   254                                                           # Uniformly add\n",
      "   255                                                           mask = working_memories_to_record < list_len\n",
      "   256                                                           num_memories_to_add = mask.sum()\n",
      "   257                                                           if num_memories_to_add == 0:\n",
      "   258                                                               working_memories_to_record -= list_len\n",
      "   259                                                               continue\n",
      "   260                                                           self_idx = working_memories_to_record[mask].copy()\n",
      "   261                                                           working_memories_to_record[mask] = len(self)  # Kinda hacky, but works\n",
      "   262                                                           working_memories_to_record -= list_len\n",
      "   263                                           \n",
      "   264                                                       # Get values\n",
      "   265      1120          0.0      0.0      0.1              for k in self.storage:\n",
      "   266                                                           # Skip certain keys\n",
      "   267      1040          0.0      0.0      0.1                  if k not in ('states', 'actions', 'action_logs', 'state_vals', 'advantages', 'propagated_rewards', 'staleness'): continue\n",
      "   268                                           \n",
      "   269                                                           # Special cases\n",
      "   270       560          0.0      0.0      0.0                  if k == 'states':\n",
      "   271       320          0.0      0.0      1.1                      val = _utility.processing.split_state(\n",
      "   272       160          0.0      0.0      5.9                          self._append_suffix(\n",
      "   273        80          0.0      0.0      0.0                              self.storage[k][list_num],\n",
      "   274        80          0.0      0.0      0.0                              keys=self.storage['keys'][list_num]),\n",
      "   275        80          0.0      0.0      0.0                          idx=self_idx,\n",
      "   276        80          0.0      0.0      0.0                          **self.split_args)\n",
      "   277                                                               \n",
      "   278                                                           # Single value case\n",
      "   279       480          0.0      0.0      0.0                  elif k in ('staleness',):\n",
      "   280        80          0.0      0.0      6.8                      val = torch.tensor(len(self_idx)*[self.storage[k][list_num]], device=self.device)\n",
      "   281                                           \n",
      "   282                                                           # Main case\n",
      "   283                                                           else: \n",
      "   284       400          0.0      0.0      0.0                      if k in ('propagated_rewards', 'normalized_rewards') and self.storage[k][list_num] is None:\n",
      "   285                                                                   raise ValueError('Make sure to run `self.propagate_rewards(); self.normalize_rewards()` before sampling.')\n",
      "   286       400          0.0      0.0      0.0                      if k in ('advantages',) and self.storage[k][list_num] is None:\n",
      "   287                                                                   raise ValueError('Make sure to run `self.compute_advantages()` before sampling.')\n",
      "   288       400          0.0      0.0      0.9                      if (len(self_idx) == self.storage[k][list_num].shape[0]) and (self_idx[:-1] < self_idx[1:]).all():\n",
      "   289       400          0.0      0.0      0.1                          val = self.storage[k][list_num]\n",
      "   290                                                               else: val = self.storage[k][list_num][self_idx]\n",
      "   291                                           \n",
      "   292                                                           # Record\n",
      "   293       560          0.0      0.0      0.2                  ret[k].append(val)\n",
      "   294                                           \n",
      "   295                                                       # Record memory indices and iterate\n",
      "   296                                                       # memory_indices += [(list_num, i) for i in self_idx]\n",
      "   297        80          0.0      0.0      0.0              if self.storage['staleness'][list_num] == 0:\n",
      "   298        80          0.0      0.0      0.2                  num_new_memories_recorded += num_memories_to_add\n",
      "   299                                                       else:\n",
      "   300                                                           num_replay_memories_recorded += num_memories_to_add\n",
      "   301                                           \n",
      "   302                                                       # Break if enough memories retrieved\n",
      "   303        80          0.0      0.0      0.1              if num_replay_memories_recorded + num_new_memories_recorded >= num_memories: break\n",
      "   304                                           \n",
      "   305                                                   # Catch if too few\n",
      "   306                                                   else: warnings.warn(\n",
      "   307                                                       f'Only able to gather {num_replay_memories_recorded + num_new_memories_recorded}'\n",
      "   308                                                       f' memories with current parameters, {num_memories} requested.')\n",
      "   309                                           \n",
      "   310                                                   # Stack tensors\n",
      "   311       640          0.0      0.0      0.1          for k in ret:\n",
      "   312       560          0.3      0.0     81.0              if k == 'states': ret[k] = self._concat_states(ret[k], efficient=efficient)\n",
      "   313       480          0.0      0.0      1.3              else: ret[k] = torch.concat(ret[k], dim=0)\n",
      "   314                                                   # memory_indices = torch.tensor(self._index_to_flat_index(memory_indices))\n",
      "   315                                           \n",
      "   316                                                   # Shuffle\n",
      "   317                                                   # NOTE: Not reproducible currently, takes maybe .1 seconds for 10k but\n",
      "   318                                                   #       is roughly Omega(N^1.1)\n",
      "   319        80          0.0      0.0      0.0          if shuffle:\n",
      "   320                                                       perm = torch.randperm(num_memories)\n",
      "   321                                                       for k in ret:\n",
      "   322                                                           if k == 'states': ret[k] = [s[perm] for s in ret[k]]\n",
      "   323                                                           else: ret[k] = ret[k][perm]\n",
      "   324                                                       # memory_indices = memory_indices[perm]\n",
      "   325                                           \n",
      "   326                                                   # Return\n",
      "   327        80          0.0      0.0      0.1          return dict(ret)  # memory_indices\n",
      "\n",
      "Total time: 0.313697 s\n",
      "File: /home/thema/repos/inept/celltrip/memory.py\n",
      "Function: _concat_states at line 561\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   561                                               def _concat_states(self, states, efficient=False):\n",
      "   562                                                   # Pad with duplicate nodes when not sufficient\n",
      "   563                                                   # NOTE: Inefficient, nested tensor doesn't have enough\n",
      "   564                                                   # functionality yet\n",
      "   565        80          0.0      0.0      0.0          if len(states[0]) == 2:\n",
      "   566                                                       # Regular case\n",
      "   567                                                       shapes = [s[1].shape[1] for s in states]\n",
      "   568                                                       max_nodes = max(shapes)\n",
      "   569                                                       states = [\n",
      "   570                                                           torch.concat([\n",
      "   571                                                               s[i]\n",
      "   572                                                               if i == 0 or np.ceil(max_nodes/s[i].shape[1]) == 1 else\n",
      "   573                                                               s[i].repeat(  # TODO: Maybe do NAN instead? Make sure policy can handle it\n",
      "   574                                                                   1, int(np.ceil(max_nodes/s[i].shape[1])), 1)[:, :max_nodes]\n",
      "   575                                                               for s in states],\n",
      "   576                                                           dim=0) for i in range(2)]\n",
      "   577                                                   # elif (np.array([len(s) for s in states]) == 1).all():\n",
      "   578        80          0.0      0.0      0.0          elif efficient:\n",
      "   579                                                       # Lite case (more memory and compute efficient)\n",
      "   580                                                       # NOTE: Shuffle currently incompatible\n",
      "   581                                                       # NOTE: Screws with policy indexing, need to use batch with no minibatches \n",
      "   582                                                       # TODO: Implement for else case, with more uneven node dims\n",
      "   583        80          0.1      0.0     39.0              states = [(s[0], s[0], torch.eye(s[0].shape[0], device=self.device)) if len(s) == 1 else s for s in states]\n",
      "   584        80          0.0      0.0      0.2              max_self_shape = max(s[0].shape[0] for s in states)\n",
      "   585        80          0.0      0.0      0.1              max_node_shape = max(s[1].shape[0] for s in states)\n",
      "   586        80          0.0      0.0      3.4              s0 = torch.zeros((len(states), max_self_shape, states[0][0].shape[-1]), device=self.device)\n",
      "   587       160          0.0      0.0      4.4              for i, s in enumerate(states): s = s[0]; s0[i, :s.shape[0], :s.shape[1]] = s\n",
      "   588        80          0.0      0.0      3.0              s1 = torch.zeros((len(states), max_node_shape, states[0][1].shape[-1]), device=self.device)\n",
      "   589       160          0.0      0.0      2.5              for i, s in enumerate(states): s = s[1]; s1[i, :s.shape[0], :s.shape[1]] = s\n",
      "   590        80          0.0      0.0     11.5              s2 = torch.ones((len(states), max_self_shape, max_node_shape), dtype=torch.bool, device=self.device)\n",
      "   591       160          0.1      0.0     35.8              for i, s in enumerate(states): s = s[2]; s2[i, :s.shape[0], :s.shape[1]] = s\n",
      "   592        80          0.0      0.0      0.0              states = [s0, s1, s2]\n",
      "   593                                                   else:\n",
      "   594                                                       # Lite case (not memory or compute efficient, but closer to previous format for non-lite)\n",
      "   595                                                       # Unfold to all 3 representations\n",
      "   596                                                       states = [(s[0], s[0], torch.eye(s[0].shape[0], device=self.device)) if len(s) == 1 else s for s in states]\n",
      "   597                                                       # Shape, Pad, and concat\n",
      "   598                                                       states = [(se.unsqueeze(1), no.expand(se.shape[0], *no.shape), ma.unsqueeze(1)) for se, no, ma in states]\n",
      "   599                                                       batch_num = sum(s[0].shape[0] for s in states)\n",
      "   600                                                       max_node_shape = max(s[1].shape[1] for s in states)\n",
      "   601                                                       max_mask_shape = max(s[2].shape[2] for s in states)\n",
      "   602                                                       s0 = torch.concat([s[0] for s in states], dim=0)\n",
      "   603                                                       # torch.concat([F.pad(s[1], (0, 0, 0, max_node_shape-s[1].shape[1]), value=0) for s in states], dim=0),  # Wanted to pad with nan, but nan*0=nan\n",
      "   604                                                       s1 = torch.zeros((batch_num, max_node_shape, states[0][1].shape[-1]), device=self.device)\n",
      "   605                                                       for i, s in enumerate(states):\n",
      "   606                                                           s = s[1]\n",
      "   607                                                           s1[i*s.shape[0]:(i+1)*s.shape[0], :s.shape[1], :s.shape[2]] = s[0]\n",
      "   608                                                       s2 = torch.zeros((batch_num, 1, max_mask_shape), device=self.device)\n",
      "   609                                                       # torch.concat([F.pad(s[2], (0, max_mask_shape-s[2].shape[2], 0, 0), value=True) for s in states], dim=0)\n",
      "   610                                                       for i, s in enumerate(states):\n",
      "   611                                                           s = s[2]\n",
      "   612                                                           s2[i*s.shape[0]:(i+1)*s.shape[0], :, :s.shape[2]] = s[0]\n",
      "   613                                                       states = [s0, s1, s2]\n",
      "   614                                           \n",
      "   615        80          0.0      0.0      0.0          return states\n",
      "\n",
      "Total time: 2.72269 s\n",
      "File: /home/thema/repos/inept/celltrip/policy.py\n",
      "Function: update at line 1230\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "  1230                                               def update(\n",
      "  1231                                                   self,\n",
      "  1232                                                   memory,\n",
      "  1233                                                   update_iterations=None,\n",
      "  1234                                                   standardize_returns=False,\n",
      "  1235                                                   verbose=False,\n",
      "  1236                                                   # Collective args\n",
      "  1237                                                   sync_iterations=None,\n",
      "  1238                                                   **kwargs,\n",
      "  1239                                               ):\n",
      "  1240                                                   # NOTE: The number of epochs is spread across `world_size` workers\n",
      "  1241                                                   # NOTE: Assumes col.init_collective_group has already been called if world_size > 1\n",
      "  1242                                                   # Parameters\n",
      "  1243         1          0.0      0.0      0.0          if update_iterations is None: update_iterations = self.update_iterations\n",
      "  1244         1          0.0      0.0      0.0          if sync_iterations is None: sync_iterations = self.sync_iterations\n",
      "  1245                                           \n",
      "  1246                                                   # Collective operations\n",
      "  1247         1          0.0      0.0      0.0          use_collective = col.is_group_initialized('default')\n",
      "  1248                                           \n",
      "  1249                                                   # Batch parameters\n",
      "  1250         1          0.0      0.0      0.0          level_dict = {'pool': 0, 'epoch': 1, 'batch': 2, 'minibatch': 3}\n",
      "  1251         1          0.0      0.0      0.0          load_level = level_dict[self.load_level]\n",
      "  1252         1          0.0      0.0      0.0          cast_level = level_dict[self.cast_level]\n",
      "  1253         1          0.0      0.0      0.0          assert cast_level >= load_level, 'Cannot cast without first loading'\n",
      "  1254                                           \n",
      "  1255                                                   # Determine level sizes\n",
      "  1256         1          0.0      0.0      0.0          memory_size = len(memory)\n",
      "  1257         1          0.0      0.0      0.0          pool_size = self.pool_size\n",
      "  1258         1          0.0      0.0      0.0          if pool_size is not None: pool_size = int(min(pool_size, memory_size))\n",
      "  1259         1          0.0      0.0      0.0          epoch_size = self.epoch_size if not (self.epoch_size is None and pool_size is not None) else pool_size\n",
      "  1260         1          0.0      0.0      0.0          if epoch_size is not None and pool_size is not None: epoch_size = int(min(epoch_size, pool_size))\n",
      "  1261         1          0.0      0.0      0.0          batch_size = self.batch_size if not (self.batch_size is None and epoch_size is not None) else epoch_size\n",
      "  1262         1          0.0      0.0      0.0          if batch_size is not None and epoch_size is not None: batch_size = int(min(batch_size, epoch_size))\n",
      "  1263         1          0.0      0.0      0.0          minibatch_size = self.minibatch_size if not (self.minibatch_size is None and batch_size is not None) else batch_size\n",
      "  1264         1          0.0      0.0      0.0          if minibatch_size is not None and batch_size is not None: minibatch_size = int(min(minibatch_size, batch_size))\n",
      "  1265         1          0.0      0.0      0.0          if sync_iterations == 1:\n",
      "  1266                                                       # Adjust batch size if gradients are synchronized\n",
      "  1267         1          0.0      0.0      0.0              if batch_size is not None: batch_size = np.ceil(batch_size / self.get_world_size('learners')).astype(int)\n",
      "  1268                                                       else: minibatch_size = np.ceil(minibatch_size / self.get_world_size('learners')).astype(int)\n",
      "  1269                                           \n",
      "  1270                                                   # Cap at max size to reduce redundancy for sequential samples\n",
      "  1271                                                   # NOTE: Pool->epoch is the only non-sequential sample, and is thus not included here\n",
      "  1272                                                   # max_unique_memories = batch_size * update_iterations\n",
      "  1273                                                   # epoch_size = min(epoch_size, max_unique_memories)\n",
      "  1274                                           \n",
      "  1275                                                   # Update moving return mean\n",
      "  1276         1          0.0      0.0      0.0          if standardize_returns:\n",
      "  1277                                                       # TODO: Clean this up if it works\n",
      "  1278                                                       advantages = torch.concat([memory.storage['advantages'][i] for i in range(memory.get_steps()) if memory.storage['staleness'][i]==0]).to(self.policy_iteration.device)\n",
      "  1279                                                       state_vals = torch.concat([memory.storage['state_vals'][i] for i in range(memory.get_steps()) if memory.storage['staleness'][i]==0]).to(self.policy_iteration.device)\n",
      "  1280                                                       self.return_standardization.update(advantages - state_vals)\n",
      "  1281                                           \n",
      "  1282                                                   # Load pool\n",
      "  1283         1          0.0      0.0      0.0          total_losses = defaultdict(lambda: [])\n",
      "  1284         4          0.0      0.0      0.0          pool_data = _utility.processing.sample_and_cast(\n",
      "  1285         1          0.0      0.0      0.0              memory, None, None, pool_size,\n",
      "  1286         1          0.0      0.0      0.0              current_level=0, load_level=load_level, cast_level=cast_level,\n",
      "  1287         2          0.0      0.0      0.0              device=self.policy_iteration.device, **kwargs)\n",
      "  1288                                           \n",
      "  1289                                                   # Train\n",
      "  1290         1          0.0      0.0      0.0          iterations = 0; synchronized = True; escape = False\n",
      "  1291        10          0.0      0.0      0.0          while True:\n",
      "  1292                                                       # Load epoch\n",
      "  1293        40          0.0      0.0      0.0              epoch_data = _utility.processing.sample_and_cast(\n",
      "  1294        10          0.0      0.0      0.0                  memory, pool_data, pool_size, epoch_size,\n",
      "  1295        10          0.0      0.0      0.0                  current_level=1, load_level=load_level, cast_level=cast_level,\n",
      "  1296        20          0.0      0.0      0.0                  device=self.policy_iteration.device, **kwargs)\n",
      "  1297        10          0.0      0.0      0.0              batches = np.ceil(epoch_size/batch_size).astype(int) if epoch_size is not None else 1\n",
      "  1298        90          0.0      0.0      0.0              for batch_num in range(batches):\n",
      "  1299                                                           # Load batch\n",
      "  1300        80          0.0      0.0      0.0                  batch_losses = defaultdict(lambda: 0)\n",
      "  1301       320          0.0      0.0      0.0                  batch_data = _utility.processing.sample_and_cast(\n",
      "  1302        80          0.0      0.0      0.0                      memory, epoch_data, epoch_size, batch_size,\n",
      "  1303        80          0.0      0.0      0.0                      current_level=2, load_level=load_level, cast_level=cast_level,\n",
      "  1304        80          0.0      0.0      0.0                      device=self.policy_iteration.device, sequential_num=batch_num,\n",
      "  1305        80          0.0      0.0      0.0                      **kwargs)\n",
      "  1306        80          0.0      0.0      0.1                  minibatches = np.ceil(batch_size/minibatch_size).astype(int) if batch_size is not None else 1\n",
      "  1307       160          0.0      0.0      0.0                  for minibatch_num in range(minibatches):\n",
      "  1308                                                               # Load minibatch\n",
      "  1309       320          0.5      0.0     18.9                      minibatch_data = _utility.processing.sample_and_cast(\n",
      "  1310        80          0.0      0.0      0.0                          memory, batch_data, batch_size, minibatch_size,\n",
      "  1311        80          0.0      0.0      0.0                          current_level=3, load_level=load_level, cast_level=cast_level,\n",
      "  1312        80          0.0      0.0      0.0                          device=self.policy_iteration.device, sequential_num=minibatch_num,\n",
      "  1313        80          0.0      0.0      0.0                          **kwargs)\n",
      "  1314                                           \n",
      "  1315                                                               # Get subset data\n",
      "  1316        80          0.0      0.0      0.0                      states = minibatch_data['states']\n",
      "  1317        80          0.0      0.0      0.0                      actions = minibatch_data['actions']\n",
      "  1318        80          0.0      0.0      0.0                      action_logs = minibatch_data['action_logs']\n",
      "  1319        80          0.0      0.0      0.0                      state_vals = minibatch_data['state_vals']\n",
      "  1320        80          0.0      0.0      0.0                      advantages = minibatch_data['advantages']\n",
      "  1321                                                               # rewards = minibatch_data['propagated_rewards']\n",
      "  1322                                           \n",
      "  1323                                                               # Perform backward\n",
      "  1324       160          1.1      0.0     40.5                      loss, loss_ppo, loss_critic, loss_entropy, loss_kl = self.backward(\n",
      "  1325        80          0.0      0.0      0.0                          states, actions, action_logs, state_vals, advantages=advantages, rewards=None)\n",
      "  1326                                           \n",
      "  1327                                                               # Scale and calculate gradient\n",
      "  1328        80          0.0      0.0      0.0                      accumulation_frac = states[0].shape[0] / batch_size\n",
      "  1329        80          0.0      0.0      0.1                      loss = loss * accumulation_frac\n",
      "  1330        80          0.4      0.0     15.2                      loss.backward()  # Longest computation\n",
      "  1331                                           \n",
      "  1332                                                               # Scale and record\n",
      "  1333        80          0.0      0.0      0.1                      batch_losses['Total'] += loss.detach()\n",
      "  1334        80          0.0      0.0      0.2                      batch_losses['PPO'] += loss_ppo.detach().mean() * accumulation_frac\n",
      "  1335        80          0.0      0.0      0.1                      batch_losses['critic'] += loss_critic.detach().mean() * accumulation_frac\n",
      "  1336        80          0.0      0.0      0.1                      batch_losses['entropy'] += loss_entropy.detach().mean() * accumulation_frac\n",
      "  1337        80          0.0      0.0      0.1                      batch_losses['KL'] += loss_kl.detach().mean() * accumulation_frac\n",
      "  1338                                                           \n",
      "  1339                                                           # Record\n",
      "  1340       480          0.0      0.0      0.0                  for k, v in batch_losses.items(): total_losses[k].append(v)\n",
      "  1341                                           \n",
      "  1342                                                           # Synchronize GPU policies and step\n",
      "  1343                                                           # NOTE: Synchronize gradients every batch if =1, else synchronize whole model\n",
      "  1344                                                           # NOTE: =1 keeps optimizers in sync without need for whole-model synchronization\n",
      "  1345        80          0.0      0.0      0.1                  if sync_iterations == 1: self.synchronize('learners', grad=True)  # Sync only grad\n",
      "  1346        80          0.0      0.0      0.0                  if self.kl_early_stop and synchronized: self.copy_policy()\n",
      "  1347        80          0.1      0.0      2.8                  nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.grad_clip)\n",
      "  1348        80          0.1      0.0      5.3                  self.optimizer.step()\n",
      "  1349        80          0.0      0.0      0.5                  self.optimizer.zero_grad()\n",
      "  1350        80          0.0      0.0      0.0                  if sync_iterations != 1:\n",
      "  1351                                                               # Synchronize for offsets\n",
      "  1352                                                               sync_loop = (iterations) % sync_iterations == 0\n",
      "  1353                                                               last_epoch = iterations == update_iterations\n",
      "  1354                                                               if use_collective and (sync_loop or last_epoch):\n",
      "  1355                                                                   self.synchronize('learners')\n",
      "  1356                                                                   synchronized = True\n",
      "  1357                                                               else: synchronized = False\n",
      "  1358                                           \n",
      "  1359                                                           # Update KL beta\n",
      "  1360                                                           # NOTE: Same as Torch KLPENPPOLoss implementation\n",
      "  1361        80          0.4      0.0     15.4                  if self.kl_early_stop or self.kl_beta != 0:\n",
      "  1362                                                               loss_kl_mean = loss_kl.detach().mean()\n",
      "  1363                                                               self.synchronize('learners', sync_list=[loss_kl_mean])\n",
      "  1364                                                               if not self.kl_early_stop:\n",
      "  1365                                                                   exp_limit = 32\n",
      "  1366                                                                   if loss_kl_mean < self.kl_target / 1.5 and self.kl_beta > 2**-exp_limit: self.kl_beta.data *= self.kl_beta_increment[0]\n",
      "  1367                                                                   elif loss_kl_mean > self.kl_target * 1.5 and self.kl_beta < 2**exp_limit: self.kl_beta.data *= self.kl_beta_increment[1]\n",
      "  1368                                                           # Escape and roll back if KLD too high\n",
      "  1369        80          0.0      0.0      0.0                  if self.kl_early_stop:\n",
      "  1370                                                               if loss_kl_mean > 1.5 * self.kl_target:\n",
      "  1371                                                                   if iterations - sync_iterations > 0:\n",
      "  1372                                                                       # Revert to previous synchronized state within kl target\n",
      "  1373                                                                       self.revert_policy()\n",
      "  1374                                                                       # iterations -= sync_iterations\n",
      "  1375                                                                       escape = True; break\n",
      "  1376                                                                   else:\n",
      "  1377                                                                       warnings.warn(\n",
      "  1378                                                                           'Update exceeded KL target too fast! Proceeding with update, but may be unstable. '\n",
      "  1379                                                                           'Try lowering clip or learning rate parameters.')\n",
      "  1380                                                                       escape = True; break\n",
      "  1381                                                                   \n",
      "  1382                                                       # Iterate\n",
      "  1383        10          0.0      0.0      0.0              iterations += 1\n",
      "  1384        10          0.0      0.0      0.0              if iterations >= update_iterations: escape = True\n",
      "  1385                                           \n",
      "  1386                                                       # CLI\n",
      "  1387        10          0.0      0.0      0.0              if verbose and (iterations in (1, 5) or iterations % 10 == 0 or escape):\n",
      "  1388         6          0.0      0.0      0.0                  print(\n",
      "  1389        12          0.0      0.0      0.0                      f'Iteration {iterations:02} - '\n",
      "  1390         3          0.0      0.0      0.1                      + f' + '.join([f'{k} ({np.mean([v.item() for v in vl[-batches:]]):.5f})' for k, vl in total_losses.items()])\n",
      "  1391         3          0.0      0.0      0.0                      + f' :: Log STD ({self.get_log_std():.5f})'\n",
      "  1392         3          0.0      0.0      0.0                      + f' :: Advantage STD ({self.return_standardization.std.mean().item():.5f})')\n",
      "  1393                                           \n",
      "  1394                                                       # Break\n",
      "  1395        10          0.0      0.0      0.0              if escape: break\n",
      "  1396                                           \n",
      "  1397                                                   # Update scheduler\n",
      "  1398         1          0.0      0.0      0.0          self.scheduler.step()\n",
      "  1399                                                   # Update records\n",
      "  1400         1          0.0      0.0      0.0          self.policy_iteration += 1\n",
      "  1401         1          0.0      0.0      0.1          self.copy_policy()\n",
      "  1402                                                   # Return\n",
      "  1403         1          0.0      0.0      0.1          return iterations, {k: np.mean([v.item() for v in vl]) for k, vl in total_losses.items()}\n",
      "\n",
      "Total time: 0.00332553 s\n",
      "File: /home/thema/repos/inept/celltrip/utility/processing.py\n",
      "Function: split_state at line 517\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   517                                           def split_state(\n",
      "   518                                               state,\n",
      "   519                                               idx=None,\n",
      "   520                                               sample_strategy='random-proximity',\n",
      "   521                                               # Strategy kwargs\n",
      "   522                                               max_nodes=None,\n",
      "   523                                               reproducible_strategy='mean',\n",
      "   524                                               sample_dim=None,  # Should be the dim of the env\n",
      "   525                                               return_mask=False,\n",
      "   526                                           ):\n",
      "   527                                               \"Split full state matrix into individual inputs, self_idx is an optional array\"\n",
      "   528                                               # Skip if indicated \n",
      "   529                                           \n",
      "   530                                               # Parameters\n",
      "   531        80          0.0      0.0      1.8      if idx is None: idx = np.arange(state.shape[0]).tolist()\n",
      "   532        80          0.0      0.0     10.2      if not _utility.general.is_list_like(idx): idx = [idx]\n",
      "   533        80          0.0      0.0      0.7      self_idx = idx\n",
      "   534        80          0.0      0.0      0.7      del idx\n",
      "   535        80          0.0      0.0      3.3      device = state.device\n",
      "   536                                           \n",
      "   537                                               # Batch input for Lite model\n",
      "   538        80          0.0      0.0      0.9      if sample_strategy is None:\n",
      "   539                                                   # All processing case\n",
      "   540        80          0.0      0.0      6.4          if len(self_idx) == state.shape[0]:\n",
      "   541                                                       # This optimization saves a lot of time\n",
      "   542        80          0.0      0.0     76.1              if (self_idx[:-1] < self_idx[1:]).all(): return state,\n",
      "   543                                                       elif (np.unique(self_idx) == np.arange(state.shape[0])).all(): return state[self_idx],\n",
      "   544                                           \n",
      "   545                                                   # Subset case\n",
      "   546                                                   self_entity = state[self_idx]\n",
      "   547                                                   node_entities = state  # Could remove the self_idx in the case len == 1, but doesn't really matter\n",
      "   548                                                   mask = torch.eye(state.shape[0], dtype=torch.bool, device=device)[self_idx]\n",
      "   549                                                   return self_entity, node_entities, mask\n",
      "   550                                           \n",
      "   551                                               # Get self features for each node\n",
      "   552                                               self_entity = state[self_idx]\n",
      "   553                                           \n",
      "   554                                               # Get node features for each state\n",
      "   555                                               node_mask = torch.eye(state.shape[0], dtype=torch.bool, device=device)\n",
      "   556                                               node_mask = ~node_mask\n",
      "   557                                           \n",
      "   558                                               # Enforce reproducibility\n",
      "   559                                               if reproducible_strategy is not None: generator = torch.Generator(device=device)\n",
      "   560                                               else: generator = None\n",
      "   561                                           \n",
      "   562                                               # Hashing method\n",
      "   563                                               if reproducible_strategy is None:\n",
      "   564                                                   pass\n",
      "   565                                               # Hashing method\n",
      "   566                                               # NOTE: Tensors are hashed by object, so would be unreliable to directly hash tensor\n",
      "   567                                               elif reproducible_strategy == 'hash':\n",
      "   568                                                   generator.manual_seed(hash(str(state.detach().numpy())))\n",
      "   569                                               # First number\n",
      "   570                                               elif reproducible_strategy == 'first':\n",
      "   571                                                   generator.manual_seed((2**16*state.flatten()[0]).to(torch.long).item())\n",
      "   572                                               # Mean value\n",
      "   573                                               elif reproducible_strategy == 'mean':\n",
      "   574                                                   generator.manual_seed((2**16*state.mean()).to(torch.long).item())\n",
      "   575                                               # Set seed (not recommended)\n",
      "   576                                               elif type(reproducible_strategy) != str:\n",
      "   577                                                   generator.manual_seed(reproducible_strategy)\n",
      "   578                                               else:\n",
      "   579                                                   raise ValueError(f'Reproducible strategy \\'{reproducible_strategy}\\' not found.')\n",
      "   580                                           \n",
      "   581                                               # Enforce max nodes\n",
      "   582                                               num_nodes = state.shape[0] - 1\n",
      "   583                                               use_mask = max_nodes is not None and max_nodes < num_nodes\n",
      "   584                                               if use_mask:\n",
      "   585                                                   # Set new num_nodes\n",
      "   586                                                   num_nodes = max_nodes - 1\n",
      "   587                                           \n",
      "   588                                                   # Random sample `num_nodes` to `max_nodes`\n",
      "   589                                                   if sample_strategy == 'random':\n",
      "   590                                                       # Filter nodes to `max_nodes` per idx\n",
      "   591                                                       probs = torch.empty_like(node_mask, dtype=torch.get_default_dtype()).normal_(generator=generator)\n",
      "   592                                                       probs[~node_mask] = 0\n",
      "   593                                                       selected_idx = probs.topk(num_nodes, dim=-1)[1]  # Take `num_nodes` highest values\n",
      "   594                                           \n",
      "   595                                                       # Create new mask\n",
      "   596                                                       node_mask = torch.zeros((state.shape[0], state.shape[0]), dtype=torch.bool, device=device)\n",
      "   597                                                       node_mask[torch.arange(node_mask.shape[0]).unsqueeze(-1).expand(node_mask.shape[0], num_nodes), selected_idx] = True\n",
      "   598                                           \n",
      "   599                                                   # Sample closest nodes\n",
      "   600                                                   elif sample_strategy == 'proximity':\n",
      "   601                                                       # Check for dim pass\n",
      "   602                                                       assert sample_dim is not None, (\n",
      "   603                                                           f'`sample_dim` argument must be passed if `sample_strategy` is \\'{sample_strategy}\\'')\n",
      "   604                                           \n",
      "   605                                                       # Get inter-node distances\n",
      "   606                                                       dist = _utility.distance.euclidean_distance(state[..., :sample_dim])\n",
      "   607                                                       dist[~node_mask] = -1  # Set self-dist lowest for case of ties\n",
      "   608                                           \n",
      "   609                                                       # Select `max_nodes` closest\n",
      "   610                                                       selected_idx = dist.topk(num_nodes+1, largest=False, dim=-1)[1][..., 1:]\n",
      "   611                                                       \n",
      "   612                                                       # Create new mask\n",
      "   613                                                       node_mask = torch.zeros((state.shape[0], state.shape[0]), dtype=torch.bool, device=device)\n",
      "   614                                                       node_mask[torch.arange(node_mask.shape[0]).unsqueeze(-1).expand(node_mask.shape[0], num_nodes), selected_idx] = True\n",
      "   615                                           \n",
      "   616                                                   # Randomly sample from a distribution of node distance\n",
      "   617                                                   elif sample_strategy == 'random-proximity':\n",
      "   618                                                       # Check for dim pass\n",
      "   619                                                       assert sample_dim is not None, (\n",
      "   620                                                           f'`sample_dim` argument must be passed if `sample_strategy` is \\'{sample_strategy}\\'')\n",
      "   621                                           \n",
      "   622                                                       # Get inter-node distances\n",
      "   623                                                       dist = _utility.distance.euclidean_distance(state[..., :sample_dim])\n",
      "   624                                                       prob = 1 / (dist+1)\n",
      "   625                                                       prob[~node_mask] = 0  # Remove self\n",
      "   626                                           \n",
      "   627                                                       # Randomly sample\n",
      "   628                                                       # NOTE: If you get an error `_assert_async_cuda_kernel`, weights probably exploded making `actions = [nan]`\n",
      "   629                                                       node_mask = torch.zeros((state.shape[0], state.shape[0]), dtype=torch.bool, device=device)\n",
      "   630                                                       idx = prob.multinomial(num_nodes, replacement=False, generator=generator)\n",
      "   631                                           \n",
      "   632                                                       # Apply sampling\n",
      "   633                                                       mat = torch.arange(node_mask.shape[0], device=device).unsqueeze(-1).expand(node_mask.shape[0], num_nodes)\n",
      "   634                                                       node_mask[mat, idx] = True\n",
      "   635                                                   else:\n",
      "   636                                                       # TODO: Verify works\n",
      "   637                                                       raise ValueError(f'Sample strategy \\'{sample_strategy}\\' not found.')\n",
      "   638                                                   \n",
      "   639                                               # Shrink mask to appropriate size\n",
      "   640                                               # NOTE: Randomization needs to be done on all samples for reproducibility\n",
      "   641                                               # print(node_mask)\n",
      "   642                                               node_mask = node_mask[self_idx]\n",
      "   643                                               \n",
      "   644                                               # Final formation\n",
      "   645                                               node_entities = state.unsqueeze(0).expand(len(self_idx), *state.shape)\n",
      "   646                                               node_entities = node_entities[node_mask].reshape(len(self_idx), num_nodes, state.shape[1])\n",
      "   647                                           \n",
      "   648                                               # Return\n",
      "   649                                               ret = (self_entity, node_entities)\n",
      "   650                                               if return_mask: ret += (node_mask,)\n",
      "   651                                               return ret\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update\n",
    "import line_profiler\n",
    "import numpy as np\n",
    "prof = line_profiler.LineProfiler(policy.update, memory.fast_sample, memory._concat_states, celltrip.utility.processing.split_state)\n",
    "ret = prof.runcall(policy.update, memory, verbose=True)\n",
    "print(', '.join([f'{k}: {v:.3f}' for k, v in ret[1].items()]))\n",
    "prof.print_stats(output_unit=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     # Forward\n",
    "#     import line_profiler\n",
    "#     memory.mark_sampled()\n",
    "#     prof = line_profiler.LineProfiler(\n",
    "#         celltrip.train.simulate_until_completion,\n",
    "#         celltrip.policy.PPO.forward, celltrip.policy.EntitySelfAttentionLite.forward, celltrip.policy.ResidualAttention.forward,\n",
    "#         celltrip.environment.EnvironmentBase.step)\n",
    "#     while memory.get_new_steps() < 5_000:\n",
    "#         env.reset()\n",
    "#         ret = prof.runcall(celltrip.train.simulate_until_completion, env, policy, memory)\n",
    "#         print(', '.join([f'{k}: {v:.3f}' for k, v in ret[2].items()]))\n",
    "#     memory.compute_advantages()\n",
    "#     # prof.print_stats(output_unit=1)\n",
    "\n",
    "#     # Update\n",
    "#     import line_profiler\n",
    "#     prof = line_profiler.LineProfiler(policy.update, memory.fast_sample, celltrip.utility.processing.split_state)\n",
    "#     ret = prof.runcall(policy.update, memory, verbose=True)\n",
    "#     print(', '.join([f'{k}: {v:.3f}' for k, v in ret[1].items()]))\n",
    "#     # prof.print_stats(output_unit=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
