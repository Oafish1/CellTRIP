{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bc6db96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T04:33:19.316608Z",
     "iopub.status.busy": "2025-09-10T04:33:19.316435Z",
     "iopub.status.idle": "2025-09-10T04:33:19.327146Z",
     "shell.execute_reply": "2025-09-10T04:33:19.326692Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2754518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T04:33:19.328703Z",
     "iopub.status.busy": "2025-09-10T04:33:19.328538Z",
     "iopub.status.idle": "2025-09-10T04:33:22.271532Z",
     "shell.execute_reply": "2025-09-10T04:33:22.271011Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.neighbors\n",
    "import sklearn.neural_network\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "import celltrip\n",
    "\n",
    "os.environ['AWS_PROFILE'] = 'waisman-admin'\n",
    "mpl.rcParams['pdf.fonttype'] = mpl.rcParams['ps.fonttype'] = 42\n",
    "sns.set_theme(context='paper', style='darkgrid', palette='colorblind')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1aac9c",
   "metadata": {},
   "source": [
    "# Load Data and Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6367c9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T04:33:22.273509Z",
     "iopub.status.busy": "2025-09-10T04:33:22.273321Z",
     "iopub.status.idle": "2025-09-10T04:34:39.350460Z",
     "shell.execute_reply": "2025-09-10T04:34:39.349899Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read data files\n",
    "adatas = [\n",
    "    celltrip.utility.processing.merge_adatas(\n",
    "        *celltrip.utility.processing.read_adatas(*[\n",
    "            f's3://nkalafut-celltrip/Flysta3D/{p}_{m}.h5ad'\n",
    "            for p in ('E14-16h_a', 'E16-18h_a', 'L1_a', 'L2_a', 'L3_b')\n",
    "            # for p in ('L2_a',)\n",
    "        ], backed=True), backed=True)\n",
    "    for m in ('expression', 'spatial')]\n",
    "# Model location and name (should be prefix for .weights, .pre, and .mask file)\n",
    "# prefix, training_step = 's3://nkalafut-celltrip/checkpoints/flysta-250909-5', 800  # Double-standard\n",
    "prefix, training_step = 's3://nkalafut-celltrip/checkpoints/flysta-250909-4', 800  # Regular\n",
    "# Generate or load preprocessing\n",
    "preprocessing = celltrip.utility.processing.Preprocessing().load(f'{prefix}.pre')\n",
    "with celltrip.utility.general.open_s3_or_local(f'{prefix}.mask', 'rb') as f:\n",
    "    mask = np.loadtxt(f).astype(bool)\n",
    "adatas[0].obs['Training'] = mask  # For meta export, note that obs is stored in memory\n",
    "# Create sample env (kind of a dumb workaround, TODO)\n",
    "m1, m2 = [preprocessing.transform(ad[:2].X, subset_modality=i)[0] for i, ad in enumerate(adatas)]\n",
    "env = celltrip.environment.EnvironmentBase(\n",
    "    torch.tensor(m1), torch.tensor(m2), target_modalities=[1], compute_rewards=False, dim=8).eval().to('cuda')\n",
    "# Load policy\n",
    "policy = celltrip.policy.create_agent_from_env(\n",
    "    env, forward_batch_size=1_000, vision_size=1_000, pinning_spatial=[1]).eval().to('cuda')\n",
    "policy.load_checkpoint(f'{prefix}-{training_step:04}.weights');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f1aed",
   "metadata": {},
   "source": [
    "# Generate Steady States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105cac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T04:34:39.353077Z",
     "iopub.status.busy": "2025-09-10T04:34:39.352674Z",
     "iopub.status.idle": "2025-09-10T04:42:41.073684Z",
     "shell.execute_reply": "2025-09-10T04:42:41.073283Z"
    }
   },
   "outputs": [],
   "source": [
    "# for dev in (pbar := tqdm.tqdm(adatas[0].obs['development'].unique(), desc='')):\n",
    "#     # Subset and preprocess the data\n",
    "#     pbar.set_description(f'{dev} (Preprocessing)')\n",
    "#     samples = adatas[0].obs.index[adatas[0].obs['development']==dev]\n",
    "#     # if len(samples) > 10_000: samples = np.random.choice(samples, 10_000, replace=False)  # For runtime, TESTING\n",
    "#     m1, m2 = [\n",
    "#         celltrip.utility.processing.chunk_X(\n",
    "#             ad[samples], chunk_size=2_000,\n",
    "#             func=lambda x: preprocessing.transform(x, subset_modality=i)[0])\n",
    "#             for i, ad in enumerate(adatas)]\n",
    "#     # Initialize environment\n",
    "#     pbar.set_description(f'{dev} (Initializing)')\n",
    "#     env = celltrip.environment.EnvironmentBase(\n",
    "#         torch.tensor(m1), torch.tensor(m2), target_modalities=[1], compute_rewards=False, dim=8).eval(time_scale=1).to('cuda')  # 32/env.max_time\n",
    "#     # Simulate to steady state\n",
    "#     pbar.set_description(f'{dev} (Running)')\n",
    "#     # env.train().eval(time_scale=1)\n",
    "#     env.reset()\n",
    "#     ret = celltrip.train.simulate_until_completion(env, policy, skip_states=100, store_states='cpu')  # progress_bar=True\n",
    "#     steady_state = ret[-1][-1, :, :env.dim]\n",
    "#     target_state = env.modalities[env.target_modalities[0]].cpu()\n",
    "#     with torch.no_grad():\n",
    "#         imputed_steady_state = policy.pinning[0](steady_state.to('cuda'), Y=target_state.to('cuda')).detach().cpu().numpy()\n",
    "#     imputed_steady_state, = preprocessing.inverse_transform(imputed_steady_state, subset_modality=1)\n",
    "#     # Save\n",
    "#     pbar.set_description(f'{dev} (Saving)')\n",
    "#     np.save(f'../plots/flysta/CellTRIP_{dev}.npy', imputed_steady_state)\n",
    "#     np.save(f'../plots/flysta/spatial_{dev}.npy', adatas[1][samples].X)\n",
    "#     adatas[0].obs.loc[samples].to_csv(f'../plots/flysta/meta_{dev}.csv', index=False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd516593",
   "metadata": {},
   "source": [
    "## Run Comparison Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ca499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T04:42:41.206079Z",
     "iopub.status.busy": "2025-09-10T04:42:41.205882Z",
     "iopub.status.idle": "2025-09-10T04:45:08.042713Z",
     "shell.execute_reply": "2025-09-10T04:45:08.042191Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Load full data\n",
    "# X, Y = [\n",
    "#     celltrip.utility.processing.chunk_X(\n",
    "#         ad, chunk_size=2_000,\n",
    "#         func=lambda x: preprocessing.transform(x, subset_modality=i)[0])\n",
    "#         for i, ad in enumerate(adatas)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a2d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T04:45:08.044730Z",
     "iopub.status.busy": "2025-09-10T04:45:08.044414Z",
     "iopub.status.idle": "2025-09-10T04:54:12.919074Z",
     "shell.execute_reply": "2025-09-10T04:54:12.910893Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Train MLP and export predictions\n",
    "# model = sklearn.neural_network.MLPRegressor(max_iter=100, verbose=True).fit(X[mask], Y[mask])\n",
    "# for dev in (pbar := tqdm.tqdm(adatas[0].obs['development'].unique())):\n",
    "#     # Subset and preprocess the data\n",
    "#     pbar.set_description(f'{dev} (Preprocessing)')\n",
    "#     samples = adatas[0].obs.index[adatas[0].obs['development']==dev]\n",
    "#     X_dev, Y_dev = [\n",
    "#         celltrip.utility.processing.chunk_X(\n",
    "#             ad[samples], chunk_size=2_000,\n",
    "#             func=lambda x: preprocessing.transform(x, subset_modality=i)[0])\n",
    "#             for i, ad in enumerate(adatas)]\n",
    "#     # Run model\n",
    "#     pbar.set_description(f'{dev} (Running)')\n",
    "#     Y_pred = model.predict(X_dev)\n",
    "#     imputed_steady_state, = preprocessing.inverse_transform(Y_pred, subset_modality=1)\n",
    "#     # Save\n",
    "#     pbar.set_description(f'{dev} (Saving)')\n",
    "#     np.save(f'../plots/flysta/MLP_{dev}.npy', imputed_steady_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e789165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T04:54:12.921087Z",
     "iopub.status.busy": "2025-09-10T04:54:12.920823Z",
     "iopub.status.idle": "2025-09-10T04:59:53.610048Z",
     "shell.execute_reply": "2025-09-10T04:59:53.609562Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Export KNN predictions\n",
    "# model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=10).fit(X[mask], Y[mask])\n",
    "# for dev in (pbar := tqdm.tqdm(adatas[0].obs['development'].unique())):\n",
    "#     # Subset and preprocess the data\n",
    "#     pbar.set_description(f'{dev} (Preprocessing)')\n",
    "#     samples = adatas[0].obs.index[adatas[0].obs['development']==dev]\n",
    "#     X_dev, Y_dev = [\n",
    "#         celltrip.utility.processing.chunk_X(\n",
    "#             ad[samples], chunk_size=2_000,\n",
    "#             func=lambda x: preprocessing.transform(x, subset_modality=i)[0])\n",
    "#             for i, ad in enumerate(adatas)]\n",
    "#     # Run model\n",
    "#     pbar.set_description(f'{dev} (Running)')\n",
    "#     Y_pred = model.predict(X_dev)\n",
    "#     imputed_steady_state, = preprocessing.inverse_transform(Y_pred, subset_modality=1)\n",
    "#     # Save\n",
    "#     pbar.set_description(f'{dev} (Saving)')\n",
    "#     np.save(f'../plots/flysta/KNN_{dev}.npy', imputed_steady_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7b72d",
   "metadata": {},
   "source": [
    "# Recover Validation State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06104732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate training and validation stages\n",
    "# development = np.array(['E14-16h_a', 'E16-18h_a', 'L1_a', 'L2_a', 'L3_b'])  # Ordered\n",
    "# development_training = adatas[0].obs.loc[mask, 'development'].unique()\n",
    "# development_validation = adatas[0].obs.loc[~mask, 'development'].unique()\n",
    "# assert len(np.intersect1d(development_training, development_validation)) == 0  # Properly partitioned\n",
    "# # Get possible interpolation stages\n",
    "# possible_interpolated_stages = []\n",
    "# for i in np.argwhere(np.isin(development, development_validation)).flatten():\n",
    "#     if i == 0 or i == len(development)-1: continue\n",
    "#     possible_interpolated_stages.append(development[i-1:i+2])\n",
    "# # Set interpolation\n",
    "# start_stage, interp_stage, end_stage = possible_interpolated_stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb0683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1281it [00:17, 71.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Grab data\n",
    "# start_idx = np.argwhere(adatas[0].obs['development'] == start_stage).flatten()\n",
    "# end_idx = np.argwhere(adatas[0].obs['development'] == end_stage).flatten()\n",
    "# start_exp = celltrip.utility.processing.chunk_X(\n",
    "#     adatas[0][start_idx], chunk_size=2_000,\n",
    "#     func=lambda x: preprocessing.transform(x, subset_modality=0)[0])\n",
    "# end_exp = celltrip.utility.processing.chunk_X(\n",
    "#     adatas[0][end_idx], chunk_size=2_000,\n",
    "#     func=lambda x: preprocessing.transform(x, subset_modality=0)[0])\n",
    "# start_obs = celltrip.utility.general.transform_and_center(celltrip.utility.processing.chunk_X(adatas[1][start_idx], chunk_size=2_000))\n",
    "# end_obs = celltrip.utility.general.transform_and_center(celltrip.utility.processing.chunk_X(adatas[1][end_idx], chunk_size=2_000))\n",
    "\n",
    "# # Use K-Means to create start and end pseudocells\n",
    "# start_n_pcells = end_n_pcells = 5_000\n",
    "# start_pcell_ids = sklearn.cluster.KMeans(n_clusters=start_n_pcells, random_state=42).fit_predict(start_obs)\n",
    "# end_pcell_ids = sklearn.cluster.KMeans(n_clusters=end_n_pcells, random_state=42).fit_predict(start_obs)\n",
    "\n",
    "# # Get expression and spatial for pseudocells\n",
    "# start_processed_exp = np.stack([start_exp[np.argwhere(start_pcell_ids==i).flatten()].mean(axis=0) for i in range(start_n_pcells)], axis=0)\n",
    "# start_processed_obs = np.stack([start_obs[np.argwhere(start_pcell_ids==i).flatten()].mean(axis=0) for i in range(start_n_pcells)], axis=0)\n",
    "# end_processed_exp = np.stack([end_exp[np.argwhere(end_pcell_ids==i).flatten()].mean(axis=0) for i in range(end_n_pcells)], axis=0)\n",
    "# end_processed_obs = np.stack([end_obs[np.argwhere(end_pcell_ids==i).flatten()].mean(axis=0) for i in range(end_n_pcells)], axis=0)\n",
    "\n",
    "# # Calculate OT matrix\n",
    "# import ot\n",
    "# a, b = ot.utils.unif(start_processed_obs.shape[0]), ot.utils.unif(end_processed_obs.shape[0])\n",
    "# M = ot.dist(start_processed_obs, end_processed_obs)\n",
    "# M /= M.max()\n",
    "# OT_mat = ot.emd(a, b, M, numItermax=1_000_000)\n",
    "# # OT_mat = ot.solve(M, a, b)\n",
    "# # OT_mat = ot.sinkhorn(a, b, M, 1e-1)\n",
    "\n",
    "# # Calculate pseudocells\n",
    "# pcells = [([i], np.argwhere(OT_mat[i] > 0).flatten()) for i in range(OT_mat.shape[0]) if OT_mat[i].sum() > 0]\n",
    "# start_pcells_exp, end_pcells_exp = [], []\n",
    "# for pcell_start, pcell_end in pcells:\n",
    "#     start_pcells_exp.append(start_processed_exp[pcell_start].mean(axis=0))\n",
    "#     end_pcells_exp.append(end_processed_exp[pcell_end].mean(axis=0))\n",
    "# start_pcells_exp = np.stack(start_pcells_exp, axis=0)\n",
    "# end_pcells_exp = np.stack(end_pcells_exp, axis=0)\n",
    "\n",
    "# # Create env\n",
    "# m1_start, m1_end = start_pcells_exp, end_pcells_exp  # preprocessing.transform\n",
    "# env = celltrip.environment.EnvironmentBase(\n",
    "#     torch.tensor(m1_start), target_modalities=None, compute_rewards=False, dim=8).eval(time_scale=1).to('cuda')\n",
    "\n",
    "# # Get transition states\n",
    "# env.reset()\n",
    "# celltrip.train.simulate_until_completion(env, policy, store_states=False)  # Set env at steady state\n",
    "# env.time = 0  # Reset timing\n",
    "# env.set_modalities([torch.tensor(m1_end)]).to('cuda')  # Set to ending expression\n",
    "# transition_states = celltrip.train.simulate_until_completion(env, policy, skip_states=50, store_states='cpu', progress_bar=True)[-1][..., :env.dim].cpu()\n",
    "\n",
    "# # Impute transition states\n",
    "# with torch.no_grad():\n",
    "#     imputed_transition_states = policy.pinning[0](transition_states.to('cuda')).detach().cpu().numpy()\n",
    "# imputed_transition_states, = preprocessing.inverse_transform(imputed_transition_states, subset_modality=1)\n",
    "# np.save(f'../plots/flysta/Interpolated_{interp_stage}.npy', imputed_transition_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37705ce",
   "metadata": {},
   "source": [
    "# Perform Knockdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca63f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 53/13668 [02:27<10:31:28,  2.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m env\u001b[38;5;241m.\u001b[39mset_velocities(steady_vel)  \u001b[38;5;66;03m# Or maybe 0?\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get knockdowns\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m iso_modality, \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_m1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_modality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m iso_modality \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(iso_modality)\u001b[38;5;241m.\u001b[39mto(env\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     26\u001b[0m env\u001b[38;5;241m.\u001b[39mmodalities[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mmodalities[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m (iso_modality \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0\u001b[39m\u001b[38;5;241m*\u001b[39miso_modality)\n",
      "File \u001b[0;32m~/repos/inept/celltrip/utility/processing.py:278\u001b[0m, in \u001b[0;36mPreprocessing.transform\u001b[0;34m(self, modalities, adata_vars, force_filter, subset_features, subset_modality, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# PCA\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpca_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m     modalities \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mtransform(m) \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m m \u001b[38;5;28;01mfor\u001b[39;00m m, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(modalities, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpca_class[sm])]\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Scaling\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# m1 * m1.shape[1] / np.sqrt(preprocessing.pca_class[0].explained_variance_).sum()\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# NOTE: Returns features before PCA transformation, also always dense\u001b[39;00m\n\u001b[1;32m    284\u001b[0m modalities \u001b[38;5;241m=\u001b[39m [m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39missparse(m) \u001b[38;5;28;01melse\u001b[39;00m m\u001b[38;5;241m.\u001b[39mtoarray() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m modalities]\n",
      "File \u001b[0;32m~/repos/inept/celltrip/utility/processing.py:278\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# PCA\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpca_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m     modalities \u001b[38;5;241m=\u001b[39m [\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m m \u001b[38;5;28;01mfor\u001b[39;00m m, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(modalities, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpca_class[sm])]\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Scaling\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# m1 * m1.shape[1] / np.sqrt(preprocessing.pca_class[0].explained_variance_).sum()\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# NOTE: Returns features before PCA transformation, also always dense\u001b[39;00m\n\u001b[1;32m    284\u001b[0m modalities \u001b[38;5;241m=\u001b[39m [m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39missparse(m) \u001b[38;5;28;01melse\u001b[39;00m m\u001b[38;5;241m.\u001b[39mtoarray() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m modalities]\n",
      "File \u001b[0;32m~/miniconda3/envs/ct/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/ct/lib/python3.10/site-packages/sklearn/decomposition/_base.py:146\u001b[0m, in \u001b[0;36m_BasePCA.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    141\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    143\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    144\u001b[0m     X, dtype\u001b[38;5;241m=\u001b[39m[xp\u001b[38;5;241m.\u001b[39mfloat64, xp\u001b[38;5;241m.\u001b[39mfloat32], accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m), reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    145\u001b[0m )\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_is_centered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ct/lib/python3.10/site-packages/sklearn/decomposition/_base.py:156\u001b[0m, in \u001b[0;36m_BasePCA._transform\u001b[0;34m(self, X, xp, x_is_centered)\u001b[0m\n\u001b[1;32m    149\u001b[0m X_transformed \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x_is_centered:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# Apply the centering after the projection.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# For dense X this avoids copying or mutating the data passed by\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# the caller.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# For sparse X it keeps sparsity and avoids having to wrap X into\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# a linear operator.\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     X_transformed \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# For some solvers (such as \"arpack\" and \"covariance_eigh\"), on\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# rank deficient data, some components can have a variance\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# arbitrarily close to zero, leading to non-finite results when\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# whitening. To avoid this problem we clip the variance below.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     scale \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplained_variance_)\n",
      "File \u001b[0;32m~/miniconda3/envs/ct/lib/python3.10/site-packages/sklearn/utils/_array_api.py:412\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.reshape\u001b[0;34m(self, x, shape, copy)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconcat\u001b[39m(\u001b[38;5;28mself\u001b[39m, arrays, \u001b[38;5;241m*\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mconcatenate(arrays, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreshape\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, shape, \u001b[38;5;241m*\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    413\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m    The Array API specification requires shape to be a tuple.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m    https://data-apis.org/array-api/latest/API_specification/generated/array_api.reshape.html\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shape, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for dev in adatas[0].obs['development'].unique():\n",
    "    # Subset and preprocess the data\n",
    "    samples = adatas[0].obs.index[adatas[0].obs['development']==dev]\n",
    "    raw_m1 = celltrip.utility.processing.chunk_X(adatas[0][samples], chunk_size=2_000)\n",
    "    m1, = preprocessing.transform(raw_m1, subset_modality=0)\n",
    "    m2 = celltrip.utility.processing.chunk_X(\n",
    "        adatas[1][samples], chunk_size=2_000,\n",
    "        func=lambda x: preprocessing.transform(x, subset_modality=1)[0])\n",
    "    # Initialize environment\n",
    "    env = celltrip.environment.EnvironmentBase(\n",
    "        torch.tensor(m1), torch.tensor(m2), target_modalities=[1], compute_rewards=False, dim=8).eval(time_scale=1).to('cuda')\n",
    "    # Simulate to steady state\n",
    "    env.reset()\n",
    "    celltrip.train.simulate_until_completion(env, policy)\n",
    "    steady_pos, steady_vel = (env.pos, env.vel)\n",
    "    # Perturb\n",
    "    for i, gene in enumerate(tqdm.tqdm(adatas[0].var.index)):\n",
    "        # Reset environment\n",
    "        env.set_max_time(5*env.delta).reset()  # TODO: Run longer\n",
    "        env.set_positions(steady_pos)\n",
    "        env.set_velocities(steady_vel)  # Or maybe 0?\n",
    "        # Get knockdowns\n",
    "        iso_modality, = preprocessing.transform(raw_m1, subset_features=[i], subset_modality=0)\n",
    "        iso_modality = torch.tensor(iso_modality).to(env.device)\n",
    "        env.modalities[0] = env.modalities[0] - (iso_modality - 0*iso_modality)\n",
    "        # Simulate\n",
    "        states = celltrip.train.simulate_until_completion(env, policy, store_states='cpu')[-1]\n",
    "        # Impute\n",
    "        with torch.no_grad():\n",
    "            imputed_states = policy.pinning[0](states[..., :env.dim].to('cuda')).detach().cpu().numpy()\n",
    "        imputed_states, = preprocessing.inverse_transform(imputed_states, subset_modality=1)\n",
    "        # Record\n",
    "        def add_record(states, ct):\n",
    "            results.append({\n",
    "                'Gene': gene,\n",
    "                'Development': dev,\n",
    "                'Cell Type': ct,\n",
    "                'Effect Size': np.square(states[-1] - states[0]).mean(axis=-1).mean(),\n",
    "                'Trajectory Length': np.square(states[1:] - states[:-1]).mean(axis=(0, -1)).mean()})\n",
    "        add_record(imputed_states, 'All')\n",
    "        for ct in adatas[0][samples].obs['annotation'].unique():\n",
    "            add_record(imputed_states[:, adatas[0][samples].obs['annotation']==ct], ct)\n",
    "# Convert and save\n",
    "pd.DataFrame(results).to_csv('../plots/flysta/knockdown.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
