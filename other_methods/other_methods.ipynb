{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:10:30.566908Z",
     "iopub.status.busy": "2025-01-16T23:10:30.566454Z",
     "iopub.status.idle": "2025-01-16T23:10:32.361001Z",
     "shell.execute_reply": "2025-01-16T23:10:32.360490Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import scipy.sparse\n",
    "import torch\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../examples'))\n",
    "import data\n",
    "import inept\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:10:32.363302Z",
     "iopub.status.busy": "2025-01-16T23:10:32.363033Z",
     "iopub.status.idle": "2025-01-16T23:10:32.365748Z",
     "shell.execute_reply": "2025-01-16T23:10:32.365382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataset_name = 'MMD-MA'\n",
    "imputation_target = None\n",
    "dimensions = 3\n",
    "seed = 42\n",
    "\n",
    "# Derivatives\n",
    "RUN_FOLDER = os.path.join(BASE_FOLDER, 'runs', dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:10:32.367432Z",
     "iopub.status.busy": "2025-01-16T23:10:32.367278Z",
     "iopub.status.idle": "2025-01-16T23:10:33.241310Z",
     "shell.execute_reply": "2025-01-16T23:10:33.240850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and save to file\n",
    "modalities, types, features = data.load_data(dataset_name, DATA_FOLDER)\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER): os.makedirs(RUN_FOLDER)\n",
    "for i in range(len(modalities)):\n",
    "    modality = modalities[i]\n",
    "\n",
    "    # Regular matrices (ManiNetCluster, JAMIE)\n",
    "    np.savetxt(os.path.join(RUN_FOLDER, f'X{i+1}.txt'), modality, delimiter='\\t')\n",
    "\n",
    "    # Similarity matrices (MMD-MA)\n",
    "    modality_z = modality - modality.mean(axis=0, keepdims=True) / modality.std(axis=0, keepdims=True)\n",
    "    similarity = np.matmul(modality_z, modality_z.T)\n",
    "    np.savetxt(os.path.join(RUN_FOLDER, f'X{i+1}_sim.tsv'), similarity, delimiter='\\t')\n",
    "\n",
    "    # Anndata matrices (scVI)\n",
    "    # adata = sc.AnnData(modalities[0])\n",
    "    # adata.var_names = features[0] if isinstance(features[0][0], str) else [f'Feature_{fi}' for fi in features[0]]\n",
    "    # adata.obs_names = [f'Cell_{j}' for j in range(len(adata.obs_names))]\n",
    "    # adata.obs['cell_type'] = types[0][:, 0]\n",
    "    # adata.obs['time'] = types[0][:, -1]\n",
    "    # # adata.obs['batch'] = 0\n",
    "    # adata.write(os.path.join(RUN_FOLDER, f'X{i+1}.h5ad'), compression='gzip')\n",
    "\n",
    "    # HDFS\n",
    "    # https://github.com/scverse/anndata/issues/595#issuecomment-1824376236\n",
    "    concatenated_modalities = np.concatenate(modalities, axis=-1)\n",
    "    barcodes = [f'Cell {i}' for i in range(concatenated_modalities.shape[0])]\n",
    "    feature_types = modalities[0].shape[1] * ['Gene Expression'] + modalities[1].shape[1] * ['Peaks']\n",
    "    feature_names = np.concatenate(features)\n",
    "    feature_ids = np.array(np.arange(feature_names.shape[0]), dtype='str')\n",
    "    genome = concatenated_modalities.shape[1] * ['Something']\n",
    "    sparse_data = scipy.sparse.csr_matrix(concatenated_modalities)\n",
    "\n",
    "    def int_max(x):\n",
    "        return int(max(np.floor(len(str(int(max(x)))) / 4), 1) * 4)\n",
    "    def str_max(x):\n",
    "        return max([len(i) for i in x])\n",
    "\n",
    "    with h5py.File(os.path.join(RUN_FOLDER, f'X.h5'), 'w') as f:\n",
    "        grp = f.create_group('matrix')\n",
    "        grp.create_dataset('barcodes', data=np.array(barcodes, dtype=f'|S{str_max(barcodes)}'))\n",
    "        grp.create_dataset('data', data=np.array(sparse_data.data, dtype=f'<i{int_max(sparse_data.data)}'))\n",
    "        ftrs = grp.create_group('features')\n",
    "        # # this group will lack the following keys:\n",
    "        # # '_all_tag_keys', 'feature_type', 'genome', 'id', 'name', 'pattern', 'read', 'sequence'\n",
    "        ftrs.create_dataset('feature_type', data=np.array(feature_types, dtype=f'|S{str_max(feature_types)}'))\n",
    "        ftrs.create_dataset('genome', data=np.array(genome, dtype=f'|S{str_max(genome)}'))\n",
    "        ftrs.create_dataset('id', data=np.array(feature_ids, dtype=f'|S{str_max(feature_ids)}'))\n",
    "        ftrs.create_dataset('name', data=np.array(feature_names, dtype=f'|S{str_max([str(fn) for fn in feature_names])}'))\n",
    "        grp.create_dataset('indices', data=np.array(sparse_data.indices, dtype=f'<i{int_max(sparse_data.indices)}'))\n",
    "        grp.create_dataset('indptr', data=np.array(sparse_data.indptr, dtype=f'<i{int_max(sparse_data.indptr)}'))\n",
    "        grp.create_dataset('shape', data=np.array(sparse_data.shape[::-1], dtype=f'<i{int_max(sparse_data.shape)}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated File\n",
      "└── matrix\n",
      "    ├── barcodes (300)\n",
      "    ├── data (900000)\n",
      "    ├── features\n",
      "    │   ├── feature_type (3000)\n",
      "    │   ├── genome (3000)\n",
      "    │   ├── id (3000)\n",
      "    │   └── name (3000)\n",
      "    ├── indices (900000)\n",
      "    ├── indptr (301)\n",
      "    └── shape (2)\n",
      "\n",
      "Baseline File\n",
      "└── matrix\n",
      "    ├── barcodes (5517)\n",
      "    ├── data (73404624)\n",
      "    ├── features\n",
      "    │   ├── _all_tag_keys (2)\n",
      "    │   ├── feature_type (147085)\n",
      "    │   ├── genome (147085)\n",
      "    │   ├── id (147085)\n",
      "    │   ├── interval (147085)\n",
      "    │   └── name (147085)\n",
      "    ├── indices (73404624)\n",
      "    ├── indptr (5518)\n",
      "    └── shape (2)\n"
     ]
    }
   ],
   "source": [
    "print('Generated File')\n",
    "with h5py.File(os.path.join(RUN_FOLDER, f'X.h5'), 'r') as f: inept.utilities.h5_tree(f)\n",
    "print('\\nBaseline File')\n",
    "with h5py.File('/home/thema/Downloads/DM_rep4.h5', 'r') as f: inept.utilities.h5_tree(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:10:33.243535Z",
     "iopub.status.busy": "2025-01-16T23:10:33.243375Z",
     "iopub.status.idle": "2025-01-16T23:11:21.942767Z",
     "shell.execute_reply": "2025-01-16T23:11:21.939936Z"
    }
   },
   "outputs": [],
   "source": [
    "# LMA\n",
    "# https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "new_wd = os.path.join(RUN_FOLDER, 'LMA')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n maninetcluster \\\n",
    " python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " --align lma \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:11:21.946314Z",
     "iopub.status.busy": "2025-01-16T23:11:21.946100Z",
     "iopub.status.idle": "2025-01-16T23:12:12.906694Z",
     "shell.execute_reply": "2025-01-16T23:12:12.903989Z"
    }
   },
   "outputs": [],
   "source": [
    "# CCA\n",
    "# https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "new_wd = os.path.join(RUN_FOLDER, 'CCA')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n maninetcluster \\\n",
    " python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " --align cca \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:12:12.910330Z",
     "iopub.status.busy": "2025-01-16T23:12:12.910105Z",
     "iopub.status.idle": "2025-01-16T23:12:14.902190Z",
     "shell.execute_reply": "2025-01-16T23:12:14.899428Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLMA\n",
    "# https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "new_wd = os.path.join(RUN_FOLDER, 'NLMA')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n maninetcluster \\\n",
    " python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " --align nlma \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:12:14.906381Z",
     "iopub.status.busy": "2025-01-16T23:12:14.906183Z",
     "iopub.status.idle": "2025-01-16T23:13:00.288100Z",
     "shell.execute_reply": "2025-01-16T23:13:00.287534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random seed: 42\n",
      "Shape of Raw data\n",
      "Dataset 0: (300, 2000)\n",
      "Dataset 1: (300, 1000)\n",
      "Device: cuda:0\n",
      "---------------------------------\n",
      "Find correspondence between Dataset 1 and Dataset 2\n",
      "epoch:[500/2000] err:4.0456 alpha:0.5672\n",
      "epoch:[1000/2000] err:3.3870 alpha:0.6652\n",
      "epoch:[1500/2000] err:3.2498 alpha:0.7004\n",
      "epoch:[2000/2000] err:3.2501 alpha:0.7204\n",
      "Finished Matching!\n",
      "---------------------------------\n",
      "Train coupled autoencoders\n",
      "epoch:[500/10000]: loss:0.219908\n",
      "epoch:[1000/10000]: loss:0.246082\n",
      "epoch:[1500/10000]: loss:0.368438\n",
      "epoch:[2000/10000]: loss:0.369630\n",
      "epoch:[2500/10000]: loss:0.377075\n",
      "epoch:[3000/10000]: loss:0.416460\n",
      "Finished Mapping!\n",
      "---------------------------------\n",
      "JAMIE Done!\n",
      "Distance: 0.33583855256438255\n",
      "Correspondence: 13.384832225739956\n",
      "Mapping: 36.05147510394454\n",
      "Total: 49.77214588224888\n",
      "\n",
      "\n",
      "/home/thema/miniconda3/envs/jamie/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/thema/miniconda3/envs/jamie/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/thema/miniconda3/envs/jamie/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/thema/miniconda3/envs/jamie/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "2025-01-21 12:00:32.835793: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-21 12:00:32.970260: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-21 12:00:33.454476: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-21 12:00:33.454548: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-21 12:00:33.454555: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/thema/repos/inept/other_methods/JAMIE/jamie/jamie.py:440: UserWarning: PCA dim must be lower than 300, found 512, adjusting to compensate.\n",
      "  warnings.warn(\n",
      "/home/thema/repos/inept/other_methods/JAMIE/jamie/jamie.py:440: UserWarning: PCA dim must be lower than 300, found 512, adjusting to compensate.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JAMIE\n",
    "new_wd = os.path.join(RUN_FOLDER, 'JAMIE')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n jamie \\\n",
    " python {os.path.join(BASE_FOLDER, 'jamie_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:13:00.290378Z",
     "iopub.status.busy": "2025-01-16T23:13:00.290205Z",
     "iopub.status.idle": "2025-01-16T23:15:45.265938Z",
     "shell.execute_reply": "2025-01-16T23:15:45.263148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-21 12:01:26.466098: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2025-01-21 12:01:26.466121: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2025-01-21 12:01:26.466125: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2025-01-21 12:01:26.466128: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2025-01-21 12:01:26.466130: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MMD-MA\n",
    "# https://bitbucket.org/noblelab/2019_mmd_wabi/src/master/manifoldAlignDistortionPen_mmd_multipleStarts.py\n",
    "new_wd = os.path.join(RUN_FOLDER, 'MMD-MA')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n mmdma \\\n",
    " python {os.path.join(BASE_FOLDER, '2019_mmd_wabi/manifoldAlignDistortionPen_mmd_multipleStarts.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1_sim.tsv')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2_sim.tsv')} \\\n",
    " --seed {seed} \\\n",
    " --p {dimensions}\n",
    "!python {os.path.join(BASE_FOLDER, 'mmd_helper.py')}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:15:45.273304Z",
     "iopub.status.busy": "2025-01-16T23:15:45.272675Z",
     "iopub.status.idle": "2025-01-16T23:15:45.276854Z",
     "shell.execute_reply": "2025-01-16T23:15:45.276237Z"
    }
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "new_wd = os.path.join(RUN_FOLDER, 'KNN')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "# Load data\n",
    "X1, X2 = np.loadtxt('../X1.txt'), np.loadtxt('../X2.txt')\n",
    "dataset = [X1, X2]\n",
    "X, Y = dataset[-imputation_target], dataset[imputation_target]\n",
    "\n",
    "# Fit model\n",
    "knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=10)\n",
    "knn.fit(X, Y)\n",
    "projection = [None for _ in range(2)]\n",
    "projection[imputation_target] = knn.predict(X)\n",
    "\n",
    "# Write to file\n",
    "for i, proj in enumerate(projection):\n",
    "    if proj is not None: np.savetxt(f'P{i+1}.txt', proj)\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:15:45.279560Z",
     "iopub.status.busy": "2025-01-16T23:15:45.279323Z",
     "iopub.status.idle": "2025-01-16T23:15:45.282628Z",
     "shell.execute_reply": "2025-01-16T23:15:45.282110Z"
    }
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "new_wd = os.path.join(RUN_FOLDER, 'MLP')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "import sklearn.neighbors\n",
    "import sklearn.neural_network\n",
    "\n",
    "# Load data\n",
    "X1, X2 = np.loadtxt('../X1.txt'), np.loadtxt('../X2.txt')\n",
    "dataset = [X1, X2]\n",
    "X, Y = dataset[-imputation_target], dataset[imputation_target]\n",
    "\n",
    "# Fit model\n",
    "mlp = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(128,), max_iter=1_000)\n",
    "mlp.fit(X, Y)\n",
    "projection = [None for _ in range(2)]\n",
    "projection[imputation_target] = mlp.predict(X)\n",
    "\n",
    "# Write to file\n",
    "for i, proj in enumerate(projection):\n",
    "    if proj is not None: np.savetxt(f'P{i+1}.txt', proj)\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:15:45.284858Z",
     "iopub.status.busy": "2025-01-16T23:15:45.284665Z",
     "iopub.status.idle": "2025-01-16T23:15:45.287587Z",
     "shell.execute_reply": "2025-01-16T23:15:45.287087Z"
    }
   },
   "outputs": [],
   "source": [
    "# JAMIE\n",
    "# https://github.com/Oafish1/JAMIE\n",
    "new_wd = os.path.join(RUN_FOLDER, 'JAMIE')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n jamie \\\n",
    " python {os.path.join(BASE_FOLDER, 'jamie_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " -t {imputation_target+1} \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:PyTorch CUDA version: 10.0.130\n",
      "INFO:root:Parameter data: ['/home/thema/repos/inept/other_methods/runs/MERFISH/X.h5']\n",
      "INFO:root:Parameter snareseq: False\n",
      "INFO:root:Parameter shareseq: None\n",
      "INFO:root:Parameter nofilter: False\n",
      "INFO:root:Parameter linear: False\n",
      "INFO:root:Parameter clustermethod: leiden\n",
      "INFO:root:Parameter validcluster: 0\n",
      "INFO:root:Parameter testcluster: 1\n",
      "INFO:root:Parameter outdir: /home/thema/repos/inept/other_methods/runs/MERFISH/JAMIE\n",
      "INFO:root:Parameter naive: False\n",
      "INFO:root:Parameter hidden: [16]\n",
      "INFO:root:Parameter pretrain: \n",
      "INFO:root:Parameter lossweight: [1.33]\n",
      "INFO:root:Parameter optim: adam\n",
      "INFO:root:Parameter lr: [0.01]\n",
      "INFO:root:Parameter batchsize: [512]\n",
      "INFO:root:Parameter earlystop: 25\n",
      "INFO:root:Parameter seed: [182822]\n",
      "INFO:root:Parameter device: 0\n",
      "INFO:root:Parameter ext: pdf\n",
      "INFO:root:Reading RNA data\n",
      "INFO:root:Read in /home/thema/repos/inept/other_methods/runs/MERFISH/X.h5 for (2150, 253) (obs x var)\n",
      "Trying to set attribute `.obs` of view, making a copy.\n",
      "WARNING:root:Got multiple chromosomes for gene DUS4L-BCAP29: {'4', '7'}, skipping\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thema/repos/inept/other_methods/babel/bin/train_model.py\", line 537, in <module>\n",
      "    main()\n",
      "  File \"/home/thema/repos/inept/other_methods/babel/bin/train_model.py\", line 241, in main\n",
      "    **rna_data_kwargs,\n",
      "  File \"/home/thema/repos/inept/other_methods/babel/babel/sc_data_loaders.py\", line 346, in __init__\n",
      "    self.data_raw.var_names, gtf_file=gtf_file, return_chrom=True\n",
      "  File \"/home/thema/repos/inept/other_methods/babel/babel/sc_data_loaders.py\", line 1361, in reorder_genes_by_pos\n",
      "    assert genes_intersection, \"Got empty list of intersected genes\"\n",
      "AssertionError: Got empty list of intersected genes\n",
      "\n",
      "ERROR conda.cli.main_run:execute(47): `conda run python /home/thema/repos/inept/other_methods/babel/bin/train_model.py --data /home/thema/repos/inept/other_methods/runs/MERFISH/X.h5 --outdir /home/thema/repos/inept/other_methods/runs/MERFISH/JAMIE` failed. (See above for error)\n",
      "/bin/bash: -c: line 0: syntax error near unexpected token `('\n",
      "/bin/bash: -c: line 0: `conda run -n babel   python {os.path.join(BASE_FOLDER, 'babel/bin/predict_model.py')}   --checkpoint {os.path.join(new_wd, net_asdf)}   --data {os.path.join(RUN_FOLDER, 'X.h5')}   --outdir {new_wd}'\n"
     ]
    }
   ],
   "source": [
    "# BABEL\n",
    "# https://github.com/wukevin/babel\n",
    "if False:\n",
    "    new_wd = os.path.join(RUN_FOLDER, 'babel')\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "    !conda run -n babel \\\n",
    "    python {os.path.join(BASE_FOLDER, 'babel/bin/train_model.py')} \\\n",
    "    --data {os.path.join(RUN_FOLDER, 'X.h5')} \\\n",
    "    --outdir {new_wd}\n",
    "    !conda run -n babel \\\n",
    "    python {os.path.join(BASE_FOLDER, 'babel/bin/predict_model.py')} \\\n",
    "    --checkpoint {os.path.join(new_wd, net_asdf)} \\\n",
    "    --data {os.path.join(RUN_FOLDER, 'X.h5')} \\\n",
    "    --outdir {new_wd}\n",
    "\n",
    "    os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:15:45.289936Z",
     "iopub.status.busy": "2025-01-16T23:15:45.289712Z",
     "iopub.status.idle": "2025-01-16T23:15:45.293345Z",
     "shell.execute_reply": "2025-01-16T23:15:45.292585Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# scVI\n",
    "if False:  # Not done\n",
    "    new_wd = os.path.join(RUN_FOLDER, 'scVI')\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "\n",
    "    X_fname = os.path.join(RUN_FOLDER, 'X1.txt')\n",
    "    Y_fname = os.path.join(RUN_FOLDER, 'X2.txt')\n",
    "\n",
    "    import numpy as np\n",
    "    import scvi\n",
    "\n",
    "    scvi.settings.seed = 42\n",
    "\n",
    "    X = np.loadtxt(X_fname)\n",
    "    Y = np.loadtxt(Y_fname)\n",
    "\n",
    "    scvi.model.SCVI.setup_anndata\n",
    "\n",
    "\n",
    "    os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perturbation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance\n",
    "# https://shap.readthedocs.io/en/latest/\n",
    "new_wd = os.path.join(RUN_FOLDER, 'variance')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    " # Load data\n",
    "X1, X2 = np.loadtxt(os.path.join(RUN_FOLDER, 'X1.txt')), np.loadtxt(os.path.join(RUN_FOLDER, 'X2.txt'))\n",
    "dataset = [X1, X2]\n",
    "\n",
    "# Get variance\n",
    "importance = [np.var(X, axis=0) for X in dataset]\n",
    "importance = [imp / imp.sum() for imp in dataset]\n",
    "\n",
    "# Write to file\n",
    "for i, imp in enumerate(importance):\n",
    "    if imp is not None: np.savetxt(f'I{i+1}.txt', imp)\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
