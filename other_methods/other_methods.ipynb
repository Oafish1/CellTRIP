{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:30:19.366044Z",
     "iopub.status.busy": "2025-01-30T17:30:19.365065Z",
     "iopub.status.idle": "2025-01-30T17:30:22.657871Z",
     "shell.execute_reply": "2025-01-30T17:30:22.657393Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy.sparse\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import celltrip\n",
    "\n",
    "os.environ['AWS_PROFILE'] = 'waisman-admin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run name\n",
    "run_name = 'Flysta3D_L3_a'\n",
    "\n",
    "# Policy and Environment\n",
    "train_split = .8\n",
    "train_partitions = False\n",
    "input_modalities = None\n",
    "target_modalities = [1]\n",
    "dim = 8\n",
    "\n",
    "# Data\n",
    "input_files = []\n",
    "merge_files = [[f's3://nkalafut-celltrip/Flysta3D/{p}_{m}.h5ad' for p in ('L2_a',)] for m in ('expression', 'spatial')]\n",
    "backed = True\n",
    "partition_cols = ['development']  # slice_ID\n",
    "type_key = 'annotation'  # MERFISH ('layer'), scGLUE ('cell_type'), scMultiSim ('cell.type'), Flysta3D ('annotation'), TemporalBrain ('Cell type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run name\n",
    "# run_name = 'MERFISH'\n",
    "\n",
    "# # Policy and Environment\n",
    "# train_split = .8\n",
    "# train_partitions = False\n",
    "# input_modalities = None\n",
    "# target_modalities = [1]\n",
    "# dim = 8\n",
    "\n",
    "# # Data\n",
    "# input_files = ['s3://nkalafut-celltrip/MERFISH/expression.h5ad', 's3://nkalafut-celltrip/MERFISH/spatial.h5ad']\n",
    "# merge_files = []\n",
    "# backed = True\n",
    "# partition_cols = None\n",
    "# type_key = 'layer'  # MERFISH ('layer'), scGLUE ('cell_type'), scMultiSim ('cell.type'), Flysta3D ('annotation'), TemporalBrain ('Cell type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/repos/inept/celltrip/utility/processing.py:110: RuntimeWarning: Modality 0 too small for PCA (253 features), skipping\n",
      "  warnings.warn(\n",
      "/home/thema/repos/inept/celltrip/utility/processing.py:110: RuntimeWarning: Modality 1 too small for PCA (2 features), skipping\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Seeding\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize locally\n",
    "env_init, policy_init, memory_init = celltrip.train.get_initializers(\n",
    "    input_files=input_files, merge_files=merge_files, backed=backed,\n",
    "    partition_cols=partition_cols, dataloader_kwargs={'mask': train_split, 'mask_partitions': train_partitions, 'num_nodes': 1_500},  # TODO: Subsampling, maybe remove initial env.reset()\n",
    "    environment_kwargs={'input_modalities': input_modalities, 'target_modalities': target_modalities, 'dim': dim})  # , 'spherical': discrete\n",
    "env = env_init().to('cuda')\n",
    "\n",
    "# Store mask for later use\n",
    "full_train_mask = env.dataloader.mask\n",
    "env.dataloader.mask = None\n",
    "\n",
    "# Directories\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data')\n",
    "RUN_FOLDER = os.path.join(BASE_FOLDER, 'runs', run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset(partition=('L2_a',))\n",
    "env.reset(partition=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:30:22.660521Z",
     "iopub.status.busy": "2025-01-30T17:30:22.660207Z",
     "iopub.status.idle": "2025-01-30T17:30:40.192705Z",
     "shell.execute_reply": "2025-01-30T17:30:40.191803Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# # rypltvk5 (ts), 32jqyk54, c8zsunc9,\n",
    "# run_id = '32jqyk54'\n",
    "# total_statistics = False\n",
    "\n",
    "# # Get run\n",
    "# api = wandb.Api()\n",
    "# run = api.run(f'oafish/cellTRIP/{run_id}')\n",
    "# config = defaultdict(lambda: {})\n",
    "# for k, v in run.config.items():\n",
    "#     dict_name, key = k.split('/')\n",
    "#     config[dict_name][key] = v\n",
    "# config = dict(config)\n",
    "\n",
    "# # Parameters\n",
    "# dataset_name = config['data']['dataset']\n",
    "# imputation_target = config['env']['reward_distance_target']\n",
    "# dimensions = config['env']['dim']\n",
    "# notebook_seed = np.random.randint(2**32)\n",
    "\n",
    "# # Apply seed\n",
    "# torch.manual_seed(notebook_seed)\n",
    "# if torch.cuda.is_available(): torch.cuda.manual_seed(notebook_seed)\n",
    "# np.random.seed(notebook_seed)\n",
    "\n",
    "# # Derivatives\n",
    "# RUN_FOLDER = os.path.join(BASE_FOLDER, 'runs', dataset_name)\n",
    "\n",
    "# # Load data and save to file\n",
    "# modalities, types, features = data.load_data(dataset_name, DATA_FOLDER)\n",
    "# ppc = celltrip.utilities.Preprocessing(**config['data'])  # Potentially mismatched if sampled\n",
    "# modalities, types = ppc.fit_transform(modalities, types, total_statistics=total_statistics)\n",
    "\n",
    "# if not os.path.exists(RUN_FOLDER): os.makedirs(RUN_FOLDER)\n",
    "# for i in range(len(modalities)):\n",
    "#     modality = modalities[i]\n",
    "\n",
    "#     # Regular matrices (ManiNetCluster, JAMIE)\n",
    "#     np.savetxt(os.path.join(RUN_FOLDER, f'X{i+1}.txt'), modality, delimiter='\\t')\n",
    "\n",
    "#     # Similarity matrices (MMD-MA)\n",
    "#     modality_z = modality - modality.mean(axis=0, keepdims=True) / modality.std(axis=0, keepdims=True)\n",
    "#     similarity = np.matmul(modality_z, modality_z.T)\n",
    "#     np.savetxt(os.path.join(RUN_FOLDER, f'X{i+1}_sim.tsv'), similarity, delimiter='\\t')\n",
    "\n",
    "#     # Anndata matrices (scVI)\n",
    "#     # adata = sc.AnnData(modalities[0])\n",
    "#     # adata.var_names = features[0] if isinstance(features[0][0], str) else [f'Feature_{fi}' for fi in features[0]]\n",
    "#     # adata.obs_names = [f'Cell_{j}' for j in range(len(adata.obs_names))]\n",
    "#     # adata.obs['cell_type'] = types[0][:, 0]\n",
    "#     # adata.obs['time'] = types[0][:, -1]\n",
    "#     # # adata.obs['batch'] = 0\n",
    "#     # adata.write(os.path.join(RUN_FOLDER, f'X{i+1}.h5ad'), compression='gzip')\n",
    "\n",
    "#     # HDFS\n",
    "#     # https://github.com/scverse/anndata/issues/595#issuecomment-1824376236\n",
    "#     concatenated_modalities = np.concatenate(modalities, axis=-1)\n",
    "#     barcodes = [f'Cell {i}' for i in range(concatenated_modalities.shape[0])]\n",
    "#     feature_types = modalities[0].shape[1] * ['Gene Expression'] + modalities[1].shape[1] * ['Peaks']\n",
    "#     feature_names = np.concatenate(features)\n",
    "#     feature_ids = np.array(np.arange(feature_names.shape[0]), dtype='str')\n",
    "#     genome = concatenated_modalities.shape[1] * ['Something']\n",
    "#     sparse_data = scipy.sparse.csr_matrix(concatenated_modalities)\n",
    "\n",
    "#     def int_max(x):\n",
    "#         return int(max(np.floor(len(str(int(max(x)))) / 4), 1) * 4)\n",
    "#     def str_max(x):\n",
    "#         return max([len(i) for i in x])\n",
    "\n",
    "#     with h5py.File(os.path.join(RUN_FOLDER, f'X.h5'), 'w') as f:\n",
    "#         grp = f.create_group('matrix')\n",
    "#         grp.create_dataset('barcodes', data=np.array(barcodes, dtype=f'|S{str_max(barcodes)}'))\n",
    "#         grp.create_dataset('data', data=np.array(sparse_data.data, dtype=f'<i{int_max(sparse_data.data)}'))\n",
    "#         ftrs = grp.create_group('features')\n",
    "#         # # this group will lack the following keys:\n",
    "#         # # '_all_tag_keys', 'feature_type', 'genome', 'id', 'name', 'pattern', 'read', 'sequence'\n",
    "#         ftrs.create_dataset('feature_type', data=np.array(feature_types, dtype=f'|S{str_max(feature_types)}'))\n",
    "#         ftrs.create_dataset('genome', data=np.array(genome, dtype=f'|S{str_max(genome)}'))\n",
    "#         ftrs.create_dataset('id', data=np.array(feature_ids, dtype=f'|S{str_max(feature_ids)}'))\n",
    "#         ftrs.create_dataset('name', data=np.array(feature_names, dtype=f'|S{str_max([str(fn) for fn in feature_names])}'))\n",
    "#         grp.create_dataset('indices', data=np.array(sparse_data.indices, dtype=f'<i{int_max(sparse_data.indices)}'))\n",
    "#         grp.create_dataset('indptr', data=np.array(sparse_data.indptr, dtype=f'<i{int_max(sparse_data.indptr)}'))\n",
    "#         grp.create_dataset('shape', data=np.array(sparse_data.shape[::-1], dtype=f'<i{int_max(sparse_data.shape)}'))\n",
    "\n",
    "# # Preview h5 files\n",
    "# # print('Generated File')\n",
    "# # with h5py.File(os.path.join(RUN_FOLDER, f'X.h5'), 'r') as f: celltrip.utilities.h5_tree(f)\n",
    "# # print('\\nBaseline File')\n",
    "# # with h5py.File('/home/thema/Downloads/DM_rep4.h5', 'r') as f: celltrip.utilities.h5_tree(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:30:40.197641Z",
     "iopub.status.busy": "2025-01-30T17:30:40.197160Z",
     "iopub.status.idle": "2025-01-30T17:30:40.235474Z",
     "shell.execute_reply": "2025-01-30T17:30:40.234448Z"
    }
   },
   "outputs": [],
   "source": [
    "# if imputation_target is None:\n",
    "#     # LMA\n",
    "#     # https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "#     method_name = 'LMA'\n",
    "#     print(f'Running {method_name}')\n",
    "#     new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "#     if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "#     os.chdir(new_wd)\n",
    "\n",
    "#     !conda run -n maninetcluster \\\n",
    "#     python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    "#     {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    "#     {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    "#     --align lma \\\n",
    "#     -p {dimensions}\n",
    "\n",
    "#     os.chdir(BASE_FOLDER)\n",
    "\n",
    "\n",
    "#     # CCA\n",
    "#     # https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "#     method_name = 'CCA'\n",
    "#     print(f'Running {method_name}')\n",
    "#     new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "#     if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "#     os.chdir(new_wd)\n",
    "\n",
    "#     !conda run -n maninetcluster \\\n",
    "#     python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    "#     {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    "#     {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    "#     --align cca \\\n",
    "#     -p {dimensions}\n",
    "\n",
    "#     os.chdir(BASE_FOLDER)\n",
    "\n",
    "\n",
    "#     # NLMA\n",
    "#     # https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "#     method_name = 'NLMA'\n",
    "#     print(f'Running {method_name}')\n",
    "#     new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "#     if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "#     os.chdir(new_wd)\n",
    "\n",
    "#     !conda run -n maninetcluster \\\n",
    "#     python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    "#     {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    "#     {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    "#     --align nlma \\\n",
    "#     -p {dimensions}\n",
    "\n",
    "#     os.chdir(BASE_FOLDER)\n",
    "\n",
    "\n",
    "#     # JAMIE\n",
    "#     method_name = 'JAMIE'\n",
    "#     print(f'Running {method_name}')\n",
    "#     new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "#     if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "#     os.chdir(new_wd)\n",
    "\n",
    "#     !conda run -n jamie \\\n",
    "#     python {os.path.join(BASE_FOLDER, 'jamie_helper.py')} \\\n",
    "#     {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    "#     {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    "#     -p {dimensions} \\\n",
    "#     -s {notebook_seed} \\\n",
    "#     --suffix {notebook_seed}\n",
    "\n",
    "#     os.chdir(BASE_FOLDER)\n",
    "\n",
    "\n",
    "#     # MMD-MA\n",
    "#     # https://bitbucket.org/noblelab/2019_mmd_wabi/src/master/manifoldAlignDistortionPen_mmd_multipleStarts.py\n",
    "#     method_name = 'MMD-MA'\n",
    "#     print(f'Running {method_name}')\n",
    "#     new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "#     if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "#     os.chdir(new_wd)\n",
    "\n",
    "#     fname1, fname2 = f'alpha_hat_{notebook_seed}_10000.txt', f'beta_hat_{notebook_seed}_10000.txt'\n",
    "#     !conda run -n mmdma \\\n",
    "#     python {os.path.join(BASE_FOLDER, '2019_mmd_wabi/manifoldAlignDistortionPen_mmd_multipleStarts.py')} \\\n",
    "#     {os.path.join(RUN_FOLDER, 'X1_sim.tsv')} \\\n",
    "#     {os.path.join(RUN_FOLDER, 'X2_sim.tsv')} \\\n",
    "#     --seed {notebook_seed} \\\n",
    "#     --p {dimensions}\n",
    "#     !python {os.path.join(BASE_FOLDER, 'mmd_helper.py')} \\\n",
    "#     {fname1} \\\n",
    "#     {fname2} \\\n",
    "#     --suffix {notebook_seed}\n",
    "\n",
    "#     os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_imputation(X, Y, seed, **kwargs):\n",
    "    # Seed and generate\n",
    "    torch.manual_seed(seed)\n",
    "    projection = torch.distributions.Normal(0, 1).sample(Y.shape)\n",
    "    np.savetxt(f'I_{seed}.txt', projection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_imputation(X, Y, seed, *, train_mask, n_neighbors=10):\n",
    "    import sklearn.neighbors\n",
    "    \n",
    "    # Fit model\n",
    "    knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "    knn.fit(X[train_mask], Y[train_mask])\n",
    "    projection = knn.predict(X)\n",
    "    np.savetxt(f'I_{seed}.txt', projection)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_imputation(X, Y, seed, *, train_mask):\n",
    "    import sklearn.neural_network\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Fit model\n",
    "    mlp = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(128,), max_iter=1_000)\n",
    "    mlp.fit(X[train_mask], Y[train_mask])\n",
    "    projection = mlp.predict(X)\n",
    "    np.savetxt(f'I_{seed}.txt', projection)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jamie_imputation(X, Y, seed, *, train_mask):\n",
    "    # https://github.com/Oafish1/JAMIE\n",
    "    import subprocess\n",
    "\n",
    "    # Generate files\n",
    "    fname_X = os.path.join(RUN_FOLDER, 'X.txt')\n",
    "    fname_Y = os.path.join(RUN_FOLDER, 'Y.txt')\n",
    "    fname_train_mask = os.path.join(RUN_FOLDER, 'train_mask.txt')\n",
    "    if not os.path.exists(fname_X): np.savetxt(fname_X, X)\n",
    "    if not os.path.exists(fname_Y): np.savetxt(fname_Y, Y)\n",
    "    if not os.path.exists(fname_train_mask): np.savetxt(fname_train_mask, train_mask)\n",
    "\n",
    "    # Run\n",
    "    subprocess.run(\n",
    "        f'conda run -n jamie '\n",
    "        f'python \"{os.path.join(BASE_FOLDER, \"jamie_helper.py\")}\" '\n",
    "        f'\"{fname_X}\" \"{fname_Y}\" -m \"{fname_train_mask}\" '\n",
    "        f'-t 2 -p {dim} -s {seed} --suffix \"{seed}\"',\n",
    "        stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT, shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # BABEL\n",
    "    # INCOMPATIBLE\n",
    "    # https://github.com/wukevin/babel\n",
    "    # method_name = 'BABEL'\n",
    "    # print(f'Running {method_name}')\n",
    "    # new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    # if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    # os.chdir(new_wd)\n",
    "\n",
    "    # !conda run -n babel \\\n",
    "    # python {os.path.join(BASE_FOLDER, 'babel/bin/train_model.py')} \\\n",
    "    # --data {os.path.join(RUN_FOLDER, 'X.h5')} \\\n",
    "    # --outdir {new_wd}\n",
    "    # !conda run -n babel \\\n",
    "    # python {os.path.join(BASE_FOLDER, 'babel/bin/predict_model.py')} \\\n",
    "    # --checkpoint {os.path.join(new_wd, net_asdf)} \\\n",
    "    # --data {os.path.join(RUN_FOLDER, 'X.h5')} \\\n",
    "    # --outdir {new_wd}\n",
    "\n",
    "    # os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # scVI\n",
    "    # Not done\n",
    "    # method_name = 'scVI'\n",
    "    # print(f'Running {method_name}')\n",
    "    # new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    # if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    # os.chdir(new_wd)\n",
    "\n",
    "    # X_fname = os.path.join(RUN_FOLDER, 'X1.txt')\n",
    "    # Y_fname = os.path.join(RUN_FOLDER, 'X2.txt')\n",
    "\n",
    "    # import numpy as np\n",
    "    # import scvi\n",
    "\n",
    "    # scvi.settings.seed = 42\n",
    "\n",
    "    # X = np.loadtxt(X_fname)\n",
    "    # Y = np.loadtxt(Y_fname)\n",
    "\n",
    "    # scvi.model.SCVI.setup_anndata\n",
    "\n",
    "    # os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Random\n",
      "Running Random\n",
      "Running Random\n",
      "Running Random\n",
      "Running Random\n",
      "Running KNN\n",
      "Running KNN\n",
      "Running KNN\n",
      "Running KNN\n",
      "Running KNN\n",
      "Running MLP\n",
      "Running MLP\n",
      "Running MLP\n",
      "Running MLP\n",
      "Running MLP\n"
     ]
    }
   ],
   "source": [
    "seeds = [42, 128, 256, 512, 1024]\n",
    "imputation_methods = {\n",
    "    'Random': random_imputation,\n",
    "    'KNN': knn_imputation,\n",
    "    'MLP': mlp_imputation,\n",
    "    # 'JAMIE': jamie_imputation,\n",
    "}\n",
    "\n",
    "# Load data\n",
    "    # NOTE: Currently incompatible with multi-multi\n",
    "X, Y = env.get_input_modalities()[0].cpu(), env.get_target_modalities()[0].cpu()\n",
    "train_mask = pd.DataFrame(full_train_mask, index=env.dataloader.adatas[0].obs.index).loc[env.keys].to_numpy().flatten()\n",
    "\n",
    "for method, seed in itertools.product(imputation_methods, seeds):\n",
    "    # CLI\n",
    "    print(f'Running {method}')\n",
    "\n",
    "    # Create folder and change wd\n",
    "    new_wd = os.path.join(RUN_FOLDER, method)\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "    # Run function\n",
    "    imputation_methods[method](X, Y, seed, train_mask=train_mask)\n",
    "\n",
    "    # Revert wd\n",
    "    os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perturbation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:35:53.496184Z",
     "iopub.status.busy": "2025-01-30T17:35:53.495677Z",
     "iopub.status.idle": "2025-01-30T17:35:53.872630Z",
     "shell.execute_reply": "2025-01-30T17:35:53.872126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Variance\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ppc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m X1, X2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(RUN_FOLDER, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX1.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)), np\u001b[38;5;241m.\u001b[39mloadtxt(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(RUN_FOLDER, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX2.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m [X1, X2]\n\u001b[0;32m---> 12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mppc\u001b[49m\u001b[38;5;241m.\u001b[39minverse_transform(dataset)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Get variance\u001b[39;00m\n\u001b[1;32m     15\u001b[0m importance \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mvar(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m dataset]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ppc' is not defined"
     ]
    }
   ],
   "source": [
    "# # Variance\n",
    "# # https://shap.readthedocs.io/en/latest/\n",
    "# method_name = 'Variance'\n",
    "# print(f'Running {method_name}')\n",
    "# new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "# if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "# os.chdir(new_wd)\n",
    "\n",
    "#  # Load data\n",
    "# X1, X2 = np.loadtxt(os.path.join(RUN_FOLDER, 'X1.txt')), np.loadtxt(os.path.join(RUN_FOLDER, 'X2.txt'))\n",
    "# dataset = [X1, X2]\n",
    "# dataset = ppc.inverse_transform(dataset)\n",
    "\n",
    "# # Get variance\n",
    "# importance = [np.var(X, axis=0) for X in dataset]\n",
    "# importance = [imp / imp.sum() for imp in importance]\n",
    "\n",
    "# # Write to file\n",
    "# for i, imp in enumerate(importance):\n",
    "#     if imp is not None: np.savetxt(f'F{i+1}.txt', imp)\n",
    "\n",
    "# os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
