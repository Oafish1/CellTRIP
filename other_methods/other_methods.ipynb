{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:10:30.566908Z",
     "iopub.status.busy": "2025-01-16T23:10:30.566454Z",
     "iopub.status.idle": "2025-01-16T23:10:32.361001Z",
     "shell.execute_reply": "2025-01-16T23:10:32.360490Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import torch\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../examples'))\n",
    "import data\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:10:32.363302Z",
     "iopub.status.busy": "2025-01-16T23:10:32.363033Z",
     "iopub.status.idle": "2025-01-16T23:10:32.365748Z",
     "shell.execute_reply": "2025-01-16T23:10:32.365382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataset_name = 'MERFISH'\n",
    "imputation_target = None\n",
    "dimensions = 3\n",
    "seed = 42\n",
    "\n",
    "# Derivatives\n",
    "RUN_FOLDER = os.path.join(BASE_FOLDER, 'runs', dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:10:32.367432Z",
     "iopub.status.busy": "2025-01-16T23:10:32.367278Z",
     "iopub.status.idle": "2025-01-16T23:10:33.241310Z",
     "shell.execute_reply": "2025-01-16T23:10:33.240850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and save to file\n",
    "modalities, types, features = data.load_data(dataset_name, DATA_FOLDER)\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER): os.makedirs(RUN_FOLDER)\n",
    "for i in range(len(modalities)):\n",
    "    modality = modalities[i]\n",
    "\n",
    "    # Regular matrices (ManiNetCluster, JAMIE)\n",
    "    np.savetxt(os.path.join(RUN_FOLDER, f'X{i+1}.txt'), modality, delimiter='\\t')\n",
    "\n",
    "    # Similarity matrices (MMD-MA)\n",
    "    modality_z = modality - modality.mean(axis=0, keepdims=True) / modality.std(axis=0, keepdims=True)\n",
    "    similarity = np.matmul(modality_z, modality_z.T)\n",
    "    np.savetxt(os.path.join(RUN_FOLDER, f'X{i+1}_sim.tsv'), similarity, delimiter='\\t')\n",
    "\n",
    "    # Anndata matrices (scVI)\n",
    "    adata = sc.AnnData(modalities[0])\n",
    "    adata.var_names = features[0] if isinstance(features[0][0], str) else [f'Feature_{fi}' for fi in features[0]]\n",
    "    adata.obs_names = [f'Cell_{j}' for j in range(len(adata.obs_names))]\n",
    "    adata.obs['cell_type'] = types[0][:, 0]\n",
    "    adata.obs['time'] = types[0][:, -1]\n",
    "    # adata.obs['batch'] = 0\n",
    "    adata.write(os.path.join(RUN_FOLDER, f'X{i+1}.h5ad'), compression='gzip')\n",
    "\n",
    "    # HDFS\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h5_tree(val, pre=''):\n",
    "    items = len(val)\n",
    "    for key, val in val.items():\n",
    "        items -= 1\n",
    "        if items == 0:\n",
    "            # the last item\n",
    "            if type(val) == h5py._hl.group.Group:\n",
    "                print(pre + '└── ' + key)\n",
    "                h5_tree(val, pre+'    ')\n",
    "            else:\n",
    "                try:\n",
    "                    print(pre + '└── ' + key + ' (%d)' % len(val))\n",
    "                except TypeError:\n",
    "                    print(pre + '└── ' + key + ' (scalar)')\n",
    "        else:\n",
    "            if type(val) == h5py._hl.group.Group:\n",
    "                print(pre + '├── ' + key)\n",
    "                h5_tree(val, pre+'│   ')\n",
    "            else:\n",
    "                try:\n",
    "                    print(pre + '├── ' + key + ' (%d)' % len(val))\n",
    "                except TypeError:\n",
    "                    print(pre + '├── ' + key + ' (scalar)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└── matrix\n",
      "    ├── barcodes (2150)\n",
      "    ├── data (119608)\n",
      "    ├── features\n",
      "    │   ├── feature_type (255)\n",
      "    │   ├── genome (255)\n",
      "    │   ├── id (255)\n",
      "    │   └── name (255)\n",
      "    ├── indices (119608)\n",
      "    ├── indptr (2151)\n",
      "    └── shape (2)\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "concatenated_modalities = np.concatenate(modalities, axis=-1)\n",
    "barcodes = [f'Cell {i}' for i in range(concatenated_modalities.shape[0])]\n",
    "feature_types = modalities[0].shape[1] * ['Gene Expression'] + modalities[1].shape[1] * ['Peaks']\n",
    "feature_names = np.concatenate(features)\n",
    "feature_ids = np.array(np.arange(feature_names.shape[0]), dtype='str')\n",
    "genome = concatenated_modalities.shape[1] * ['Something']\n",
    "sparse_data = scipy.sparse.csr_matrix(concatenated_modalities)\n",
    "\n",
    "def int_max(x):\n",
    "    return int(max(np.floor(len(str(int(max(x)))) / 4), 1) * 4)\n",
    "def str_max(x):\n",
    "    return max([len(i) for i in x])\n",
    "\n",
    "with h5py.File(os.path.join(RUN_FOLDER, f'X.h5'), 'w') as f:\n",
    "    grp = f.create_group('matrix')\n",
    "    grp.create_dataset('barcodes', data=np.array(barcodes, dtype=f'|S{str_max(barcodes)}'))\n",
    "    grp.create_dataset('data', data=np.array(sparse_data.data, dtype=f'<i{int_max(sparse_data.data)}'))\n",
    "    ftrs = grp.create_group('features')\n",
    "    # # this group will lack the following keys:\n",
    "    # # '_all_tag_keys', 'feature_type', 'genome', 'id', 'name', 'pattern', 'read', 'sequence'\n",
    "    ftrs.create_dataset('feature_type', data=np.array(feature_types, dtype=f'|S{str_max(feature_types)}'))\n",
    "    ftrs.create_dataset('genome', data=np.array(genome, dtype=f'|S{str_max(genome)}'))\n",
    "    ftrs.create_dataset('id', data=np.array(feature_ids, dtype=f'|S{str_max(feature_ids)}'))\n",
    "    ftrs.create_dataset('name', data=np.array(feature_names, dtype=f'|S{str_max(feature_names)}'))\n",
    "    grp.create_dataset('indices', data=np.array(sparse_data.indices, dtype=f'<i{int_max(sparse_data.indices)}'))\n",
    "    grp.create_dataset('indptr', data=np.array(sparse_data.indptr, dtype=f'<i{int_max(sparse_data.indptr)}'))\n",
    "    grp.create_dataset('shape', data=np.array(sparse_data.shape[::-1], dtype=f'<i{int_max(sparse_data.shape)}'))\n",
    "    h5_tree(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└── matrix\n",
      "    ├── barcodes (5517)\n",
      "    ├── data (73404624)\n",
      "    ├── features\n",
      "    │   ├── _all_tag_keys (2)\n",
      "    │   ├── feature_type (147085)\n",
      "    │   ├── genome (147085)\n",
      "    │   ├── id (147085)\n",
      "    │   ├── interval (147085)\n",
      "    │   └── name (147085)\n",
      "    ├── indices (73404624)\n",
      "    ├── indptr (5518)\n",
      "    └── shape (2)\n",
      "[b'Gene Expression' b'Gene Expression' b'Gene Expression' ... b'Peaks'\n",
      " b'Peaks' b'Peaks']\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('/home/thema/Downloads/DM_rep4.h5', 'r') as f:\n",
    "    h5_tree(f)\n",
    "    print(f['matrix']['features']['feature_type'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:10:33.243535Z",
     "iopub.status.busy": "2025-01-16T23:10:33.243375Z",
     "iopub.status.idle": "2025-01-16T23:11:21.942767Z",
     "shell.execute_reply": "2025-01-16T23:11:21.939936Z"
    }
   },
   "outputs": [],
   "source": [
    "# LMA\n",
    "# https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "new_wd = os.path.join(RUN_FOLDER, 'LMA')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n maninetcluster \\\n",
    " python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " --align lma \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:11:21.946314Z",
     "iopub.status.busy": "2025-01-16T23:11:21.946100Z",
     "iopub.status.idle": "2025-01-16T23:12:12.906694Z",
     "shell.execute_reply": "2025-01-16T23:12:12.903989Z"
    }
   },
   "outputs": [],
   "source": [
    "# CCA\n",
    "# https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "new_wd = os.path.join(RUN_FOLDER, 'CCA')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n maninetcluster \\\n",
    " python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " --align cca \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:12:12.910330Z",
     "iopub.status.busy": "2025-01-16T23:12:12.910105Z",
     "iopub.status.idle": "2025-01-16T23:12:14.902190Z",
     "shell.execute_reply": "2025-01-16T23:12:14.899428Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLMA\n",
    "# https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "new_wd = os.path.join(RUN_FOLDER, 'NLMA')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n maninetcluster \\\n",
    " python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " --align nlma \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:12:14.906381Z",
     "iopub.status.busy": "2025-01-16T23:12:14.906183Z",
     "iopub.status.idle": "2025-01-16T23:13:00.288100Z",
     "shell.execute_reply": "2025-01-16T23:13:00.287534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random seed: 42\r\n",
      "Shape of Raw data\r\n",
      "Dataset 0: (300, 2000)\r\n",
      "Dataset 1: (300, 1000)\r\n",
      "Device: cuda:0\r\n",
      "---------------------------------\r\n",
      "Find correspondence between Dataset 1 and Dataset 2\r\n",
      "epoch:[500/2000] err:4.0456 alpha:0.5672\r\n",
      "epoch:[1000/2000] err:3.3870 alpha:0.6652\r\n",
      "epoch:[1500/2000] err:3.2498 alpha:0.7004\r\n",
      "epoch:[2000/2000] err:3.2501 alpha:0.7204\r\n",
      "Finished Matching!\r\n",
      "---------------------------------\r\n",
      "Train coupled autoencoders\r\n",
      "epoch:[500/10000]: loss:0.219908\r\n",
      "epoch:[1000/10000]: loss:0.246082\r\n",
      "epoch:[1500/10000]: loss:0.368438\r\n",
      "epoch:[2000/10000]: loss:0.369630\r\n",
      "epoch:[2500/10000]: loss:0.377075\r\n",
      "epoch:[3000/10000]: loss:0.416460\r\n",
      "Finished Mapping!\r\n",
      "---------------------------------\r\n",
      "JAMIE Done!\r\n",
      "Distance: 0.17655884940177202\r\n",
      "Correspondence: 1.8783075734972954\r\n",
      "Mapping: 35.27650882396847\r\n",
      "Total: 37.33137524686754\r\n",
      "\r\n",
      "\r\n",
      "/home/thema/miniconda3/envs/jamie/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\r\n",
      "  @numba.jit()\r\n",
      "/home/thema/miniconda3/envs/jamie/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\r\n",
      "  @numba.jit()\r\n",
      "/home/thema/miniconda3/envs/jamie/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\r\n",
      "  @numba.jit()\r\n",
      "/home/thema/miniconda3/envs/jamie/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\r\n",
      "  @numba.jit()\r\n",
      "2025-01-16 17:12:20.432770: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2025-01-16 17:12:20.566788: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-01-16 17:12:21.052402: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\r\n",
      "2025-01-16 17:12:21.052463: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\r\n",
      "2025-01-16 17:12:21.052470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n",
      "/home/thema/repos/inept/other_methods/JAMIE/jamie/jamie.py:440: UserWarning: PCA dim must be lower than 300, found 512, adjusting to compensate.\r\n",
      "  warnings.warn(\r\n",
      "/home/thema/repos/inept/other_methods/JAMIE/jamie/jamie.py:440: UserWarning: PCA dim must be lower than 300, found 512, adjusting to compensate.\r\n",
      "  warnings.warn(\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# JAMIE\n",
    "new_wd = os.path.join(RUN_FOLDER, 'JAMIE')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n jamie \\\n",
    " python {os.path.join(BASE_FOLDER, 'jamie_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:13:00.290378Z",
     "iopub.status.busy": "2025-01-16T23:13:00.290205Z",
     "iopub.status.idle": "2025-01-16T23:15:45.265938Z",
     "shell.execute_reply": "2025-01-16T23:15:45.263148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-16 17:13:02.943094: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2025-01-16 17:13:02.943111: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2025-01-16 17:13:02.943115: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2025-01-16 17:13:02.943118: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "2025-01-16 17:13:02.943121: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# MMD-MA\n",
    "# https://bitbucket.org/noblelab/2019_mmd_wabi/src/master/manifoldAlignDistortionPen_mmd_multipleStarts.py\n",
    "new_wd = os.path.join(RUN_FOLDER, 'MMD-MA')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n mmdma \\\n",
    " python {os.path.join(BASE_FOLDER, '2019_mmd_wabi/manifoldAlignDistortionPen_mmd_multipleStarts.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1_sim.tsv')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2_sim.tsv')} \\\n",
    " --seed {seed} \\\n",
    " --p {dimensions}\n",
    "!python {os.path.join(BASE_FOLDER, 'mmd_helper.py')}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:15:45.273304Z",
     "iopub.status.busy": "2025-01-16T23:15:45.272675Z",
     "iopub.status.idle": "2025-01-16T23:15:45.276854Z",
     "shell.execute_reply": "2025-01-16T23:15:45.276237Z"
    }
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "new_wd = os.path.join(RUN_FOLDER, 'KNN')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "# Load data\n",
    "X1, X2 = np.loadtxt('../X1.txt'), np.loadtxt('../X2.txt')\n",
    "dataset = [X1, X2]\n",
    "X, Y = dataset[-imputation_target], dataset[imputation_target]\n",
    "\n",
    "# Fit model\n",
    "knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=10)\n",
    "knn.fit(X, Y)\n",
    "projection = [None for _ in range(2)]\n",
    "projection[imputation_target] = knn.predict(X)\n",
    "\n",
    "# Write to file\n",
    "for i, proj in enumerate(projection):\n",
    "    if proj is not None: np.savetxt(f'P{i+1}.txt', proj)\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:15:45.279560Z",
     "iopub.status.busy": "2025-01-16T23:15:45.279323Z",
     "iopub.status.idle": "2025-01-16T23:15:45.282628Z",
     "shell.execute_reply": "2025-01-16T23:15:45.282110Z"
    }
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "new_wd = os.path.join(RUN_FOLDER, 'MLP')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "import sklearn.neighbors\n",
    "import sklearn.neural_network\n",
    "\n",
    "# Load data\n",
    "X1, X2 = np.loadtxt('../X1.txt'), np.loadtxt('../X2.txt')\n",
    "dataset = [X1, X2]\n",
    "X, Y = dataset[-imputation_target], dataset[imputation_target]\n",
    "\n",
    "# Fit model\n",
    "mlp = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(128,), max_iter=1_000)\n",
    "mlp.fit(X, Y)\n",
    "projection = [None for _ in range(2)]\n",
    "projection[imputation_target] = mlp.predict(X)\n",
    "\n",
    "# Write to file\n",
    "for i, proj in enumerate(projection):\n",
    "    if proj is not None: np.savetxt(f'P{i+1}.txt', proj)\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:15:45.284858Z",
     "iopub.status.busy": "2025-01-16T23:15:45.284665Z",
     "iopub.status.idle": "2025-01-16T23:15:45.287587Z",
     "shell.execute_reply": "2025-01-16T23:15:45.287087Z"
    }
   },
   "outputs": [],
   "source": [
    "# JAMIE\n",
    "# https://github.com/Oafish1/JAMIE\n",
    "new_wd = os.path.join(RUN_FOLDER, 'JAMIE')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n jamie \\\n",
    " python {os.path.join(BASE_FOLDER, 'jamie_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " -t {imputation_target+1} \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BABEL\n",
    "# https://github.com/wukevin/babel/blob/main/bin/convert_to_10x_dir_files.py\n",
    "new_wd = os.path.join(RUN_FOLDER, 'JAMIE')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n babel \\\n",
    " python {os.path.join(BASE_FOLDER, 'babel/bin/train.py')} \\\n",
    " --data {os.path.join(RUN_FOLDER, 'X.h5')} \\\n",
    " --outdir {new_wd}\n",
    "!conda run -n babel \\\n",
    " python {os.path.join(BASE_FOLDER, 'babel/bin/predict_model.py')} \\\n",
    " --checkpoint {os.path.join(new_wd, net_asdf)} \\\n",
    " --data {os.path.join(RUN_FOLDER, 'X.h5')} \\\n",
    " --outdir {new_wd}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:15:45.289936Z",
     "iopub.status.busy": "2025-01-16T23:15:45.289712Z",
     "iopub.status.idle": "2025-01-16T23:15:45.293345Z",
     "shell.execute_reply": "2025-01-16T23:15:45.292585Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:  # Not done\n",
    "    # scVI\n",
    "    new_wd = os.path.join(RUN_FOLDER, 'scVI')\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "\n",
    "    X_fname = os.path.join(RUN_FOLDER, 'X1.txt')\n",
    "    Y_fname = os.path.join(RUN_FOLDER, 'X2.txt')\n",
    "\n",
    "    import numpy as np\n",
    "    import scvi\n",
    "\n",
    "    scvi.settings.seed = 42\n",
    "\n",
    "    X = np.loadtxt(X_fname)\n",
    "    Y = np.loadtxt(Y_fname)\n",
    "\n",
    "    scvi.model.SCVI.setup_anndata\n",
    "\n",
    "\n",
    "    os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
