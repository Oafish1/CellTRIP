{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:10:30.566908Z",
     "iopub.status.busy": "2025-01-16T23:10:30.566454Z",
     "iopub.status.idle": "2025-01-16T23:10:32.361001Z",
     "shell.execute_reply": "2025-01-16T23:10:32.360490Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import scipy.sparse\n",
    "import torch\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../examples'))\n",
    "import data\n",
    "import celltrip\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO\n",
    "  - Run models multiple times with different seeds to get plotted variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:10:32.367432Z",
     "iopub.status.busy": "2025-01-16T23:10:32.367278Z",
     "iopub.status.idle": "2025-01-16T23:10:33.241310Z",
     "shell.execute_reply": "2025-01-16T23:10:33.240850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataset_name = 'MMD-MA'\n",
    "imputation_target = None\n",
    "dimensions = 3\n",
    "seed = 42\n",
    "\n",
    "# Derivatives\n",
    "RUN_FOLDER = os.path.join(BASE_FOLDER, 'runs', dataset_name)\n",
    "\n",
    "# Load data and save to file\n",
    "modalities, types, features = data.load_data(dataset_name, DATA_FOLDER)\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER): os.makedirs(RUN_FOLDER)\n",
    "for i in range(len(modalities)):\n",
    "    modality = modalities[i]\n",
    "\n",
    "    # Regular matrices (ManiNetCluster, JAMIE)\n",
    "    np.savetxt(os.path.join(RUN_FOLDER, f'X{i+1}.txt'), modality, delimiter='\\t')\n",
    "\n",
    "    # Similarity matrices (MMD-MA)\n",
    "    modality_z = modality - modality.mean(axis=0, keepdims=True) / modality.std(axis=0, keepdims=True)\n",
    "    similarity = np.matmul(modality_z, modality_z.T)\n",
    "    np.savetxt(os.path.join(RUN_FOLDER, f'X{i+1}_sim.tsv'), similarity, delimiter='\\t')\n",
    "\n",
    "    # Anndata matrices (scVI)\n",
    "    # adata = sc.AnnData(modalities[0])\n",
    "    # adata.var_names = features[0] if isinstance(features[0][0], str) else [f'Feature_{fi}' for fi in features[0]]\n",
    "    # adata.obs_names = [f'Cell_{j}' for j in range(len(adata.obs_names))]\n",
    "    # adata.obs['cell_type'] = types[0][:, 0]\n",
    "    # adata.obs['time'] = types[0][:, -1]\n",
    "    # # adata.obs['batch'] = 0\n",
    "    # adata.write(os.path.join(RUN_FOLDER, f'X{i+1}.h5ad'), compression='gzip')\n",
    "\n",
    "    # HDFS\n",
    "    # https://github.com/scverse/anndata/issues/595#issuecomment-1824376236\n",
    "    concatenated_modalities = np.concatenate(modalities, axis=-1)\n",
    "    barcodes = [f'Cell {i}' for i in range(concatenated_modalities.shape[0])]\n",
    "    feature_types = modalities[0].shape[1] * ['Gene Expression'] + modalities[1].shape[1] * ['Peaks']\n",
    "    feature_names = np.concatenate(features)\n",
    "    feature_ids = np.array(np.arange(feature_names.shape[0]), dtype='str')\n",
    "    genome = concatenated_modalities.shape[1] * ['Something']\n",
    "    sparse_data = scipy.sparse.csr_matrix(concatenated_modalities)\n",
    "\n",
    "    def int_max(x):\n",
    "        return int(max(np.floor(len(str(int(max(x)))) / 4), 1) * 4)\n",
    "    def str_max(x):\n",
    "        return max([len(i) for i in x])\n",
    "\n",
    "    with h5py.File(os.path.join(RUN_FOLDER, f'X.h5'), 'w') as f:\n",
    "        grp = f.create_group('matrix')\n",
    "        grp.create_dataset('barcodes', data=np.array(barcodes, dtype=f'|S{str_max(barcodes)}'))\n",
    "        grp.create_dataset('data', data=np.array(sparse_data.data, dtype=f'<i{int_max(sparse_data.data)}'))\n",
    "        ftrs = grp.create_group('features')\n",
    "        # # this group will lack the following keys:\n",
    "        # # '_all_tag_keys', 'feature_type', 'genome', 'id', 'name', 'pattern', 'read', 'sequence'\n",
    "        ftrs.create_dataset('feature_type', data=np.array(feature_types, dtype=f'|S{str_max(feature_types)}'))\n",
    "        ftrs.create_dataset('genome', data=np.array(genome, dtype=f'|S{str_max(genome)}'))\n",
    "        ftrs.create_dataset('id', data=np.array(feature_ids, dtype=f'|S{str_max(feature_ids)}'))\n",
    "        ftrs.create_dataset('name', data=np.array(feature_names, dtype=f'|S{str_max([str(fn) for fn in feature_names])}'))\n",
    "        grp.create_dataset('indices', data=np.array(sparse_data.indices, dtype=f'<i{int_max(sparse_data.indices)}'))\n",
    "        grp.create_dataset('indptr', data=np.array(sparse_data.indptr, dtype=f'<i{int_max(sparse_data.indptr)}'))\n",
    "        grp.create_dataset('shape', data=np.array(sparse_data.shape[::-1], dtype=f'<i{int_max(sparse_data.shape)}'))\n",
    "\n",
    "# Preview h5 files\n",
    "# print('Generated File')\n",
    "# with h5py.File(os.path.join(RUN_FOLDER, f'X.h5'), 'r') as f: celltrip.utilities.h5_tree(f)\n",
    "# print('\\nBaseline File')\n",
    "# with h5py.File('/home/thema/Downloads/DM_rep4.h5', 'r') as f: celltrip.utilities.h5_tree(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:10:33.243535Z",
     "iopub.status.busy": "2025-01-16T23:10:33.243375Z",
     "iopub.status.idle": "2025-01-16T23:11:21.942767Z",
     "shell.execute_reply": "2025-01-16T23:11:21.939936Z"
    }
   },
   "outputs": [],
   "source": [
    "# LMA\n",
    "# https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "new_wd = os.path.join(RUN_FOLDER, 'LMA')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n maninetcluster \\\n",
    " python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " --align lma \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:11:21.946314Z",
     "iopub.status.busy": "2025-01-16T23:11:21.946100Z",
     "iopub.status.idle": "2025-01-16T23:12:12.906694Z",
     "shell.execute_reply": "2025-01-16T23:12:12.903989Z"
    }
   },
   "outputs": [],
   "source": [
    "# CCA\n",
    "# https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "new_wd = os.path.join(RUN_FOLDER, 'CCA')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n maninetcluster \\\n",
    " python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " --align cca \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:12:12.910330Z",
     "iopub.status.busy": "2025-01-16T23:12:12.910105Z",
     "iopub.status.idle": "2025-01-16T23:12:14.902190Z",
     "shell.execute_reply": "2025-01-16T23:12:14.899428Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLMA\n",
    "# https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "new_wd = os.path.join(RUN_FOLDER, 'NLMA')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n maninetcluster \\\n",
    " python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " --align nlma \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:12:14.906381Z",
     "iopub.status.busy": "2025-01-16T23:12:14.906183Z",
     "iopub.status.idle": "2025-01-16T23:13:00.288100Z",
     "shell.execute_reply": "2025-01-16T23:13:00.287534Z"
    }
   },
   "outputs": [],
   "source": [
    "# JAMIE\n",
    "new_wd = os.path.join(RUN_FOLDER, 'JAMIE')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n jamie \\\n",
    " python {os.path.join(BASE_FOLDER, 'jamie_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:13:00.290378Z",
     "iopub.status.busy": "2025-01-16T23:13:00.290205Z",
     "iopub.status.idle": "2025-01-16T23:15:45.265938Z",
     "shell.execute_reply": "2025-01-16T23:15:45.263148Z"
    }
   },
   "outputs": [],
   "source": [
    "# MMD-MA\n",
    "# https://bitbucket.org/noblelab/2019_mmd_wabi/src/master/manifoldAlignDistortionPen_mmd_multipleStarts.py\n",
    "new_wd = os.path.join(RUN_FOLDER, 'MMD-MA')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n mmdma \\\n",
    " python {os.path.join(BASE_FOLDER, '2019_mmd_wabi/manifoldAlignDistortionPen_mmd_multipleStarts.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1_sim.tsv')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2_sim.tsv')} \\\n",
    " --seed {seed} \\\n",
    " --p {dimensions}\n",
    "!python {os.path.join(BASE_FOLDER, 'mmd_helper.py')}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:15:45.273304Z",
     "iopub.status.busy": "2025-01-16T23:15:45.272675Z",
     "iopub.status.idle": "2025-01-16T23:15:45.276854Z",
     "shell.execute_reply": "2025-01-16T23:15:45.276237Z"
    }
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "new_wd = os.path.join(RUN_FOLDER, 'KNN')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "# Load data\n",
    "X1, X2 = np.loadtxt('../X1.txt'), np.loadtxt('../X2.txt')\n",
    "dataset = [X1, X2]\n",
    "X, Y = dataset[-imputation_target], dataset[imputation_target]\n",
    "\n",
    "# Fit model\n",
    "knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=10)\n",
    "knn.fit(X, Y)\n",
    "projection = [None for _ in range(2)]\n",
    "projection[imputation_target] = knn.predict(X)\n",
    "\n",
    "# Write to file\n",
    "for i, proj in enumerate(projection):\n",
    "    if proj is not None: np.savetxt(f'I{i+1}.txt', proj)\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:15:45.279560Z",
     "iopub.status.busy": "2025-01-16T23:15:45.279323Z",
     "iopub.status.idle": "2025-01-16T23:15:45.282628Z",
     "shell.execute_reply": "2025-01-16T23:15:45.282110Z"
    }
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "new_wd = os.path.join(RUN_FOLDER, 'MLP')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "import sklearn.neighbors\n",
    "import sklearn.neural_network\n",
    "\n",
    "# Load data\n",
    "X1, X2 = np.loadtxt('../X1.txt'), np.loadtxt('../X2.txt')\n",
    "dataset = [X1, X2]\n",
    "X, Y = dataset[-imputation_target], dataset[imputation_target]\n",
    "\n",
    "# Fit model\n",
    "mlp = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(128,), max_iter=1_000)\n",
    "mlp.fit(X, Y)\n",
    "projection = [None for _ in range(2)]\n",
    "projection[imputation_target] = mlp.predict(X)\n",
    "\n",
    "# Write to file\n",
    "for i, proj in enumerate(projection):\n",
    "    if proj is not None: np.savetxt(f'I{i+1}.txt', proj)\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:15:45.284858Z",
     "iopub.status.busy": "2025-01-16T23:15:45.284665Z",
     "iopub.status.idle": "2025-01-16T23:15:45.287587Z",
     "shell.execute_reply": "2025-01-16T23:15:45.287087Z"
    }
   },
   "outputs": [],
   "source": [
    "# JAMIE\n",
    "# https://github.com/Oafish1/JAMIE\n",
    "new_wd = os.path.join(RUN_FOLDER, 'JAMIE')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    "!conda run -n jamie \\\n",
    " python {os.path.join(BASE_FOLDER, 'jamie_helper.py')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    " {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    " -t {imputation_target+1} \\\n",
    " -p {dimensions}\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BABEL\n",
    "# https://github.com/wukevin/babel\n",
    "if False:\n",
    "    new_wd = os.path.join(RUN_FOLDER, 'babel')\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "    !conda run -n babel \\\n",
    "    python {os.path.join(BASE_FOLDER, 'babel/bin/train_model.py')} \\\n",
    "    --data {os.path.join(RUN_FOLDER, 'X.h5')} \\\n",
    "    --outdir {new_wd}\n",
    "    !conda run -n babel \\\n",
    "    python {os.path.join(BASE_FOLDER, 'babel/bin/predict_model.py')} \\\n",
    "    --checkpoint {os.path.join(new_wd, net_asdf)} \\\n",
    "    --data {os.path.join(RUN_FOLDER, 'X.h5')} \\\n",
    "    --outdir {new_wd}\n",
    "\n",
    "    os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T23:15:45.289936Z",
     "iopub.status.busy": "2025-01-16T23:15:45.289712Z",
     "iopub.status.idle": "2025-01-16T23:15:45.293345Z",
     "shell.execute_reply": "2025-01-16T23:15:45.292585Z"
    }
   },
   "outputs": [],
   "source": [
    "# scVI\n",
    "if False:  # Not done\n",
    "    new_wd = os.path.join(RUN_FOLDER, 'scVI')\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "\n",
    "    X_fname = os.path.join(RUN_FOLDER, 'X1.txt')\n",
    "    Y_fname = os.path.join(RUN_FOLDER, 'X2.txt')\n",
    "\n",
    "    import numpy as np\n",
    "    import scvi\n",
    "\n",
    "    scvi.settings.seed = 42\n",
    "\n",
    "    X = np.loadtxt(X_fname)\n",
    "    Y = np.loadtxt(Y_fname)\n",
    "\n",
    "    scvi.model.SCVI.setup_anndata\n",
    "\n",
    "\n",
    "    os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance\n",
    "# https://shap.readthedocs.io/en/latest/\n",
    "new_wd = os.path.join(RUN_FOLDER, 'variance')\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    " # Load data\n",
    "X1, X2 = np.loadtxt(os.path.join(RUN_FOLDER, 'X1.txt')), np.loadtxt(os.path.join(RUN_FOLDER, 'X2.txt'))\n",
    "dataset = [X1, X2]\n",
    "\n",
    "# Get variance\n",
    "importance = [np.var(X, axis=0) for X in dataset]\n",
    "importance = [imp / imp.sum() for imp in dataset]\n",
    "\n",
    "# Write to file\n",
    "for i, imp in enumerate(importance):\n",
    "    if imp is not None: np.savetxt(f'F{i+1}.txt', imp)\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
