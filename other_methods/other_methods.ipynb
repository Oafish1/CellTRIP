{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:30:19.366044Z",
     "iopub.status.busy": "2025-01-30T17:30:19.365065Z",
     "iopub.status.idle": "2025-01-30T17:30:22.657871Z",
     "shell.execute_reply": "2025-01-30T17:30:22.657393Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import scipy.sparse\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../examples'))\n",
    "import data\n",
    "import celltrip\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BASE_FOLDER = os.path.abspath('')\n",
    "DATA_FOLDER = os.path.join(BASE_FOLDER, '../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO\n",
    "  - Seeded 5-fold (i.e., same sequence of seeds each time)\n",
    "  - Make into pure python to run as script\n",
    "  - More methods overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:30:22.660521Z",
     "iopub.status.busy": "2025-01-30T17:30:22.660207Z",
     "iopub.status.idle": "2025-01-30T17:30:40.192705Z",
     "shell.execute_reply": "2025-01-30T17:30:40.191803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# rypltvk5 (ts), 32jqyk54, c8zsunc9,\n",
    "run_id = 'c8zsunc9'\n",
    "total_statistics = False\n",
    "\n",
    "# Get run\n",
    "api = wandb.Api()\n",
    "run = api.run(f'oafish/cellTRIP/{run_id}')\n",
    "config = defaultdict(lambda: {})\n",
    "for k, v in run.config.items():\n",
    "    dict_name, key = k.split('/')\n",
    "    config[dict_name][key] = v\n",
    "config = dict(config)\n",
    "\n",
    "# Parameters\n",
    "dataset_name = config['data']['dataset']\n",
    "imputation_target = config['env']['reward_distance_target']\n",
    "dimensions = config['env']['dim']\n",
    "notebook_seed = np.random.randint(2**32)\n",
    "\n",
    "# Apply seed\n",
    "torch.manual_seed(notebook_seed)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed(notebook_seed)\n",
    "np.random.seed(notebook_seed)\n",
    "\n",
    "# Derivatives\n",
    "RUN_FOLDER = os.path.join(BASE_FOLDER, 'runs', dataset_name)\n",
    "\n",
    "# Load data and save to file\n",
    "modalities, types, features = data.load_data(dataset_name, DATA_FOLDER)\n",
    "ppc = celltrip.utilities.Preprocessing(**config['data'])  # Potentially mismatched if sampled\n",
    "modalities, types = ppc.fit_transform(modalities, types, total_statistics=total_statistics)\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER): os.makedirs(RUN_FOLDER)\n",
    "for i in range(len(modalities)):\n",
    "    modality = modalities[i]\n",
    "\n",
    "    # Regular matrices (ManiNetCluster, JAMIE)\n",
    "    np.savetxt(os.path.join(RUN_FOLDER, f'X{i+1}.txt'), modality, delimiter='\\t')\n",
    "\n",
    "    # Similarity matrices (MMD-MA)\n",
    "    modality_z = modality - modality.mean(axis=0, keepdims=True) / modality.std(axis=0, keepdims=True)\n",
    "    similarity = np.matmul(modality_z, modality_z.T)\n",
    "    np.savetxt(os.path.join(RUN_FOLDER, f'X{i+1}_sim.tsv'), similarity, delimiter='\\t')\n",
    "\n",
    "    # Anndata matrices (scVI)\n",
    "    # adata = sc.AnnData(modalities[0])\n",
    "    # adata.var_names = features[0] if isinstance(features[0][0], str) else [f'Feature_{fi}' for fi in features[0]]\n",
    "    # adata.obs_names = [f'Cell_{j}' for j in range(len(adata.obs_names))]\n",
    "    # adata.obs['cell_type'] = types[0][:, 0]\n",
    "    # adata.obs['time'] = types[0][:, -1]\n",
    "    # # adata.obs['batch'] = 0\n",
    "    # adata.write(os.path.join(RUN_FOLDER, f'X{i+1}.h5ad'), compression='gzip')\n",
    "\n",
    "    # HDFS\n",
    "    # https://github.com/scverse/anndata/issues/595#issuecomment-1824376236\n",
    "    concatenated_modalities = np.concatenate(modalities, axis=-1)\n",
    "    barcodes = [f'Cell {i}' for i in range(concatenated_modalities.shape[0])]\n",
    "    feature_types = modalities[0].shape[1] * ['Gene Expression'] + modalities[1].shape[1] * ['Peaks']\n",
    "    feature_names = np.concatenate(features)\n",
    "    feature_ids = np.array(np.arange(feature_names.shape[0]), dtype='str')\n",
    "    genome = concatenated_modalities.shape[1] * ['Something']\n",
    "    sparse_data = scipy.sparse.csr_matrix(concatenated_modalities)\n",
    "\n",
    "    def int_max(x):\n",
    "        return int(max(np.floor(len(str(int(max(x)))) / 4), 1) * 4)\n",
    "    def str_max(x):\n",
    "        return max([len(i) for i in x])\n",
    "\n",
    "    with h5py.File(os.path.join(RUN_FOLDER, f'X.h5'), 'w') as f:\n",
    "        grp = f.create_group('matrix')\n",
    "        grp.create_dataset('barcodes', data=np.array(barcodes, dtype=f'|S{str_max(barcodes)}'))\n",
    "        grp.create_dataset('data', data=np.array(sparse_data.data, dtype=f'<i{int_max(sparse_data.data)}'))\n",
    "        ftrs = grp.create_group('features')\n",
    "        # # this group will lack the following keys:\n",
    "        # # '_all_tag_keys', 'feature_type', 'genome', 'id', 'name', 'pattern', 'read', 'sequence'\n",
    "        ftrs.create_dataset('feature_type', data=np.array(feature_types, dtype=f'|S{str_max(feature_types)}'))\n",
    "        ftrs.create_dataset('genome', data=np.array(genome, dtype=f'|S{str_max(genome)}'))\n",
    "        ftrs.create_dataset('id', data=np.array(feature_ids, dtype=f'|S{str_max(feature_ids)}'))\n",
    "        ftrs.create_dataset('name', data=np.array(feature_names, dtype=f'|S{str_max([str(fn) for fn in feature_names])}'))\n",
    "        grp.create_dataset('indices', data=np.array(sparse_data.indices, dtype=f'<i{int_max(sparse_data.indices)}'))\n",
    "        grp.create_dataset('indptr', data=np.array(sparse_data.indptr, dtype=f'<i{int_max(sparse_data.indptr)}'))\n",
    "        grp.create_dataset('shape', data=np.array(sparse_data.shape[::-1], dtype=f'<i{int_max(sparse_data.shape)}'))\n",
    "\n",
    "# Preview h5 files\n",
    "# print('Generated File')\n",
    "# with h5py.File(os.path.join(RUN_FOLDER, f'X.h5'), 'r') as f: celltrip.utilities.h5_tree(f)\n",
    "# print('\\nBaseline File')\n",
    "# with h5py.File('/home/thema/Downloads/DM_rep4.h5', 'r') as f: celltrip.utilities.h5_tree(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:30:40.197641Z",
     "iopub.status.busy": "2025-01-30T17:30:40.197160Z",
     "iopub.status.idle": "2025-01-30T17:30:40.235474Z",
     "shell.execute_reply": "2025-01-30T17:30:40.234448Z"
    }
   },
   "outputs": [],
   "source": [
    "if imputation_target is None:\n",
    "    # LMA\n",
    "    # https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "    method_name = 'LMA'\n",
    "    print(f'Running {method_name}')\n",
    "    new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "    !conda run -n maninetcluster \\\n",
    "    python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    "    {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    "    {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    "    --align lma \\\n",
    "    -p {dimensions}\n",
    "\n",
    "    os.chdir(BASE_FOLDER)\n",
    "\n",
    "\n",
    "    # CCA\n",
    "    # https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "    method_name = 'CCA'\n",
    "    print(f'Running {method_name}')\n",
    "    new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "    !conda run -n maninetcluster \\\n",
    "    python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    "    {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    "    {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    "    --align cca \\\n",
    "    -p {dimensions}\n",
    "\n",
    "    os.chdir(BASE_FOLDER)\n",
    "\n",
    "\n",
    "    # NLMA\n",
    "    # https://github.com/namtk/ManiNetCluster/tree/master/inst/python\n",
    "    method_name = 'NLMA'\n",
    "    print(f'Running {method_name}')\n",
    "    new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "    !conda run -n maninetcluster \\\n",
    "    python {os.path.join(BASE_FOLDER, 'maninetcluster_helper.py')} \\\n",
    "    {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    "    {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    "    --align nlma \\\n",
    "    -p {dimensions}\n",
    "\n",
    "    os.chdir(BASE_FOLDER)\n",
    "\n",
    "\n",
    "    # JAMIE\n",
    "    method_name = 'JAMIE'\n",
    "    print(f'Running {method_name}')\n",
    "    new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "    !conda run -n jamie \\\n",
    "    python {os.path.join(BASE_FOLDER, 'jamie_helper.py')} \\\n",
    "    {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    "    {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    "    -p {dimensions} \\\n",
    "    -s {notebook_seed} \\\n",
    "    --suffix {notebook_seed}\n",
    "\n",
    "    os.chdir(BASE_FOLDER)\n",
    "\n",
    "\n",
    "    # MMD-MA\n",
    "    # https://bitbucket.org/noblelab/2019_mmd_wabi/src/master/manifoldAlignDistortionPen_mmd_multipleStarts.py\n",
    "    method_name = 'MMD-MA'\n",
    "    print(f'Running {method_name}')\n",
    "    new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "    fname1, fname2 = f'alpha_hat_{notebook_seed}_10000.txt', f'beta_hat_{notebook_seed}_10000.txt'\n",
    "    !conda run -n mmdma \\\n",
    "    python {os.path.join(BASE_FOLDER, '2019_mmd_wabi/manifoldAlignDistortionPen_mmd_multipleStarts.py')} \\\n",
    "    {os.path.join(RUN_FOLDER, 'X1_sim.tsv')} \\\n",
    "    {os.path.join(RUN_FOLDER, 'X2_sim.tsv')} \\\n",
    "    --seed {notebook_seed} \\\n",
    "    --p {dimensions}\n",
    "    !python {os.path.join(BASE_FOLDER, 'mmd_helper.py')} \\\n",
    "    {fname1} \\\n",
    "    {fname2} \\\n",
    "    --suffix {notebook_seed}\n",
    "\n",
    "    os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:30:40.243415Z",
     "iopub.status.busy": "2025-01-30T17:30:40.242598Z",
     "iopub.status.idle": "2025-01-30T17:35:53.493336Z",
     "shell.execute_reply": "2025-01-30T17:35:53.492766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running KNN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MLP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running JAMIE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random seed: 3187441637\r\n",
      "Shape of Raw data\r\n",
      "Dataset 0: (2452, 119)\r\n",
      "Dataset 1: (2452, 2)\r\n",
      "Device: cuda:0\r\n",
      "---------------------------------\r\n",
      "Find correspondence between Dataset 1 and Dataset 2\r\n",
      "epoch:[500/2000] err:0.0240 alpha:0.0013\r\n",
      "epoch:[1000/2000] err:0.0000 alpha:0.0000\r\n",
      "epoch:[1500/2000] err:0.0018 alpha:0.0001\r\n",
      "epoch:[2000/2000] err:0.1604 alpha:0.0085\r\n",
      "Finished Matching!\r\n",
      "---------------------------------\r\n",
      "Train coupled autoencoders\r\n",
      "epoch:[500/10000]: loss:3.132261\r\n",
      "epoch:[1000/10000]: loss:2.486937\r\n",
      "epoch:[1500/10000]: loss:2.302159\r\n",
      "epoch:[2000/10000]: loss:2.208240\r\n",
      "epoch:[2500/10000]: loss:2.158805\r\n",
      "epoch:[3000/10000]: loss:2.077266\r\n",
      "epoch:[3500/10000]: loss:2.003521\r\n",
      "Finished Mapping!\r\n",
      "---------------------------------\r\n",
      "JAMIE Done!\r\n",
      "Distance: 18.69392443448305\r\n",
      "Correspondence: 124.45628051646054\r\n",
      "Mapping: 152.53126152977347\r\n",
      "Total: 295.68146648071706\r\n",
      "\r\n",
      "\r\n",
      "/home/thema/miniconda3/envs/jamie/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\r\n",
      "  @numba.jit()\r\n",
      "/home/thema/miniconda3/envs/jamie/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\r\n",
      "  @numba.jit()\r\n",
      "/home/thema/miniconda3/envs/jamie/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\r\n",
      "  @numba.jit()\r\n",
      "/home/thema/miniconda3/envs/jamie/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\r\n",
      "  @numba.jit()\r\n",
      "2025-01-30 11:30:54.632523: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2025-01-30 11:30:54.991357: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-01-30 11:30:55.665741: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\r\n",
      "2025-01-30 11:30:55.665830: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\r\n",
      "2025-01-30 11:30:55.665840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n",
      "/home/thema/repos/inept/other_methods/JAMIE/jamie/jamie.py:440: UserWarning: PCA dim must be lower than 119, found 512, adjusting to compensate.\r\n",
      "  warnings.warn(\r\n",
      "/home/thema/repos/inept/other_methods/JAMIE/jamie/jamie.py:440: UserWarning: PCA dim must be lower than 2, found 512, adjusting to compensate.\r\n",
      "  warnings.warn(\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "if imputation_target is not None:\n",
    "    # Random\n",
    "    method_name = 'Random'\n",
    "    print(f'Running {method_name}')\n",
    "    new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "    # Load data\n",
    "    X1, X2 = np.loadtxt('../X1.txt'), np.loadtxt('../X2.txt')\n",
    "    dataset = [X1, X2]\n",
    "    X, Y = dataset[1-imputation_target], dataset[imputation_target]\n",
    "\n",
    "    # Seed and generate\n",
    "    torch.manual_seed(notebook_seed)\n",
    "    projection = [None for _ in range(2)]\n",
    "    projection[imputation_target] = torch.distributions.Normal(0, 1).sample(Y.shape)\n",
    "\n",
    "    # Write to file\n",
    "    for i, proj in enumerate(projection):\n",
    "        if proj is not None: np.savetxt(f'I{i+1}_{notebook_seed}.txt', proj)\n",
    "\n",
    "    os.chdir(BASE_FOLDER)\n",
    "\n",
    "\n",
    "    # KNN\n",
    "    method_name = 'KNN'\n",
    "    print(f'Running {method_name}')\n",
    "    new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "    import sklearn.neighbors\n",
    "\n",
    "    # Load data\n",
    "    X1, X2 = np.loadtxt('../X1.txt'), np.loadtxt('../X2.txt')\n",
    "    dataset = [X1, X2]\n",
    "    X, Y = dataset[1-imputation_target], dataset[imputation_target]\n",
    "\n",
    "    # Seed and random select\n",
    "    # TODO: Replace\n",
    "    np.random.seed(notebook_seed)\n",
    "    rand_idx = np.random.choice(X.shape[0], int(.8*X.shape[0]), replace=False)\n",
    "\n",
    "    # Fit model\n",
    "    knn = sklearn.neighbors.KNeighborsRegressor(n_neighbors=10)\n",
    "    knn.fit(X[rand_idx], Y[rand_idx])\n",
    "    projection = [None for _ in range(2)]\n",
    "    projection[imputation_target] = knn.predict(X)\n",
    "\n",
    "    # Write to file\n",
    "    for i, proj in enumerate(projection):\n",
    "        if proj is not None: np.savetxt(f'I{i+1}_{notebook_seed}.txt', proj)\n",
    "\n",
    "    os.chdir(BASE_FOLDER)\n",
    "\n",
    "\n",
    "    # MLP\n",
    "    method_name = 'MLP'\n",
    "    print(f'Running {method_name}')\n",
    "    new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "    import sklearn.neighbors\n",
    "    import sklearn.neural_network\n",
    "\n",
    "    # Load data\n",
    "    X1, X2 = np.loadtxt('../X1.txt'), np.loadtxt('../X2.txt')\n",
    "    dataset = [X1, X2]\n",
    "    X, Y = dataset[1-imputation_target], dataset[imputation_target]\n",
    "\n",
    "    # Seed and random select\n",
    "    # TODO: Replace\n",
    "    np.random.seed(notebook_seed)\n",
    "    rand_idx = np.random.choice(X.shape[0], int(.8*X.shape[0]), replace=False)\n",
    "\n",
    "    # Fit model\n",
    "    np.random.seed(notebook_seed)\n",
    "    mlp = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(128,), max_iter=1_000)\n",
    "    mlp.fit(X[rand_idx], Y[rand_idx])\n",
    "    projection = [None for _ in range(2)]\n",
    "    projection[imputation_target] = mlp.predict(X)\n",
    "\n",
    "    # Write to file\n",
    "    for i, proj in enumerate(projection):\n",
    "        if proj is not None: np.savetxt(f'I{i+1}_{notebook_seed}.txt', proj)\n",
    "\n",
    "    os.chdir(BASE_FOLDER)\n",
    "\n",
    "\n",
    "    # JAMIE\n",
    "    # https://github.com/Oafish1/JAMIE\n",
    "    method_name = 'JAMIE'\n",
    "    print(f'Running {method_name}')\n",
    "    new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    os.chdir(new_wd)\n",
    "\n",
    "    !conda run -n jamie \\\n",
    "    python {os.path.join(BASE_FOLDER, 'jamie_helper.py')} \\\n",
    "    {os.path.join(RUN_FOLDER, 'X1.txt')} \\\n",
    "    {os.path.join(RUN_FOLDER, 'X2.txt')} \\\n",
    "    -t {imputation_target+1} \\\n",
    "    -p {dimensions} \\\n",
    "    -s {notebook_seed} \\\n",
    "    --suffix {notebook_seed}\n",
    "\n",
    "    os.chdir(BASE_FOLDER)\n",
    "\n",
    "        \n",
    "    # BABEL\n",
    "    # INCOMPATIBLE\n",
    "    # https://github.com/wukevin/babel\n",
    "    # method_name = 'BABEL'\n",
    "    # print(f'Running {method_name}')\n",
    "    # new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    # if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    # os.chdir(new_wd)\n",
    "\n",
    "    # !conda run -n babel \\\n",
    "    # python {os.path.join(BASE_FOLDER, 'babel/bin/train_model.py')} \\\n",
    "    # --data {os.path.join(RUN_FOLDER, 'X.h5')} \\\n",
    "    # --outdir {new_wd}\n",
    "    # !conda run -n babel \\\n",
    "    # python {os.path.join(BASE_FOLDER, 'babel/bin/predict_model.py')} \\\n",
    "    # --checkpoint {os.path.join(new_wd, net_asdf)} \\\n",
    "    # --data {os.path.join(RUN_FOLDER, 'X.h5')} \\\n",
    "    # --outdir {new_wd}\n",
    "\n",
    "    # os.chdir(BASE_FOLDER)\n",
    "\n",
    "\n",
    "    # scVI\n",
    "    # Not done\n",
    "    # method_name = 'scVI'\n",
    "    # print(f'Running {method_name}')\n",
    "    # new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "    # if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "    # os.chdir(new_wd)\n",
    "\n",
    "    # X_fname = os.path.join(RUN_FOLDER, 'X1.txt')\n",
    "    # Y_fname = os.path.join(RUN_FOLDER, 'X2.txt')\n",
    "\n",
    "    # import numpy as np\n",
    "    # import scvi\n",
    "\n",
    "    # scvi.settings.seed = 42\n",
    "\n",
    "    # X = np.loadtxt(X_fname)\n",
    "    # Y = np.loadtxt(Y_fname)\n",
    "\n",
    "    # scvi.model.SCVI.setup_anndata\n",
    "\n",
    "    # os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T17:35:53.496184Z",
     "iopub.status.busy": "2025-01-30T17:35:53.495677Z",
     "iopub.status.idle": "2025-01-30T17:35:53.872630Z",
     "shell.execute_reply": "2025-01-30T17:35:53.872126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Variance\n"
     ]
    }
   ],
   "source": [
    "# Variance\n",
    "# https://shap.readthedocs.io/en/latest/\n",
    "method_name = 'Variance'\n",
    "print(f'Running {method_name}')\n",
    "new_wd = os.path.join(RUN_FOLDER, method_name)\n",
    "if not os.path.exists(new_wd): os.makedirs(new_wd)\n",
    "os.chdir(new_wd)\n",
    "\n",
    " # Load data\n",
    "X1, X2 = np.loadtxt(os.path.join(RUN_FOLDER, 'X1.txt')), np.loadtxt(os.path.join(RUN_FOLDER, 'X2.txt'))\n",
    "dataset = [X1, X2]\n",
    "\n",
    "# Get variance\n",
    "importance = [np.var(X, axis=0) for X in dataset]\n",
    "importance = [imp / imp.sum() for imp in dataset]\n",
    "\n",
    "# Write to file\n",
    "for i, imp in enumerate(importance):\n",
    "    if imp is not None: np.savetxt(f'F{i+1}.txt', imp)\n",
    "\n",
    "os.chdir(BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
